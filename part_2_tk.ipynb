{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Model Training & Evaluation - RNN   \n",
    "Now with the pretrained word embeddings acquired from Part 1 and the dataset acquired from\n",
    "Part 0, you need to train a deep learning model for sentiment classification using the training set,\n",
    "conforming to these requirements:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Use the pretrained word embeddings from Part 1 as inputs; do not update them during training\n",
    "(they are “frozen”).   \n",
    "\n",
    "• Design a simple recurrent neural network (RNN), taking the input word embeddings, and\n",
    "predicting a sentiment label for each sentence. To do that, you need to consider how to\n",
    "aggregate the word representations to represent a sentence.   \n",
    "\n",
    "• Use the validation set to gauge the performance of the model for each epoch during training.\n",
    "You are required to use accuracy as the performance metric during validation and evaluation. \n",
    "   \n",
    "• Use the mini-batch strategy during training. You may choose any preferred optimizer (e.g.,\n",
    "SGD, Adagrad, Adam, RMSprop). Be careful when you choose your initial learning rate and\n",
    "mini-batch size. (You should use the validation set to determine the optimal configuration.)\n",
    "Train the model until the accuracy score on the validation set is not increasing for a few\n",
    "epochs.\n",
    "   \n",
    "• Evaluate your trained model on the test dataset, observing the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from common_utils import load_glove_embeddings, set_seed, EmbeddingMatrix\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = 100 # word embedding size \n",
    "HIDDEN_SIZE = 128 # just as a starter to see \n",
    "NUM_EPOCHS = 100 \n",
    "EMBEDDING_DIM=100\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of <PAD> is:  18030\n"
     ]
    }
   ],
   "source": [
    "# initialize word embeddings\n",
    "word_embeddings = EmbeddingMatrix.load()\n",
    "word_embeddings.add_padding()\n",
    "\n",
    "print(\"The index of <PAD> is: \", word_embeddings.pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        embedding_dim: int,\n",
    "        word_embeddings: torch.Tensor,\n",
    "        pad_idx,\n",
    "        num_layers=1,\n",
    "        output_size=1,\n",
    "        dropout_rate=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings, freeze=True, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim, hidden_dim, num_layers, batch_first=True\n",
    "        )  # this is the num rows of the input matrix\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        if self.dropout_rate > 0:\n",
    "            embedded = self.dropout(self.embedding(x)).float()\n",
    "        else:\n",
    "            embedded = self.embedding(x).float()\n",
    "        \n",
    "        _, hidden = self.rnn(embedded, h0)\n",
    "        hidden = hidden[-1, :, :]\n",
    "        \n",
    "        if self.dropout_rate > 0:\n",
    "            hidden = self.dropout(hidden)\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        sig_out = self.sigmoid(out)\n",
    "        return sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from huggingface first \n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validate and test datasets and dataloaders\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, X, y, word_embeddings:EmbeddingMatrix =word_embeddings):\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.X = X # train_dataset['text']\n",
    "        self.y = y # train_dataset['label']\n",
    "        self.len = len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # tokenize the sentence\n",
    "        tokens = self.tokenize_sentence(self.X[index])\n",
    "        return tokens, self.y[index] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len \n",
    "\n",
    "    def tokenize_sentence(self, x): \n",
    "        '''\n",
    "    returns a list containing the embeddings of each token \n",
    "    '''\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        # word tokens to index, skip if token is not in the word embeddings\n",
    "        tokens = [self.word_embeddings.get_idx(token) for token in tokens if self.word_embeddings.get_idx(token) is not None]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def pad_collate(batch, pad_value):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    # convert xx to a tensor\n",
    "    xx = [torch.tensor(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
    "    return xx_pad, torch.tensor(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ed = EmbeddingsDataset(\n",
    "    train_dataset[\"text\"], train_dataset[\"label\"]\n",
    ")\n",
    "validation_dataset_ed = EmbeddingsDataset(\n",
    "    validation_dataset[\"text\"], validation_dataset[\"label\"]\n",
    ")\n",
    "test_dataset_ed = EmbeddingsDataset(test_dataset[\"text\"], test_dataset[\"label\"])\n",
    "\n",
    "pad_value = word_embeddings.pad_idx\n",
    "# implement minibatch training\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([32, 43])\n",
      "Sample input: \n",
      " tensor([[ 8601,   162,   464,  ..., 18030, 18030, 18030],\n",
      "        [ 8601,   162,   464,  ..., 18030, 18030, 18030],\n",
      "        [16087,  8588, 16020,  ..., 18030, 18030, 18030],\n",
      "        ...,\n",
      "        [12570, 16020, 12283,  ..., 18030, 18030, 18030],\n",
      "        [  996, 11020, 17443,  ..., 18030, 18030, 18030],\n",
      "        [ 8601,   162,  8817,  ..., 18030, 18030, 18030]])\n",
      "\n",
      "Sample label size:  torch.Size([32])\n",
      "Sample label: \n",
      " tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_dataloader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([32, 37])\n",
      "Sample input: \n",
      " tensor([[16087,  8588,   464,  ..., 18030, 18030, 18030],\n",
      "        [16870,  3275, 17759,  ..., 18030, 18030, 18030],\n",
      "        [  464, 10410, 16018,  ..., 18030, 18030, 18030],\n",
      "        ...,\n",
      "        [16020,  6129, 12995,  ..., 18030, 18030, 18030],\n",
      "        [14702, 13057, 16278,  ..., 18030, 18030, 18030],\n",
      "        [17596,  9692,  5547,  ..., 18030, 18030, 18030]])\n",
      "\n",
      "Sample label size:  torch.Size([32])\n",
      "Sample label: \n",
      " tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(validation_dataloader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "    size = len(train_dataloader.dataset)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch_no, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        y_batch = y_batch.clone().detach().float()\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        pred = pred.squeeze(1)\n",
    "\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        train_loss += loss.item() \n",
    "        train_correct += ((pred >= 0.5).float()==y_batch).sum().item() \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= num_batches \n",
    "    train_correct /= size \n",
    "\n",
    "    return train_loss, train_correct \n",
    "   \n",
    "\n",
    "def test_loop(validate_dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(validate_dataloader)\n",
    "    size = len(validate_dataloader.dataset)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in validate_dataloader:\n",
    "            y_batch = y_batch.clone().detach().float()\n",
    "            pred = model(X_batch)\n",
    "            pred = pred.squeeze(1)\n",
    "            pred_binary = (pred >= 0.5).float()\n",
    "            test_loss += loss_fn(pred, y_batch).item()\n",
    "            test_correct += (pred_binary == y_batch).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    return test_loss, test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_RNN = RNN(\n",
    "    hidden_dim=HIDDEN_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    word_embeddings=word_embeddings.to_tensor,\n",
    "    pad_idx=word_embeddings.pad_idx,\n",
    "    num_layers=1,\n",
    ")\n",
    "optim = torch.optim.Adam(basic_RNN.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# stacked RNN\n",
    "stacked_RNN = RNN(\n",
    "    hidden_dim=HIDDEN_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    word_embeddings=word_embeddings.to_tensor,\n",
    "    pad_idx=word_embeddings.pad_idx,\n",
    "    num_layers=3,\n",
    ")\n",
    "\n",
    "# RNN with dropout\n",
    "RNN_dropout = RNN(\n",
    "    hidden_dim=HIDDEN_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    word_embeddings=word_embeddings.to_tensor,\n",
    "    pad_idx=word_embeddings.pad_idx,\n",
    "    num_layers=1,\n",
    "    dropout_rate=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model, just using epoch = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \tValidation Acc:0.4821763602251407 \tTrain Acc:0.5012895662368112\n",
      "Epoch:11 \tValidation Acc:0.49530956848030017 \tTrain Acc:0.5112543962485346\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "train_acc = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    train_loss, train_correct = train_loop(\n",
    "        train_dataloader, basic_RNN, criterion, optim\n",
    "    )\n",
    "    validate_loss, validate_correct = test_loop(\n",
    "        validation_dataloader, basic_RNN, criterion\n",
    "    )\n",
    "    validation_acc.append(validate_correct)\n",
    "    train_acc.append(train_correct)\n",
    "    if i % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch:{i+1} \\tValidation Acc:{validate_correct} \\tTrain Acc:{train_correct}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_acc, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain acc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_acc, label=\"train acc\")\n",
    "plt.plot(validation_acc, label=\"validation acc\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracies\")\n",
    "plt.title(\"train vs validation accs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "We will perform grid search on the no. of training epochs, lr, optimizer and batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. RNN\n",
    "(a) Report the final configuration of your best model, namely the number of training epochs,\n",
    "learning rate, optimizer, batch size.\n",
    "(b) Report the accuracy score on the test set, as well as the accuracy score on the validation\n",
    "set for each epoch during training.\n",
    "(c) RNNs produce a hidden vector for each word, instead of the entire sentence. Which methods\n",
    "have you tried in deriving the final sentence representation to perform sentiment classification?\n",
    "Describe all the strategies you have implemented, together with their accuracy scores on the\n",
    "test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = [] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
