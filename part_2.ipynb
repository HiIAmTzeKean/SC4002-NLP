{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Model Training & Evaluation - RNN   \n",
    "Now with the pretrained word embeddings acquired from Part 1 and the dataset acquired from\n",
    "Part 0, you need to train a deep learning model for sentiment classification using the training set,\n",
    "conforming to these requirements:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Use the pretrained word embeddings from Part 1 as inputs; do not update them during training\n",
    "(they are “frozen”).   \n",
    "\n",
    "• Design a simple recurrent neural network (RNN), taking the input word embeddings, and\n",
    "predicting a sentiment label for each sentence. To do that, you need to consider how to\n",
    "aggregate the word representations to represent a sentence.   \n",
    "\n",
    "• Use the validation set to gauge the performance of the model for each epoch during training.\n",
    "You are required to use accuracy as the performance metric during validation and evaluation. \n",
    "   \n",
    "• Use the mini-batch strategy during training. You may choose any preferred optimizer (e.g.,\n",
    "SGD, Adagrad, Adam, RMSprop). Be careful when you choose your initial learning rate and\n",
    "mini-batch size. (You should use the validation set to determine the optimal configuration.)\n",
    "Train the model until the accuracy score on the validation set is not increasing for a few\n",
    "epochs.\n",
    "   \n",
    "• Evaluate your trained model on the test dataset, observing the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\SC4002-NLP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import UNK_TOKEN,EMBEDDING_DIM,SAVE_DIR,VOCAB_PATH,EMBEDDING_MATRIX_PATH,WORD2IDX_PATH,IDX2WORD_PATH,tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a\n",
    "Use the pretrained word embeddings from Part 1 as inputs; do not update them during training (they are “frozen”).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding\n",
    "embedding_matrix:np.ndarray = np.load(EMBEDDING_MATRIX_PATH)\n",
    "\n",
    "# transform the embedding matrix to tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping 'word2idx' loaded from './result/word2idx.json'.\n",
      "Mapping 'idx2word' loaded from './result/idx2word.json'.\n"
     ]
    }
   ],
   "source": [
    "# load supporting dictionaries for encoding\n",
    "# Load the 'word2idx' mapping\n",
    "with open(WORD2IDX_PATH, 'r', encoding='utf-8') as f:\n",
    "    word2idx:dict = json.load(f)\n",
    "print(f\"Mapping 'word2idx' loaded from '{WORD2IDX_PATH}'.\")\n",
    "\n",
    "# Load the 'idx2word' mapping\n",
    "with open(IDX2WORD_PATH, 'r', encoding='utf-8') as f:\n",
    "    idx2word:dict = json.load(f)\n",
    "print(f\"Mapping 'idx2word' loaded from '{IDX2WORD_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2b\n",
    "Design a simple recurrent neural network (RNN), taking the input word embeddings, and\n",
    "predicting a sentiment label for each sentence. To do that, you need to consider how to\n",
    "aggregate the word representations to represent a sentence.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare labels for each sentence\n",
    "start_time = time.time()\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time to load dataset: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# obtain labels for each sentence\n",
    "y_train = train_dataset[\"label\"]\n",
    "y_validation = validation_dataset[\"label\"]\n",
    "y_test = test_dataset[\"label\"]\n",
    "\n",
    "# split data into tokens\n",
    "train_tokenized = tokenize(train_dataset)\n",
    "validation_tokenized = tokenize(validation_dataset)\n",
    "test_tokenized = tokenize(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18030\n",
      "Vocabulary saved to ./result/vocab.json\n",
      "Vocabulary Size: 5418\n",
      "Vocabulary saved to ./result/vocab.json\n",
      "Vocabulary Size: 5456\n",
      "Vocabulary saved to ./result/vocab.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the tokens into indices with the word2idx dictionary\n",
    "def encode_tokens(tokens:list, word2idx:dict) -> list:\n",
    "    \"\"\"Encode the tokens into indices with the word2idx dictionary\n",
    "\n",
    "    :param tokens: List of sentences, where each sentence is a list of tokens\n",
    "    :type tokens: list\n",
    "    :param word2idx: Dictionary mapping words to indices\n",
    "    :type word2idx: dict\n",
    "    :return: List of sentences, where each sentence is a list of indices\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    return [[word2idx.get(token, word2idx[UNK_TOKEN]) for token in sentence] for sentence in tokens]\n",
    "\n",
    "train_encoded = encode_tokens(train_tokenized, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected structure of the encoding would be a sequence of int for each sentence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3994, 7913, 5842, 5842, 5062, 12650, 5062, 10541, 15750]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The expected structure of the encoding would be a sequence of int for \"\n",
    "      \"each sentence.\")\n",
    "train_encoded[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddding for the model as each sentence has different length, this is to\n",
    "# ensure that there is a consistent input dim for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\tFeatures Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"training...\"):\n",
    "        ids = batch[\"ids\"].to(device)\n",
    "        length = batch[\"length\"]\n",
    "        label = batch[\"label\"].to(device)\n",
    "        prediction = model(ids, length)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc=\"evaluating...\"):\n",
    "            ids = batch[\"ids\"].to(device)\n",
    "            length = batch[\"length\"]\n",
    "            label = batch[\"label\"].to(device)\n",
    "            prediction = model(ids, length)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "metrics = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(\n",
    "        train_data_loader, model, criterion, optimizer, device\n",
    "    )\n",
    "    valid_loss, valid_acc = evaluate(valid_data_loader, model, criterion, device)\n",
    "    metrics[\"train_losses\"].append(train_loss)\n",
    "    metrics[\"train_accs\"].append(train_acc)\n",
    "    metrics[\"valid_losses\"].append(valid_loss)\n",
    "    metrics[\"valid_accs\"].append(valid_acc)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"lstm.pt\")\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "    print(f\"valid_loss: {valid_loss:.3f}, valid_acc: {valid_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. RNN\n",
    "(a) Report the final configuration of your best model, namely the number of training epochs,\n",
    "learning rate, optimizer, batch size.   \n",
    "\n",
    "(b) Report the accuracy score on the test set, as well as the accuracy score on the validation\n",
    "set for each epoch during training.   \n",
    "\n",
    "(c) RNNs produce a hidden vector for each word, instead of the entire sentence. Which methods\n",
    "have you tried in deriving the final sentence representation to perform sentiment classification?\n",
    "Describe all the strategies you have implemented, together with their accuracy scores on the\n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
