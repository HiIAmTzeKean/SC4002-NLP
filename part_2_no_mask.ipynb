{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# We omit warnings to keep the output clean\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "\n",
    "from common_utils import load_glove_embeddings, set_seed, EmbeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed \n",
    "set_seed()\n",
    "\n",
    "# initialize parameters\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = 100 # word embedding size \n",
    "HIDDEN_SIZE = 128 # just as a starter to see \n",
    "NUM_EPOCHS = 100 \n",
    "EMBEDDING_DIM=100\n",
    "GRADIENT_CLIP=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from huggingface first \n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "with open('result/word2idx.json', \"r\") as file:\n",
    "    word2idx = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GloVe words loaded: 400000\n",
      "(400001, 100)\n"
     ]
    }
   ],
   "source": [
    "# initialize word embeddings\n",
    "word_embeddings = load_glove_embeddings()\n",
    "\n",
    "embeddings = [word_embeddings[word] for word in word_embeddings.keys()]\n",
    "\n",
    "embedding_matrix_np = np.array(embeddings)\n",
    "embedding_matrix_np = np.vstack((embedding_matrix_np, np.zeros((1, 100)))) # add a row of zeros for padding\n",
    "\n",
    "print(embedding_matrix_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "# TODO: change the num_tokens \n",
    "class EmbeddingsDataset2(Dataset):\n",
    "  def __init__(self, X, y, num_tokens_per_sentence=8, word_embeddings=word_embeddings):\n",
    "    self.num_tokens_per_sentence = num_tokens_per_sentence\n",
    "    self.word_embeddings = word_embeddings\n",
    "    self.X = X # train_dataset['text']\n",
    "    self.y = y # train_dataset['label']\n",
    "    self.len = len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # tokenize the sentence \n",
    "    tokens = self.tokenize_sentence(self.X[index])\n",
    "    # convert each token to embeddings \n",
    "    sentence_tensor = self.convert_sentence_into_indices(tokens)\n",
    "    label = torch.tensor(self.y[index], dtype=torch.long)\n",
    "    return sentence_tensor, label \n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len \n",
    "\n",
    "  def tokenize_sentence(self, x): \n",
    "    '''\n",
    "    returns a list containing the embeddings of each token \n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(x.lower())\n",
    "    return tokens \n",
    "  \n",
    "  def convert_sentence_into_indices(self, tokens):\n",
    "    indices = []\n",
    "    num_tokens_used = 0 \n",
    "    for token in tokens:\n",
    "      if num_tokens_used == self.num_tokens_per_sentence:\n",
    "        break # we have enough of tokens from the sentence \n",
    "      if token in word2idx:\n",
    "        indices.append(word2idx[token])\n",
    "        num_tokens_used += 1 \n",
    "    # # if not enough tokens in the sentence, use index of ?? \n",
    "    if len(indices) < self.num_tokens_per_sentence:\n",
    "      padding = [(embedding_matrix_np.shape[0] - 1 ) for _ in range(self.num_tokens_per_sentence - len(indices))]\n",
    "      indices.extend(padding)\n",
    "    #print(indices)\n",
    "    indices = torch.tensor(indices, dtype=torch.long)\n",
    "    return indices\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ed = EmbeddingsDataset2(train_dataset['text'], train_dataset['label'])\n",
    "validation_dataset_ed = EmbeddingsDataset2(validation_dataset['text'], validation_dataset['label'])\n",
    "test_dataset_ed = EmbeddingsDataset2(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "# implement minibatch training \n",
    "train_dataloader = DataLoader(train_dataset_ed, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset_ed, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset_ed, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Embeddings \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class VanillaRNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix_torch=torch.tensor(embedding_matrix_np, dtype=torch.float)):\n",
    "        super(VanillaRNNWithEmbedding, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_torch, freeze=True, padding_idx=embedding_matrix_torch.shape[0]-1)\n",
    "        self.num_layers = num_layers \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # this is the num rows of the input matrix \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, dtype=torch.float).to(x.device)\n",
    "        # Pass the embeddings through the RNN layer\n",
    "        out, hidden = self.rnn(x, h0)\n",
    "        # Max pooling\n",
    "        #out, _ = torch.max(out, dim=1)\n",
    "        # Only take the last output for each sequence\n",
    "        res = hidden[-1]\n",
    "        # Pass through the fully connected layer\n",
    "        res = self.fc(res)\n",
    "        # Apply sigmoid activation (for binary classification)\n",
    "        res = self.sigmoid(res)\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_bce(train_dataloader, model, loss_fn, optimizer):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "    size = len(train_dataloader.dataset)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch_no, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        if train_on_gpu:\n",
    "            X_batch = X_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        \n",
    "        pred = pred.squeeze(1)\n",
    "        pred_binary = (pred >= 0.5).long()\n",
    "        loss = loss_fn(pred, y_batch.float())\n",
    "        train_loss += loss.item() \n",
    "        train_correct += (pred_binary==y_batch.long()).sum().item() \n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # TODO add main branch\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= num_batches \n",
    "    train_correct /= size \n",
    "\n",
    "    return train_loss, train_correct \n",
    "   \n",
    "\n",
    "def test_loop_bce(validate_dataloader, model, loss_fn):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    num_batches = len(validate_dataloader)\n",
    "    size = len(validate_dataloader.dataset)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in validate_dataloader:\n",
    "            if train_on_gpu:\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            pred = model(X_batch)\n",
    "            pred = pred.squeeze(1)\n",
    "            pred_binary = (pred >= 0.5).long()\n",
    "            test_loss += loss_fn(pred, y_batch.float()).item()\n",
    "            test_correct += (pred_binary == y_batch.long()).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    return test_loss, test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, criterion, training_dataloader, validation_dataloader, epoch = NUM_EPOCHS):\n",
    "  validation_acc = [] \n",
    "  train_acc = []\n",
    "  train_losses, validate_losses = [], []\n",
    "  for i in range(epoch):\n",
    "    train_loss, train_correct = train_loop_bce(training_dataloader, model, criterion, optim) \n",
    "    validate_loss, validate_correct = test_loop_bce(validation_dataloader, model, criterion)\n",
    "    validation_acc.append(validate_correct)\n",
    "    train_acc.append(train_correct)\n",
    "    train_losses.append(train_loss)\n",
    "    validate_losses.append(validate_loss)\n",
    "\n",
    "    print(f\"Epoch {i+1}, Train Loss: {train_loss:.4f}, Validate Loss: {validate_loss:.4f}\")\n",
    "    #if i%10 == 0:\n",
    "    print(f\"Epoch:{i+1} \\t Train Acc:{train_correct} \\t Validation Acc:{validate_correct}\")\n",
    "  return train_acc, validation_acc, train_losses, validate_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6929, Validate Loss: 0.6921\n",
      "Epoch:1 \t Train Acc:0.5145369284876905 \t Validation Acc:0.5075046904315197\n",
      "Epoch 2, Train Loss: 0.6862, Validate Loss: 0.6875\n",
      "Epoch:2 \t Train Acc:0.5487690504103165 \t Validation Acc:0.5412757973733584\n",
      "Epoch 3, Train Loss: 0.6783, Validate Loss: 0.6827\n",
      "Epoch:3 \t Train Acc:0.5728018757327081 \t Validation Acc:0.5534709193245778\n",
      "Epoch 4, Train Loss: 0.6718, Validate Loss: 0.6847\n",
      "Epoch:4 \t Train Acc:0.5807737397420868 \t Validation Acc:0.5487804878048781\n",
      "Epoch 5, Train Loss: 0.6668, Validate Loss: 0.6810\n",
      "Epoch:5 \t Train Acc:0.5917936694021102 \t Validation Acc:0.5684803001876173\n",
      "Epoch 6, Train Loss: 0.6611, Validate Loss: 0.6864\n",
      "Epoch:6 \t Train Acc:0.603282532239156 \t Validation Acc:0.5469043151969981\n",
      "Epoch 7, Train Loss: 0.6584, Validate Loss: 0.6915\n",
      "Epoch:7 \t Train Acc:0.6067995310668229 \t Validation Acc:0.5600375234521576\n",
      "Epoch 8, Train Loss: 0.6552, Validate Loss: 0.6879\n",
      "Epoch:8 \t Train Acc:0.6160609613130129 \t Validation Acc:0.5590994371482176\n",
      "Epoch 9, Train Loss: 0.6524, Validate Loss: 0.6928\n",
      "Epoch:9 \t Train Acc:0.6167643610785463 \t Validation Acc:0.5478424015009381\n",
      "Epoch 10, Train Loss: 0.6488, Validate Loss: 0.6901\n",
      "Epoch:10 \t Train Acc:0.6218053927315358 \t Validation Acc:0.5572232645403377\n",
      "Epoch 11, Train Loss: 0.6471, Validate Loss: 0.6912\n",
      "Epoch:11 \t Train Acc:0.6245017584994138 \t Validation Acc:0.5450281425891182\n",
      "Epoch 12, Train Loss: 0.6454, Validate Loss: 0.6901\n",
      "Epoch:12 \t Train Acc:0.6229777256740915 \t Validation Acc:0.5487804878048781\n",
      "Epoch 13, Train Loss: 0.6434, Validate Loss: 0.6900\n",
      "Epoch:13 \t Train Acc:0.6249706916764362 \t Validation Acc:0.5637898686679175\n",
      "Epoch 14, Train Loss: 0.6416, Validate Loss: 0.6909\n",
      "Epoch:14 \t Train Acc:0.6291910902696366 \t Validation Acc:0.5647279549718575\n",
      "Epoch 15, Train Loss: 0.6397, Validate Loss: 0.6956\n",
      "Epoch:15 \t Train Acc:0.6295427901524033 \t Validation Acc:0.5572232645403377\n",
      "Epoch 16, Train Loss: 0.6374, Validate Loss: 0.7033\n",
      "Epoch:16 \t Train Acc:0.6322391559202813 \t Validation Acc:0.5450281425891182\n",
      "Epoch 17, Train Loss: 0.6366, Validate Loss: 0.7059\n",
      "Epoch:17 \t Train Acc:0.6347010550996482 \t Validation Acc:0.5553470919324578\n",
      "Epoch 18, Train Loss: 0.6345, Validate Loss: 0.7019\n",
      "Epoch:18 \t Train Acc:0.6321219226260258 \t Validation Acc:0.5544090056285178\n",
      "Epoch 19, Train Loss: 0.6337, Validate Loss: 0.7043\n",
      "Epoch:19 \t Train Acc:0.6384525205158265 \t Validation Acc:0.5590994371482176\n",
      "Epoch 20, Train Loss: 0.6312, Validate Loss: 0.7050\n",
      "Epoch:20 \t Train Acc:0.6432590855803048 \t Validation Acc:0.5525328330206379\n",
      "Epoch 21, Train Loss: 0.6294, Validate Loss: 0.7040\n",
      "Epoch:21 \t Train Acc:0.6429073856975381 \t Validation Acc:0.5581613508442776\n",
      "Epoch 22, Train Loss: 0.6278, Validate Loss: 0.7091\n",
      "Epoch:22 \t Train Acc:0.6411488862837046 \t Validation Acc:0.5525328330206379\n",
      "Epoch 23, Train Loss: 0.6269, Validate Loss: 0.7111\n",
      "Epoch:23 \t Train Acc:0.6427901524032825 \t Validation Acc:0.5450281425891182\n",
      "Epoch 24, Train Loss: 0.6257, Validate Loss: 0.7100\n",
      "Epoch:24 \t Train Acc:0.64947245017585 \t Validation Acc:0.5553470919324578\n",
      "Epoch 25, Train Loss: 0.6239, Validate Loss: 0.7068\n",
      "Epoch:25 \t Train Acc:0.6472450175849941 \t Validation Acc:0.550656660412758\n",
      "Epoch 26, Train Loss: 0.6217, Validate Loss: 0.7136\n",
      "Epoch:26 \t Train Acc:0.6505275498241501 \t Validation Acc:0.5459662288930581\n",
      "Epoch 27, Train Loss: 0.6192, Validate Loss: 0.7121\n",
      "Epoch:27 \t Train Acc:0.658147713950762 \t Validation Acc:0.5525328330206379\n",
      "Epoch 28, Train Loss: 0.6180, Validate Loss: 0.7142\n",
      "Epoch:28 \t Train Acc:0.658968347010551 \t Validation Acc:0.5544090056285178\n",
      "Epoch 29, Train Loss: 0.6163, Validate Loss: 0.7121\n",
      "Epoch:29 \t Train Acc:0.6593200468933177 \t Validation Acc:0.550656660412758\n",
      "Epoch 30, Train Loss: 0.6136, Validate Loss: 0.7167\n",
      "Epoch:30 \t Train Acc:0.6617819460726846 \t Validation Acc:0.5534709193245778\n",
      "Epoch 31, Train Loss: 0.6113, Validate Loss: 0.7167\n",
      "Epoch:31 \t Train Acc:0.6627198124267292 \t Validation Acc:0.5619136960600375\n",
      "Epoch 32, Train Loss: 0.6083, Validate Loss: 0.7117\n",
      "Epoch:32 \t Train Acc:0.6635404454865181 \t Validation Acc:0.5637898686679175\n",
      "Epoch 33, Train Loss: 0.6072, Validate Loss: 0.7293\n",
      "Epoch:33 \t Train Acc:0.6663540445486518 \t Validation Acc:0.5525328330206379\n",
      "Epoch 34, Train Loss: 0.6044, Validate Loss: 0.7206\n",
      "Epoch:34 \t Train Acc:0.6691676436107855 \t Validation Acc:0.5581613508442776\n",
      "Epoch 35, Train Loss: 0.6006, Validate Loss: 0.7267\n",
      "Epoch:35 \t Train Acc:0.6731535756154748 \t Validation Acc:0.5412757973733584\n",
      "Epoch 36, Train Loss: 0.5978, Validate Loss: 0.7306\n",
      "Epoch:36 \t Train Acc:0.672567409144197 \t Validation Acc:0.5459662288930581\n",
      "Epoch 37, Train Loss: 0.5943, Validate Loss: 0.7280\n",
      "Epoch:37 \t Train Acc:0.6787807737397421 \t Validation Acc:0.5590994371482176\n",
      "Epoch 38, Train Loss: 0.5904, Validate Loss: 0.7300\n",
      "Epoch:38 \t Train Acc:0.6824150058616647 \t Validation Acc:0.549718574108818\n",
      "Epoch 39, Train Loss: 0.5870, Validate Loss: 0.7343\n",
      "Epoch:39 \t Train Acc:0.6797186400937867 \t Validation Acc:0.5431519699812383\n",
      "Epoch 40, Train Loss: 0.5815, Validate Loss: 0.7354\n",
      "Epoch:40 \t Train Acc:0.6865181711606096 \t Validation Acc:0.5469043151969981\n",
      "Epoch 41, Train Loss: 0.5777, Validate Loss: 0.7415\n",
      "Epoch:41 \t Train Acc:0.6961313012895662 \t Validation Acc:0.5544090056285178\n",
      "Epoch 42, Train Loss: 0.5703, Validate Loss: 0.7463\n",
      "Epoch:42 \t Train Acc:0.6994138335287221 \t Validation Acc:0.551594746716698\n",
      "Epoch 43, Train Loss: 0.5653, Validate Loss: 0.7557\n",
      "Epoch:43 \t Train Acc:0.7017584994138335 \t Validation Acc:0.5525328330206379\n",
      "Epoch 44, Train Loss: 0.5593, Validate Loss: 0.7594\n",
      "Epoch:44 \t Train Acc:0.7086752637749121 \t Validation Acc:0.5375234521575984\n",
      "Epoch 45, Train Loss: 0.5523, Validate Loss: 0.7666\n",
      "Epoch:45 \t Train Acc:0.7121922626025792 \t Validation Acc:0.5469043151969981\n",
      "Epoch 46, Train Loss: 0.5450, Validate Loss: 0.7628\n",
      "Epoch:46 \t Train Acc:0.7186400937866354 \t Validation Acc:0.5403377110694184\n",
      "Epoch 47, Train Loss: 0.5350, Validate Loss: 0.7892\n",
      "Epoch:47 \t Train Acc:0.7243845252051583 \t Validation Acc:0.5487804878048781\n",
      "Epoch 48, Train Loss: 0.5296, Validate Loss: 0.7929\n",
      "Epoch:48 \t Train Acc:0.7313012895662369 \t Validation Acc:0.5412757973733584\n",
      "Epoch 49, Train Loss: 0.5192, Validate Loss: 0.7947\n",
      "Epoch:49 \t Train Acc:0.7379835873388042 \t Validation Acc:0.5384615384615384\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(vanilla_rnn\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m----> 5\u001b[0m train_acc_vanilla_rnn, validation_acc_vanilla_rnn, train_loss_vanilla_rnn, validation_loss_vanilla_rnn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvanilla_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optim, criterion, training_dataloader, validation_dataloader, epoch)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m      6\u001b[0m   train_loss, train_correct \u001b[38;5;241m=\u001b[39m train_loop_bce(training_dataloader, model, criterion, optim) \n\u001b[1;32m----> 7\u001b[0m   validate_loss, validate_correct \u001b[38;5;241m=\u001b[39m \u001b[43mtest_loop_bce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m   validation_acc\u001b[38;5;241m.\u001b[39mappend(validate_correct)\n\u001b[0;32m      9\u001b[0m   train_acc\u001b[38;5;241m.\u001b[39mappend(train_correct)\n",
      "Cell \u001b[1;32mIn[9], line 55\u001b[0m, in \u001b[0;36mtest_loop_bce\u001b[1;34m(validate_dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m     53\u001b[0m         pred_binary \u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     54\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 55\u001b[0m         test_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpred_binary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     57\u001b[0m test_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches\n\u001b[0;32m     58\u001b[0m test_correct \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m size\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vanilla_rnn = VanillaRNNWithEmbedding(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=2, num_classes=1)\n",
    "optim = torch.optim.Adam(vanilla_rnn.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_acc_vanilla_rnn, validation_acc_vanilla_rnn, train_loss_vanilla_rnn, validation_loss_vanilla_rnn = train(vanilla_rnn, optim, criterion, train_dataloader, validation_dataloader, epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
