{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Enhancement\n",
    "The RNN model used in Part 2 is a basic model to perform the task of sentiment classification. In\n",
    "this section, you will design strategies to improve upon the previous model you have built. You are\n",
    "required to implement the following adjustments:\n",
    "\n",
    "1. Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "2. As discussed in Question 1(c), apply your solution in mitigating the influence of OOV words\n",
    "and train your model again.\n",
    "3. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a biLSTM model and a biGRU model, incorporating recurrent computations in both directions and\n",
    "stacking multiple layers if possible.\n",
    "4. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a Convolutional Neural Network (CNN) to produce sentence representations and perform sentiment\n",
    "classification.\n",
    "5. Further improve your model. You are free to use any strategy other than the above mentioned solutions. Changing hyper-parameters or stacking more layers is not counted towards\n",
    "a meaningful improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "\n",
    "### Approach\n",
    "\n",
    "We will use the same model as in part 2 notebook, but now we will also back propagate\n",
    "the loss into the word embeddings itself. This will mean that as the model learns,\n",
    "the word embeddings would also update, causing the encoding of the words to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from common_utils import UNK_TOKEN,EMBEDDING_DIM,SAVE_DIR,VOCAB_PATH,EMBEDDING_MATRIX_PATH,WORD2IDX_PATH,IDX2WORD_PATH,tokenize, EmbeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, word_embedding:np.ndarray):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Word2Vec embedding layer\n",
    "        # freeze=False to enable updates to embeddings\n",
    "        self.word2vec_embeddings = nn.Embedding.from_pretrained(torch.tensor(word_embedding), freeze=False)\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Look up word embeddings\n",
    "        x = self.word2vec_embeddings(x)\n",
    "\n",
    "        # Pass through RNN\n",
    "        x, _ = self.rnn(x)\n",
    "\n",
    "        # Take the last hidden state\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "# Load EmbeddingMatrix\n",
    "w2v_model = EmbeddingMatrix.load()\n",
    "\n",
    "# Create RNN model\n",
    "model = RNNModel(w2v_model.vocab_size,\n",
    "                 w2v_model.dimension,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 w2v_model.embedding_matrix)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Approach\n",
    "\n",
    "As discussed in part 1, we have mentioned 2 approaches to handling of the\n",
    "OOV words. We will now demonstrate the first approach, which is to replace the OOV\n",
    "words with a special token. We will replace the OOV words with a special token\n",
    "`<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix...\n",
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GloVe words loaded: 400000\n",
      "Embedding matrix built successfully.\n",
      "Embedding matrix saved as './result/embedding_matrix.npy'.\n",
      "Mapping 'word2idx' saved as './result/word2idx.json'.\n",
      "Mapping 'idx2word' saved as './result/idx2word.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from common_utils import EMBEDDING_DIM, EMBEDDING_MATRIX_PATH, IDX2WORD_PATH, UNK_TOKEN, WORD2IDX_PATH, load_glove_embeddings\n",
    "\n",
    "\n",
    "w2v_model = EmbeddingMatrix.load()\n",
    "extended_vocab = w2v_model.vocab\n",
    "extended_vocab.add(UNK_TOKEN)\n",
    "\n",
    "# mapping of words to indices and vice versa\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted(extended_vocab))}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size_extended = len(extended_vocab)\n",
    "embedding_matrix = np.zeros((vocab_size_extended, EMBEDDING_DIM))\n",
    "\n",
    "print(\"Building embedding matrix...\")\n",
    "\n",
    "glove_dict = load_glove_embeddings()\n",
    "\n",
    "for word in extended_vocab:\n",
    "    idx = word2idx[word]\n",
    "    # if word is in glove vocab, use glove vector\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[idx] = glove_dict[word]\n",
    "    else:\n",
    "        # use random vector for unknown words\n",
    "        if word == UNK_TOKEN:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "        else:\n",
    "            # remove the unknown word from the vocab\n",
    "            # embedding_matrix[idx] = embedding_matrix[word2idx[UNK_TOKEN]]\n",
    "            word2idx.pop(word)\n",
    "            idx2word.pop(idx)\n",
    "\n",
    "print(\"Embedding matrix built successfully.\")\n",
    "\n",
    "np.save(EMBEDDING_MATRIX_PATH, embedding_matrix)\n",
    "print(f\"Embedding matrix saved as '{EMBEDDING_MATRIX_PATH}'.\")\n",
    "\n",
    "with open(WORD2IDX_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'word2idx' saved as '{WORD2IDX_PATH}'.\")\n",
    "\n",
    "with open(IDX2WORD_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'idx2word' saved as '{IDX2WORD_PATH}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second approach, we will use the FastText embeddings, which are trained on\n",
    "subword information. This will help in encoding the OOV words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of FastText for word embedding\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Load Word2Vec model\n",
    "w2v_model = EmbeddingMatrix.load()\n",
    "\n",
    "# Create a list of words from the Word2Vec model\n",
    "words = list(w2v_model.vocab)\n",
    "\n",
    "# Create a FastText model with the same dimensions as the Word2Vec model\n",
    "fasttext_model = FastText(size=w2v_model.dimension, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Build the FastText vocabulary\n",
    "fasttext_model.build_vocab(words)\n",
    "\n",
    "# Initialize embeddings with Word2Vec embeddings\n",
    "for word in words:\n",
    "    fasttext_model.wv[word] = w2v_model[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2 BiGru Model\n",
    "\n",
    "The weight matrix will be created by BatchIterator class  based on the Glove word vectors we want to use and vocabularly word2index dictionary. For each word, index pair from word2index, the weight matrix at the position equals index will be set with the Glove vector that represents given word. \n",
    "\n",
    "For words that haven't been found in the Glove mapping, the random vector will be initialised. Then the weight matrix will be passed to the embedding layer as its parameters. \n",
    "\n",
    "It is very important to set the embedding_layer.weight.requires_grad = False, so that weights will not be updated during the training. After the GRU layer, extra features will be concatenated toehter with max-pooled, avg-pooled and last hidden size. The concatenated tensor will be next passed to the linear layer, so we have to set the input size to hidden_size * 3 + num_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1153d9210>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from common_utils import EmbeddingMatrix, UNK_TOKEN, EMBEDDING_DIM, tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec embedding matrix\n",
    "w2v_model = EmbeddingMatrix.load()\n",
    "word2idx = w2v_model.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from datasets import load_dataset \n",
    "# From Assignment PDF\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import nltk \n",
    "\n",
    "def tokenize(texts, word2idx, max_len=512):\n",
    "    tokenized = []\n",
    "    lengths = []\n",
    "    for text in texts:\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        token_ids = [word2idx.get(word, word2idx[UNK_TOKEN]) for word in tokens]\n",
    "        tokenized.append(torch.tensor(token_ids[:max_len], dtype=torch.int))\n",
    "        lengths.append(len(token_ids))\n",
    "    return tokenized, lengths\n",
    "\n",
    "def prepare_dataset(dataset: datasets.DatasetDict, batch_size=50) -> torch.utils.data:\n",
    "    set_tokenized, lengths = tokenize(dataset['text'], word2idx)\n",
    "    \n",
    "    set_tokenized = pad_sequence(set_tokenized, batch_first=True)\n",
    "\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    set_labels = torch.tensor(dataset['label'])\n",
    "\n",
    "    extra_features = torch.zeros((len(set_labels), 0), dtype=torch.float)\n",
    "\n",
    "    set_data = data.TensorDataset(set_tokenized, extra_features, lengths, set_labels)\n",
    "\n",
    "    return data.DataLoader(set_data, batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = prepare_dataset(train_dataset)\n",
    "val_loader = prepare_dataset(validation_dataset)\n",
    "test_loader = prepare_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    BiDirectional GRU neural network model with pre-trained word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    hidden_size: int \n",
    "        Number of features in the hidden state\n",
    "    vocab_size: int\n",
    "        The size of the vocabulary \n",
    "    n_extra_feat: int\n",
    "        Number of additional features\n",
    "    weight_matrix: numpy.ndarray \n",
    "        Matrix of pre-trained word embeddings (word_embedding)\n",
    "    output_size: int\n",
    "        Number of classes \n",
    "    n_layer: int, optional (default=1)\n",
    "        Number of stacked recurrent layers. \n",
    "    dropout: float, optional (default=0.2)\n",
    "        Probability of an element of the tensor to be zeroed.\n",
    "    spatial_dropout: boolean, optional (default=True)\n",
    "        Whether to use the spatial dropout. \n",
    "    bidirectional: boolean, optional (default=True)\n",
    "        Whether to use the bidirectional GRU. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, vocab_size, n_extra_feat, weights_matrix, output_size, n_layers=1, dropout=0.2, spatial_dropout=True, bidirectional=True):\n",
    "\n",
    "        # Inherit everything from the nn.Module\n",
    "        super(BiGRU, self).__init__()\n",
    "\n",
    "        # Initialise attributes\n",
    "        self.hidden_size = hidden_size \n",
    "        self.vocab_size = vocab_size \n",
    "        self.n_extra_feat = n_extra_feat\n",
    "        self.weights_matrix = weights_matrix\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout\n",
    "        self.spatial_dropout = spatial_dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_directions = 2 if self.bidirectional else 1\n",
    "\n",
    "        self.vocab_size, self.embedding_dim = self.weights_matrix.shape \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix, dtype=torch.float), freeze=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        if self.spatial_dropout:\n",
    "            self.spatial_dropout1d = nn.Dropout1d(self.dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, num_layers=self.n_layers, dropout=(0 if n_layers == 1 else self.dropout_p), batch_first=True, bidirectional=self.bidirectional)\n",
    "\n",
    "        # Linear layer input size is equal to hidden_size * 3 + n_extra_feat, becuase\n",
    "        # we will concatenate max_pooling ,avg_pooling, last hidden state and additional features\n",
    "        self.linear = nn.Linear(self.hidden_size * 3 + self.n_extra_feat, self.output_size)\n",
    "    \n",
    "    def forward(self, input_seq, input_feat, input_lengths, hidden=None):\n",
    "        \"\"\"Forward propagate through the neural network model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_seq: torch.Tensor\n",
    "            Batch of input sequences.\n",
    "        input_feat: torch.Tensor\n",
    "            Batch of additional features.\n",
    "        input_lengths: torch.LongTensor\n",
    "            Batch containing sequences lengths.\n",
    "        hidden: torch.FloatTensor, optional (default=None)\n",
    "            Tensor containing initial hidden state.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Logarithm of softmaxed input tensor.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Extract batch_size\n",
    "        self.batch_size = input_seq.size(0)\n",
    "        \n",
    "        # Embeddings shapes\n",
    "        # Input: (batch_size,  seq_length)\n",
    "        # Output: (batch_size, seq_length, embedding_dim)\n",
    "        emb_out = self.embedding(input_seq)\n",
    "        \n",
    "        if self.spatial_dropout:\n",
    "            # Convert to (batch_size, embedding_dim, seq_length)\n",
    "            emb_out = emb_out.permute(0, 2, 1)\n",
    "            emb_out = self.spatial_dropout1d(emb_out)\n",
    "            # Convert back to (batch_size, seq_length, embedding_dim)\n",
    "            emb_out = emb_out.permute(0, 2, 1)\n",
    "        else:\n",
    "            emb_out = self.dropout(emb_out)\n",
    "        \n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(emb_out, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "                \n",
    "        # GRU input/output shapes, if batch_first=True\n",
    "        # Input: (batch_size, seq_len, embedding_dim)\n",
    "        # Output: (batch_size, seq_len, hidden_size*num_directions)\n",
    "        # Number of directions = 2 when used bidirectional, otherwise 1\n",
    "        # shape of hidden: (n_layers x num_directions, batch_size, hidden_size)\n",
    "        # Hidden state defaults to zero if not provided\n",
    "        gru_out, hidden = self.gru(packed_emb, hidden)\n",
    "        # gru_out: tensor containing the output features h_t from the last layer of the GRU\n",
    "        # gru_out comprises all the hidden states in the last layer (\"last\" depth-wise, not time-wise)\n",
    "        # For biGRu gru_out is the concatenation of a forward GRU representation and a backward GRU representation\n",
    "        # hidden (h_n) comprises the hidden states after the last timestep\n",
    "        \n",
    "        # Extract and sum last hidden state\n",
    "        # Input hidden shape: (n_layers x num_directions, batch_size, hidden_size)\n",
    "        # Separate hidden state layers\n",
    "        hidden = hidden.view(self.n_layers, self.n_directions, self.batch_size, self.hidden_size)\n",
    "        last_hidden = hidden[-1]\n",
    "        # last hidden shape (num_directions, batch_size, hidden_size)\n",
    "        # Sum the last hidden state of forward and backward layer\n",
    "        last_hidden = torch.sum(last_hidden, dim=0)\n",
    "        # Summed last hidden shape (batch_size, hidden_size)\n",
    "        \n",
    "        # Pad a packed batch\n",
    "        # gru_out output shape: (batch_size, seq_len, hidden_size*num_directions)\n",
    "        gru_out, lengths = nn.utils.rnn.pad_packed_sequence(gru_out, batch_first=True)\n",
    "              \n",
    "        # Sum the gru_out along the num_directions\n",
    "        if self.bidirectional:\n",
    "            gru_out = gru_out[:,:,:self.hidden_size] + gru_out[:,:,self.hidden_size:]\n",
    "        \n",
    "        # Select the maximum value over each dimension of the hidden representation (max pooling)\n",
    "        # Permute the input tensor to dimensions: (batch_size, hidden, seq_len)\n",
    "        # Output dimensions: (batch_size, hidden_size)\n",
    "        max_pool = F.adaptive_max_pool1d(gru_out.permute(0,2,1), (1,)).view(self.batch_size,-1)\n",
    "        \n",
    "        # Consider the average of the representations (mean pooling)\n",
    "        # Sum along the batch axis and divide by the corresponding lengths (FloatTensor)\n",
    "        # Output shape: (batch_size, hidden_size)\n",
    "        avg_pool = torch.sum(gru_out, dim=1) / lengths.view(-1,1).type(torch.FloatTensor) \n",
    "\n",
    "        # Concatenate max_pooling, avg_pooling, hidden state and input_feat tensor\n",
    "        concat_out = torch.cat([last_hidden, max_pool, avg_pool, input_feat], dim=1)\n",
    "\n",
    "        # concat_out = self.dropout(concat_out)\n",
    "        out = self.linear(concat_out)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "  \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiGRU(\n",
      "  (embedding): Embedding(16164, 100)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (spatial_dropout1d): Dropout1d(p=0.5, inplace=False)\n",
      "  (gru): GRU(100, 8, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=24, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_size = 8\n",
    "vocab_size = w2v_model.vocab_size\n",
    "n_extra_feat = 0\n",
    "output_size = 2\n",
    "n_layers = 1\n",
    "dropout = 0.5 \n",
    "learning_rate = 0.001\n",
    "epochs = 40 \n",
    "spatial_dropout = True\n",
    "\n",
    "# Load the weights matrix\n",
    "weights = np.load('result/embedding_matrix.npy')\n",
    "\n",
    "# Create BiGru model \n",
    "model = BiGRU(hidden_size, vocab_size, n_extra_feat, weights, output_size, n_layers, dropout, spatial_dropout, bidirectional=True)\n",
    "print(model)\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sequences, features, lengths, labels = batch\n",
    "            \n",
    "            # Move tensors to device\n",
    "            sequences = sequences.to(device)\n",
    "            features = features.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences, features, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f'Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7028\n",
      "Epoch 2/10, Loss: 0.6665\n",
      "Epoch 3/10, Loss: 0.6006\n",
      "Epoch 4/10, Loss: 0.5205\n",
      "Epoch 5/10, Loss: 0.4502\n",
      "Epoch 6/10, Loss: 0.4020\n",
      "Epoch 7/10, Loss: 0.3617\n",
      "Epoch 8/10, Loss: 0.3246\n",
      "Epoch 9/10, Loss: 0.2845\n",
      "Epoch 10/10, Loss: 0.2520\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequences, features, lengths, labels = batch \n",
    "\n",
    "        # Move tensors to device (GPU or CPU)\n",
    "        sequences = sequences.to(device)\n",
    "        features = features.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass \n",
    "        outputs = model(sequences, features, lengths)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    average_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5055, Accuracy: 0.7824\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3. Enhancement\n",
    "(a) Report the accuracy score on the test set when the word embeddings are updated (Part 3.1).\n",
    "   \n",
    "(b) Report the accuracy score on the test set when applying your method to deal with OOV words\n",
    "in Part 3.2.\n",
    "   \n",
    "(c) Report the accuracy scores of biLSTM and biGRU on the test set (Part 3.3).\n",
    "   \n",
    "(d) Report the accuracy scores of CNN on the test set (Part 3.4).\n",
    "   \n",
    "(e) Describe your final improvement strategy in Part 3.5. Report the accuracy on the test set\n",
    "using your improved model.\n",
    "   \n",
    "(f) Compare the results across different solutions above and describe your observations with possible discussions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
