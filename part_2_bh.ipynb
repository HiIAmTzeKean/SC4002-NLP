{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Model Training & Evaluation - RNN   \n",
    "Now with the pretrained word embeddings acquired from Part 1 and the dataset acquired from\n",
    "Part 0, you need to train a deep learning model for sentiment classification using the training set,\n",
    "conforming to these requirements:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Use the pretrained word embeddings from Part 1 as inputs; do not update them during training\n",
    "(they are “frozen”).   \n",
    "\n",
    "• Design a simple recurrent neural network (RNN), taking the input word embeddings, and\n",
    "predicting a sentiment label for each sentence. To do that, you need to consider how to\n",
    "aggregate the word representations to represent a sentence.   \n",
    "\n",
    "• Use the validation set to gauge the performance of the model for each epoch during training.\n",
    "You are required to use accuracy as the performance metric during validation and evaluation. \n",
    "   \n",
    "• Use the mini-batch strategy during training. You may choose any preferred optimizer (e.g.,\n",
    "SGD, Adagrad, Adam, RMSprop). Be careful when you choose your initial learning rate and\n",
    "mini-batch size. (You should use the validation set to determine the optimal configuration.)\n",
    "Train the model until the accuracy score on the validation set is not increasing for a few\n",
    "epochs.\n",
    "   \n",
    "• Evaluate your trained model on the test dataset, observing the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We omit warnings to keep the output clean\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import load_glove_embeddings, set_seed\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = 100 # word embedding size \n",
    "HIDDEN_SIZE = 128 # just as a starter to see \n",
    "NUM_EPOCHS = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GloVe words loaded: 400000\n"
     ]
    }
   ],
   "source": [
    "# initialize word embeddings\n",
    "word_embeddings = load_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanillaRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "    super().__init__()\n",
    "    self.num_layers = num_layers \n",
    "    self.hidden_size = hidden_size \n",
    "    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # this is the num rows of the input matrix \n",
    "    self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "    output, h_t = self.rnn(x, h0) # we can just use the last hidden state output as the \n",
    "    last_hidden = h_t[-1]\n",
    "    logits = self.sigmoid(self.fc(last_hidden))\n",
    "    return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from huggingface first \n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/word2idx.json', \"r\") as file:\n",
    "    word2idx = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validate and test datasets and dataloaders \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "# TODO: change the num_tokens \n",
    "class EmbeddingsDataset(Dataset):\n",
    "  def __init__(self, X, y, num_tokens_per_sentence=12, word_embeddings=word_embeddings):\n",
    "    self.num_tokens_per_sentence = 8\n",
    "    self.word_embeddings = word_embeddings\n",
    "    self.X = X # train_dataset['text']\n",
    "    self.y = y # train_dataset['label']\n",
    "    self.len = len(self.X)\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    # tokenize the sentence \n",
    "    tokens = self.tokenize_sentence(self.X[index])\n",
    "    # convert each token to embeddings \n",
    "    sentence_tensor = self.convert_sentence_into_embedding_tensor(tokens)\n",
    "    label = torch.tensor(self.y[index], dtype=torch.float)\n",
    "    return sentence_tensor, label \n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len \n",
    "\n",
    "  def tokenize_sentence(self, x): \n",
    "    '''\n",
    "    returns a list containing the embeddings of each token \n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    return tokens \n",
    "\n",
    "  def convert_sentence_into_embedding_tensor(self, tokens):\n",
    "    sentence_embeddings = [] \n",
    "    num_tokens_used = 0\n",
    "    for i, token in enumerate(tokens): \n",
    "      if num_tokens_used == self.num_tokens_per_sentence:\n",
    "        break # we have enough of tokens from the sentence \n",
    "\n",
    "      if token in self.word_embeddings: # only use words that are in the word_embeddings matrix, otherwise skip\n",
    "        embedding_tensor = torch.tensor(self.word_embeddings[token], dtype=torch.float)\n",
    "        sentence_embeddings.append(embedding_tensor)\n",
    "      num_tokens_used += 1 \n",
    "\n",
    "    # if not enough tokens in the sentence, pad with zero tensors \n",
    "    if len(sentence_embeddings) < self.num_tokens_per_sentence:\n",
    "            # Padding with zero vectors if less than 8 embeddings\n",
    "            padding = [torch.zeros(EMBEDDING_DIM) for _ in range(self.num_tokens_per_sentence - len(sentence_embeddings))]\n",
    "            sentence_embeddings.extend(padding)\n",
    "\n",
    "    sentence_tensor = torch.stack(sentence_embeddings)\n",
    "    return sentence_tensor \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ed = EmbeddingsDataset(train_dataset['text'], train_dataset['label'])\n",
    "validation_dataset_ed = EmbeddingsDataset(validation_dataset['text'], validation_dataset['label'])\n",
    "test_dataset_ed = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "\n",
    "\n",
    "# implement minibatch training \n",
    "train_dataloader = DataLoader(train_dataset_ed, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# prepare validation and test dataloaders \n",
    "validation_dataloader = DataLoader(validation_dataset_ed, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset_ed, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embeddings['the'].shape # get the size of the embeddings, so that we can use this as input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "    size = len(train_dataloader.dataset)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch_no, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        X_batch = torch.tensor(X_batch, dtype=torch.float)\n",
    "        y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        pred = pred.squeeze(1)\n",
    "        y_val = y_batch\n",
    "        loss = loss_fn(pred, y_val)\n",
    "        train_loss += loss.item() \n",
    "        train_correct += ((pred >= 0.5).float()==y_batch).sum().item() \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= num_batches \n",
    "    train_correct /= size \n",
    "\n",
    "    return train_loss, train_correct \n",
    "   \n",
    "\n",
    "def test_loop(validate_dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(validate_dataloader)\n",
    "    size = len(validate_dataloader.dataset)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in validate_dataloader:\n",
    "            X_batch = torch.tensor(X_batch, dtype=torch.float)\n",
    "            y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
    "\n",
    "            pred = model(X_batch)\n",
    "            pred = pred.squeeze(1)\n",
    "            pred_binary = (pred >= 0.5).float()\n",
    "            test_loss += loss_fn(pred, y_batch).item()\n",
    "            test_correct += (pred_binary == y_batch).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    return test_loss, test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vanillaRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initialize the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m RNN_model \u001b[38;5;241m=\u001b[39m vanillaRNN(input_size\u001b[38;5;241m=\u001b[39mINPUT_SIZE, hidden_size\u001b[38;5;241m=\u001b[39mHIDDEN_SIZE, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m      3\u001b[0m   \u001b[38;5;66;03m# num_classes==1 because binary classification\u001b[39;00m\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;66;03m# TODO: can try to increase num_layers > 1, that might perform better \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# initialize training params \u001b[39;00m\n\u001b[1;32m      6\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(RNN_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vanillaRNN' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "RNN_model = vanillaRNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=4, num_classes=1) \n",
    "  # num_classes==1 because binary classification\n",
    "  # TODO: can try to increase num_layers > 1, that might perform better \n",
    "# initialize training params \n",
    "optim = torch.optim.Adam(RNN_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model, just using epoch = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sx/8nz4dts119bfc1svlcbykndr0000gn/T/ipykernel_2892/987989442.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch, dtype=torch.float)\n",
      "/var/folders/sx/8nz4dts119bfc1svlcbykndr0000gn/T/ipykernel_2892/987989442.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "/var/folders/sx/8nz4dts119bfc1svlcbykndr0000gn/T/ipykernel_2892/987989442.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch, dtype=torch.float)\n",
      "/var/folders/sx/8nz4dts119bfc1svlcbykndr0000gn/T/ipykernel_2892/987989442.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \tValidation Acc:0.6294559099437148 \tTrain Acc:0.6021101992966003\n",
      "Epoch:11 \tValidation Acc:0.6857410881801126 \tTrain Acc:0.7083235638921453\n",
      "Epoch:21 \tValidation Acc:0.6791744840525328 \tTrain Acc:0.8248534583821805\n",
      "Epoch:31 \tValidation Acc:0.6575984990619137 \tTrain Acc:0.9331770222743259\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m----> 4\u001b[0m   train_loss, train_correct \u001b[38;5;241m=\u001b[39m train_loop(train_dataloader, RNN_model, criterion, optim) \n\u001b[1;32m      5\u001b[0m   validate_loss, validate_correct \u001b[38;5;241m=\u001b[39m test_loop(validation_dataloader, RNN_model, criterion)\n\u001b[1;32m      6\u001b[0m   validation_acc\u001b[38;5;241m.\u001b[39mappend(validate_correct)\n",
      "Cell \u001b[0;32mIn[105], line 10\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_batch, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m     11\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m y_val \u001b[38;5;241m=\u001b[39m y_batch\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m, in \u001b[0;36mvanillaRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m   h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 12\u001b[0m   output, h_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, h0) \u001b[38;5;66;03m# we can just use the last hidden state output as the \u001b[39;00m\n\u001b[1;32m     13\u001b[0m   last_hidden \u001b[38;5;241m=\u001b[39m h_t[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m   logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(last_hidden))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/torch/nn/modules/rnn.py:586\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 586\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_tanh(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    587\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    588\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    591\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    592\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_acc = [] \n",
    "train_acc = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "  train_loss, train_correct = train_loop(train_dataloader, RNN_model, criterion, optim) \n",
    "  validate_loss, validate_correct = test_loop(validation_dataloader, RNN_model, criterion)\n",
    "  validation_acc.append(validate_correct)\n",
    "  train_acc.append(train_correct)\n",
    "  if i%10 == 0:\n",
    "    print(f\"Epoch:{i+1} \\tValidation Acc:{validate_correct} \\tTrain Acc:{train_correct}\")\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_acc, label=\"train acc\")\n",
    "plt.plot(validation_acc, label=\"validation acc\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracies\")\n",
    "plt.title(\"train vs validation accs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "We will perform grid search on the no. of training epochs, lr, optimizer and batch sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. RNN\n",
    "(a) Report the final configuration of your best model, namely the number of training epochs,\n",
    "learning rate, optimizer, batch size.\n",
    "  - Implement Bayesian search\n",
    "(b) Report the accuracy score on the test set, as well as the accuracy score on the validation\n",
    "set for each epoch during training.\n",
    "(c) RNNs produce a hidden vector for each word, instead of the entire sentence. Which methods\n",
    "have you tried in deriving the final sentence representation to perform sentiment classification?\n",
    "Describe all the strategies you have implemented, together with their accuracy scores on the\n",
    "test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding with \"the\" token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use \"the\" as pad \n",
    "word2idx[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = [word_embeddings[word] for word in word_embeddings.keys()]\n",
    "\n",
    "embedding_matrix_np = np.array(embeddings)\n",
    "#embedding_matrix_np = np.vstack((np.zeros((1, 100)), embedding_matrix_np))\n",
    "\n",
    "embedding_matrix_np.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from huggingface first \n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "with open('result/word2idx.json', \"r\") as file:\n",
    "    word2idx = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop2(train_dataloader, model, loss_fn, optimizer):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "    size = len(train_dataloader.dataset)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch_no, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        y_batch = y_batch.long()\n",
    "        if train_on_gpu:\n",
    "            X_batch = X_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "\n",
    "        print(pred, y_batch)\n",
    "        loss = loss_fn(pred.squeeze(), y_batch.float())\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        #probabilities = pred.squeeze()\n",
    "        pred_binary = (pred.squeeze() >= 0.5).long()\n",
    "        train_correct += (pred_binary == y_batch.long()).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= num_batches \n",
    "    train_correct /= size \n",
    "\n",
    "    return train_loss, train_correct \n",
    "   \n",
    "\n",
    "def test_loop2(validate_dataloader, model, loss_fn):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    num_batches = len(validate_dataloader)\n",
    "    size = len(validate_dataloader.dataset)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in validate_dataloader:\n",
    "            y_batch = y_batch.long()\n",
    "            if train_on_gpu:\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            pred = model(X_batch)\n",
    "            \n",
    "            test_loss += loss_fn(pred.squeeze(), y_batch.float()).item()\n",
    "            \n",
    "            #probabilities = torch.sigmoid(pred.squeeze())\n",
    "            pred_binary = (pred.squeeze() >= 0.5).long()\n",
    "            test_correct += (pred_binary == y_batch.long()).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    return test_loss, test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "    size = len(train_dataloader.dataset)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch_no, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        y_batch = y_batch.long()\n",
    "        if train_on_gpu:\n",
    "            X_batch = X_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        pred = pred.squeeze(1)\n",
    "        y_val = y_batch\n",
    "        loss = loss_fn(pred, y_val.float())\n",
    "        train_loss += loss.item() \n",
    "        train_correct += ((pred >= 0.5).long()==y_batch).sum().item() \n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= num_batches \n",
    "    train_correct /= size \n",
    "\n",
    "    return train_loss, train_correct \n",
    "   \n",
    "\n",
    "def test_loop(validate_dataloader, model, loss_fn):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    num_batches = len(validate_dataloader)\n",
    "    size = len(validate_dataloader.dataset)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in validate_dataloader:\n",
    "            y_batch = y_batch.long()\n",
    "            if train_on_gpu:\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            pred = model(X_batch)\n",
    "            pred = pred.squeeze(1)\n",
    "            pred_binary = (pred >= 0.5).long()\n",
    "            test_loss += loss_fn(pred, y_batch.float()).item()\n",
    "            test_correct += (pred_binary == y_batch).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    return test_loss, test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "# TODO: change the num_tokens \n",
    "class EmbeddingsDataset2(Dataset):\n",
    "  def __init__(self, X, y, num_tokens_per_sentence=8, word_embeddings=word_embeddings):\n",
    "    self.num_tokens_per_sentence = num_tokens_per_sentence\n",
    "    self.word_embeddings = word_embeddings\n",
    "    self.X = X # train_dataset['text']\n",
    "    self.y = y # train_dataset['label']\n",
    "    self.len = len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # tokenize the sentence \n",
    "    tokens = self.tokenize_sentence(self.X[index])\n",
    "    # convert each token to embeddings \n",
    "    sentence_tensor = self.convert_sentence_into_indices(tokens)\n",
    "    label = torch.tensor(self.y[index], dtype=torch.long)\n",
    "    return sentence_tensor, label \n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len \n",
    "\n",
    "  def tokenize_sentence(self, x): \n",
    "    '''\n",
    "    returns a list containing the embeddings of each token \n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(x.lower())\n",
    "    return tokens \n",
    "  \n",
    "  def convert_sentence_into_indices(self, tokens):\n",
    "    indices = []\n",
    "    num_tokens_used = 0 \n",
    "    for token in tokens:\n",
    "      if num_tokens_used == self.num_tokens_per_sentence:\n",
    "        break # we have enough of tokens from the sentence \n",
    "      if token in word2idx:\n",
    "        indices.append(word2idx[token])\n",
    "        num_tokens_used += 1 \n",
    "    # # if not enough tokens in the sentence, use index of ?? \n",
    "    if len(indices) < self.num_tokens_per_sentence:\n",
    "      padding = [0 for _ in range(self.num_tokens_per_sentence - len(indices))]\n",
    "      indices.extend(padding)\n",
    "    #print(indices)\n",
    "    indices = torch.tensor(indices, dtype=torch.long)\n",
    "    return indices\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNN with \\<pad\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Embeddings \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class VanillaRNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix_torch=torch.tensor(embedding_matrix_np, dtype=torch.float)):\n",
    "        super(VanillaRNNWithEmbedding, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_torch, freeze=True)\n",
    "        self.num_layers = num_layers \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3) # this is the num rows of the input matrix \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, dtype=torch.float).to(x.device)\n",
    "        # Pass the embeddings through the RNN layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Max pooling\n",
    "        out, _ = torch.max(out, dim=1)\n",
    "        # Only take the last output for each sequence\n",
    "        #out = out[:, -1, :]\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        # Apply sigmoid activation (for binary classification)\n",
    "        #out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Embeddings \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class VanillaRNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix_torch=torch.tensor(embedding_matrix_np, dtype=torch.float)):\n",
    "        super(VanillaRNNWithEmbedding, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_torch, freeze=True)\n",
    "        self.num_layers = num_layers \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3) # this is the num rows of the input matrix \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, dtype=torch.float).to(x.device)\n",
    "        # Pass the embeddings through the RNN layer\n",
    "        out, hidden = self.rnn(x, h0)\n",
    "        # Max pooling\n",
    "        res, _ = torch.max(hidden, dim=1)\n",
    "        # Only take the last output for each sequence\n",
    "        #out = out[:, -1, :]\n",
    "        # Pass through the fully connected layer\n",
    "        res = self.fc(res)\n",
    "        # Apply sigmoid activation (for binary classification)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1885],\n",
      "        [-0.1809]], device='cuda:0', grad_fn=<AddmmBackward0>) tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32])) must be the same as input size (torch.Size([2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m train_losses, validate_losses \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m---> 19\u001b[0m   train_loss, train_correct \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRNN_embeddings_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     20\u001b[0m   validate_loss, validate_correct \u001b[38;5;241m=\u001b[39m test_loop2(validation_dataloader2, RNN_embeddings_model, criterion)\n\u001b[0;32m     21\u001b[0m   validation_acc\u001b[38;5;241m.\u001b[39mappend(validate_correct)\n",
      "Cell \u001b[1;32mIn[75], line 18\u001b[0m, in \u001b[0;36mtrain_loop2\u001b[1;34m(train_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     15\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred, y_batch)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#probabilities = pred.squeeze()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:819\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:3624\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3626\u001b[0m     )\n\u001b[0;32m   3628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[0;32m   3629\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[0;32m   3630\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([32])) must be the same as input size (torch.Size([2]))"
     ]
    }
   ],
   "source": [
    "RNN_embeddings_model = VanillaRNNWithEmbedding(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=2, num_classes=1)\n",
    "optim = torch.optim.SGD(RNN_embeddings_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "train_dataset_ed2 = EmbeddingsDataset2(train_dataset['text'], train_dataset['label'], num_tokens_per_sentence=25)\n",
    "validation_dataset_ed2 = EmbeddingsDataset2(validation_dataset['text'], validation_dataset['label'], num_tokens_per_sentence=25)\n",
    "# test_dataset_ed2 = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "\n",
    "train_dataloader2 = DataLoader(train_dataset_ed2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader2 = DataLoader(validation_dataset_ed2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "validation_acc = [] \n",
    "train_acc = []\n",
    "train_losses, validate_losses = [], []\n",
    "for i in range(NUM_EPOCHS):\n",
    "  train_loss, train_correct = train_loop2(train_dataloader2, RNN_embeddings_model, criterion, optim) \n",
    "  validate_loss, validate_correct = test_loop2(validation_dataloader2, RNN_embeddings_model, criterion)\n",
    "  validation_acc.append(validate_correct)\n",
    "  train_acc.append(train_correct)\n",
    "  train_losses.append(train_loss)\n",
    "  validate_losses.append(validate_loss)\n",
    "\n",
    "  print(f\"Epoch {i+1}, Train Loss: {train_loss:.4f}, Validate Loss: {validate_loss:.4f}\")\n",
    "  #if i%10 == 0:\n",
    "  print(f\"Epoch:{i+1} \\tValidation Acc:{validate_correct} \\tTrain Acc:{train_correct}\")\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6951, Validate Loss: 0.6895\n",
      "Epoch:1 \tValidation Acc:0.5619136960600375 \tTrain Acc:0.5073856975381008\n",
      "Epoch 2, Train Loss: 0.6906, Validate Loss: 0.6854\n",
      "Epoch:2 \tValidation Acc:0.5412757973733584 \tTrain Acc:0.5262602579132474\n",
      "Epoch 3, Train Loss: 0.6844, Validate Loss: 0.6773\n",
      "Epoch:3 \tValidation Acc:0.5881801125703565 \tTrain Acc:0.5610785463071513\n",
      "Epoch 4, Train Loss: 0.6770, Validate Loss: 0.6703\n",
      "Epoch:4 \tValidation Acc:0.5956848030018762 \tTrain Acc:0.5722157092614303\n",
      "Epoch 5, Train Loss: 0.6722, Validate Loss: 0.6682\n",
      "Epoch:5 \tValidation Acc:0.6031894934333959 \tTrain Acc:0.5835873388042204\n",
      "Epoch 6, Train Loss: 0.6673, Validate Loss: 0.6689\n",
      "Epoch:6 \tValidation Acc:0.5975609756097561 \tTrain Acc:0.5951934349355217\n",
      "Epoch 7, Train Loss: 0.6653, Validate Loss: 0.6681\n",
      "Epoch:7 \tValidation Acc:0.5928705440900562 \tTrain Acc:0.5984759671746777\n",
      "Epoch 8, Train Loss: 0.6623, Validate Loss: 0.6761\n",
      "Epoch:8 \tValidation Acc:0.5675422138836773 \tTrain Acc:0.6026963657678781\n",
      "Epoch 9, Train Loss: 0.6605, Validate Loss: 0.6606\n",
      "Epoch:9 \tValidation Acc:0.6116322701688556 \tTrain Acc:0.6073856975381008\n",
      "Epoch 10, Train Loss: 0.6577, Validate Loss: 0.6623\n",
      "Epoch:10 \tValidation Acc:0.6060037523452158 \tTrain Acc:0.6076201641266119\n",
      "Epoch 11, Train Loss: 0.6548, Validate Loss: 0.6678\n",
      "Epoch:11 \tValidation Acc:0.6050656660412758 \tTrain Acc:0.6153575615474794\n",
      "Epoch 12, Train Loss: 0.6537, Validate Loss: 0.6653\n",
      "Epoch:12 \tValidation Acc:0.6031894934333959 \tTrain Acc:0.6187573270808909\n",
      "Epoch 13, Train Loss: 0.6509, Validate Loss: 0.6609\n",
      "Epoch:13 \tValidation Acc:0.6116322701688556 \tTrain Acc:0.6228604923798359\n",
      "Epoch 14, Train Loss: 0.6463, Validate Loss: 0.6634\n",
      "Epoch:14 \tValidation Acc:0.6013133208255159 \tTrain Acc:0.6266119577960141\n",
      "Epoch 15, Train Loss: 0.6433, Validate Loss: 0.6617\n",
      "Epoch:15 \tValidation Acc:0.6078799249530957 \tTrain Acc:0.6328253223915592\n",
      "Epoch 16, Train Loss: 0.6419, Validate Loss: 0.6584\n",
      "Epoch:16 \tValidation Acc:0.6088180112570356 \tTrain Acc:0.6329425556858148\n",
      "Epoch 17, Train Loss: 0.6378, Validate Loss: 0.6561\n",
      "Epoch:17 \tValidation Acc:0.6106941838649156 \tTrain Acc:0.6406799531066822\n",
      "Epoch 18, Train Loss: 0.6333, Validate Loss: 0.6617\n",
      "Epoch:18 \tValidation Acc:0.5881801125703565 \tTrain Acc:0.6411488862837046\n",
      "Epoch 19, Train Loss: 0.6294, Validate Loss: 0.6626\n",
      "Epoch:19 \tValidation Acc:0.6050656660412758 \tTrain Acc:0.6438452520515826\n",
      "Epoch 20, Train Loss: 0.6274, Validate Loss: 0.6610\n",
      "Epoch:20 \tValidation Acc:0.6106941838649156 \tTrain Acc:0.6495896834701055\n",
      "Epoch 21, Train Loss: 0.6231, Validate Loss: 0.6608\n",
      "Epoch:21 \tValidation Acc:0.6050656660412758 \tTrain Acc:0.6506447831184057\n",
      "Epoch 22, Train Loss: 0.6189, Validate Loss: 0.6582\n",
      "Epoch:22 \tValidation Acc:0.6022514071294559 \tTrain Acc:0.6599062133645955\n",
      "Epoch 23, Train Loss: 0.6148, Validate Loss: 0.6657\n",
      "Epoch:23 \tValidation Acc:0.5938086303939962 \tTrain Acc:0.6609613130128956\n",
      "Epoch 24, Train Loss: 0.6091, Validate Loss: 0.6650\n",
      "Epoch:24 \tValidation Acc:0.6013133208255159 \tTrain Acc:0.6671746776084408\n",
      "Epoch 25, Train Loss: 0.6077, Validate Loss: 0.6572\n",
      "Epoch:25 \tValidation Acc:0.6088180112570356 \tTrain Acc:0.6681125439624853\n",
      "Epoch 26, Train Loss: 0.6015, Validate Loss: 0.6622\n",
      "Epoch:26 \tValidation Acc:0.6144465290806754 \tTrain Acc:0.6726846424384525\n",
      "Epoch 27, Train Loss: 0.5947, Validate Loss: 0.6589\n",
      "Epoch:27 \tValidation Acc:0.6125703564727955 \tTrain Acc:0.6833528722157093\n",
      "Epoch 28, Train Loss: 0.5903, Validate Loss: 0.6699\n",
      "Epoch:28 \tValidation Acc:0.6135084427767354 \tTrain Acc:0.6849941383352872\n",
      "Epoch 29, Train Loss: 0.5877, Validate Loss: 0.6695\n",
      "Epoch:29 \tValidation Acc:0.6135084427767354 \tTrain Acc:0.6887456037514654\n",
      "Epoch 30, Train Loss: 0.5849, Validate Loss: 0.6637\n",
      "Epoch:30 \tValidation Acc:0.6116322701688556 \tTrain Acc:0.6879249706916765\n",
      "Epoch 31, Train Loss: 0.5801, Validate Loss: 0.6778\n",
      "Epoch:31 \tValidation Acc:0.6116322701688556 \tTrain Acc:0.6978898007033998\n",
      "Epoch 32, Train Loss: 0.5766, Validate Loss: 0.6678\n",
      "Epoch:32 \tValidation Acc:0.6088180112570356 \tTrain Acc:0.6967174677608441\n",
      "Epoch 33, Train Loss: 0.5710, Validate Loss: 0.6815\n",
      "Epoch:33 \tValidation Acc:0.599437148217636 \tTrain Acc:0.7082063305978898\n",
      "Epoch 34, Train Loss: 0.5671, Validate Loss: 0.6740\n",
      "Epoch:34 \tValidation Acc:0.6210131332082551 \tTrain Acc:0.7023446658851114\n",
      "Epoch 35, Train Loss: 0.5631, Validate Loss: 0.6863\n",
      "Epoch:35 \tValidation Acc:0.6088180112570356 \tTrain Acc:0.7087924970691677\n",
      "Epoch 36, Train Loss: 0.5611, Validate Loss: 0.6755\n",
      "Epoch:36 \tValidation Acc:0.6125703564727955 \tTrain Acc:0.7134818288393904\n",
      "Epoch 37, Train Loss: 0.5556, Validate Loss: 0.6852\n",
      "Epoch:37 \tValidation Acc:0.6135084427767354 \tTrain Acc:0.7174677608440797\n",
      "Epoch 38, Train Loss: 0.5503, Validate Loss: 0.6930\n",
      "Epoch:38 \tValidation Acc:0.6144465290806754 \tTrain Acc:0.7172332942555686\n",
      "Epoch 39, Train Loss: 0.5482, Validate Loss: 0.6908\n",
      "Epoch:39 \tValidation Acc:0.6078799249530957 \tTrain Acc:0.7184056271981243\n",
      "Epoch 40, Train Loss: 0.5417, Validate Loss: 0.6958\n",
      "Epoch:40 \tValidation Acc:0.6106941838649156 \tTrain Acc:0.7304806565064478\n",
      "Epoch 41, Train Loss: 0.5386, Validate Loss: 0.6882\n",
      "Epoch:41 \tValidation Acc:0.6097560975609756 \tTrain Acc:0.7283704572098476\n",
      "Epoch 42, Train Loss: 0.5333, Validate Loss: 0.7040\n",
      "Epoch:42 \tValidation Acc:0.6088180112570356 \tTrain Acc:0.7343493552168816\n",
      "Epoch 43, Train Loss: 0.5315, Validate Loss: 0.7025\n",
      "Epoch:43 \tValidation Acc:0.6144465290806754 \tTrain Acc:0.7365767878077374\n",
      "Epoch 44, Train Loss: 0.5242, Validate Loss: 0.6984\n",
      "Epoch:44 \tValidation Acc:0.6097560975609756 \tTrain Acc:0.7400937866354045\n",
      "Epoch 45, Train Loss: 0.5212, Validate Loss: 0.7070\n",
      "Epoch:45 \tValidation Acc:0.6144465290806754 \tTrain Acc:0.7429073856975381\n",
      "Epoch 46, Train Loss: 0.5189, Validate Loss: 0.7336\n",
      "Epoch:46 \tValidation Acc:0.5881801125703565 \tTrain Acc:0.7439624853458382\n",
      "Epoch 47, Train Loss: 0.5157, Validate Loss: 0.7136\n",
      "Epoch:47 \tValidation Acc:0.6125703564727955 \tTrain Acc:0.7444314185228605\n",
      "Epoch 48, Train Loss: 0.5065, Validate Loss: 0.7163\n",
      "Epoch:48 \tValidation Acc:0.6041275797373359 \tTrain Acc:0.7532239155920282\n",
      "Epoch 49, Train Loss: 0.5060, Validate Loss: 0.7265\n",
      "Epoch:49 \tValidation Acc:0.6022514071294559 \tTrain Acc:0.7497069167643611\n",
      "Epoch 50, Train Loss: 0.5057, Validate Loss: 0.7219\n",
      "Epoch:50 \tValidation Acc:0.5956848030018762 \tTrain Acc:0.7552168815943728\n",
      "Epoch 51, Train Loss: 0.4988, Validate Loss: 0.7300\n",
      "Epoch:51 \tValidation Acc:0.5909943714821764 \tTrain Acc:0.7539273153575615\n",
      "Epoch 52, Train Loss: 0.4923, Validate Loss: 0.7284\n",
      "Epoch:52 \tValidation Acc:0.6210131332082551 \tTrain Acc:0.7626025791324736\n",
      "Epoch 53, Train Loss: 0.4931, Validate Loss: 0.7313\n",
      "Epoch:53 \tValidation Acc:0.6125703564727955 \tTrain Acc:0.7608440797186401\n",
      "Epoch 54, Train Loss: 0.4868, Validate Loss: 0.7236\n",
      "Epoch:54 \tValidation Acc:0.6181988742964353 \tTrain Acc:0.7627198124267291\n",
      "Epoch 55, Train Loss: 0.4820, Validate Loss: 0.7454\n",
      "Epoch:55 \tValidation Acc:0.599437148217636 \tTrain Acc:0.763305978898007\n",
      "Epoch 56, Train Loss: 0.4796, Validate Loss: 0.7451\n",
      "Epoch:56 \tValidation Acc:0.6060037523452158 \tTrain Acc:0.7661195779601406\n",
      "Epoch 57, Train Loss: 0.4737, Validate Loss: 0.7529\n",
      "Epoch:57 \tValidation Acc:0.6041275797373359 \tTrain Acc:0.7699882766705745\n",
      "Epoch 58, Train Loss: 0.4674, Validate Loss: 0.7800\n",
      "Epoch:58 \tValidation Acc:0.6031894934333959 \tTrain Acc:0.782883939038687\n",
      "Epoch 59, Train Loss: 0.4652, Validate Loss: 0.7681\n",
      "Epoch:59 \tValidation Acc:0.6097560975609756 \tTrain Acc:0.7813599062133646\n",
      "Epoch 60, Train Loss: 0.4586, Validate Loss: 0.7579\n",
      "Epoch:60 \tValidation Acc:0.6125703564727955 \tTrain Acc:0.7865181711606096\n",
      "Epoch 61, Train Loss: 0.4549, Validate Loss: 0.7769\n",
      "Epoch:61 \tValidation Acc:0.5891181988742964 \tTrain Acc:0.7861664712778429\n",
      "Epoch 62, Train Loss: 0.4561, Validate Loss: 0.7742\n",
      "Epoch:62 \tValidation Acc:0.6069418386491557 \tTrain Acc:0.7874560375146542\n",
      "Epoch 63, Train Loss: 0.4550, Validate Loss: 0.7854\n",
      "Epoch:63 \tValidation Acc:0.5984990619136961 \tTrain Acc:0.7852286049237983\n",
      "Epoch 64, Train Loss: 0.4437, Validate Loss: 0.7720\n",
      "Epoch:64 \tValidation Acc:0.6013133208255159 \tTrain Acc:0.7927315357561547\n",
      "Epoch 65, Train Loss: 0.4443, Validate Loss: 0.7832\n",
      "Epoch:65 \tValidation Acc:0.6041275797373359 \tTrain Acc:0.7900351699882767\n",
      "Epoch 66, Train Loss: 0.4373, Validate Loss: 0.7954\n",
      "Epoch:66 \tValidation Acc:0.5881801125703565 \tTrain Acc:0.7974208675263775\n",
      "Epoch 67, Train Loss: 0.4329, Validate Loss: 0.8136\n",
      "Epoch:67 \tValidation Acc:0.5975609756097561 \tTrain Acc:0.8028135990621337\n",
      "Epoch 68, Train Loss: 0.4303, Validate Loss: 0.8067\n",
      "Epoch:68 \tValidation Acc:0.6022514071294559 \tTrain Acc:0.8038686987104338\n",
      "Epoch 69, Train Loss: 0.4233, Validate Loss: 0.8333\n",
      "Epoch:69 \tValidation Acc:0.6097560975609756 \tTrain Acc:0.7980070339976554\n",
      "Epoch 70, Train Loss: 0.4167, Validate Loss: 0.8366\n",
      "Epoch:70 \tValidation Acc:0.5966228893058161 \tTrain Acc:0.8106682297772567\n",
      "Epoch 71, Train Loss: 0.4159, Validate Loss: 0.8341\n",
      "Epoch:71 \tValidation Acc:0.6041275797373359 \tTrain Acc:0.8138335287221571\n",
      "Epoch 72, Train Loss: 0.4075, Validate Loss: 0.8316\n",
      "Epoch:72 \tValidation Acc:0.5816135084427767 \tTrain Acc:0.8141852286049238\n",
      "Epoch 73, Train Loss: 0.4060, Validate Loss: 0.8358\n",
      "Epoch:73 \tValidation Acc:0.6050656660412758 \tTrain Acc:0.8200468933177022\n",
      "Epoch 74, Train Loss: 0.4029, Validate Loss: 0.8669\n",
      "Epoch:74 \tValidation Acc:0.5834896810506567 \tTrain Acc:0.8212192262602579\n",
      "Epoch 75, Train Loss: 0.4025, Validate Loss: 0.8522\n",
      "Epoch:75 \tValidation Acc:0.5900562851782364 \tTrain Acc:0.82063305978898\n",
      "Epoch 76, Train Loss: 0.3926, Validate Loss: 0.8954\n",
      "Epoch:76 \tValidation Acc:0.5797373358348968 \tTrain Acc:0.8213364595545135\n",
      "Epoch 77, Train Loss: 0.3924, Validate Loss: 0.8926\n",
      "Epoch:77 \tValidation Acc:0.5909943714821764 \tTrain Acc:0.81957796014068\n",
      "Epoch 78, Train Loss: 0.3859, Validate Loss: 0.8873\n",
      "Epoch:78 \tValidation Acc:0.599437148217636 \tTrain Acc:0.8233294255568582\n",
      "Epoch 79, Train Loss: 0.3832, Validate Loss: 0.9299\n",
      "Epoch:79 \tValidation Acc:0.5919324577861164 \tTrain Acc:0.826611957796014\n",
      "Epoch 80, Train Loss: 0.3830, Validate Loss: 0.9029\n",
      "Epoch:80 \tValidation Acc:0.5947467166979362 \tTrain Acc:0.8273153575615475\n",
      "Epoch 81, Train Loss: 0.3781, Validate Loss: 0.9181\n",
      "Epoch:81 \tValidation Acc:0.5984990619136961 \tTrain Acc:0.8310668229777257\n",
      "Epoch 82, Train Loss: 0.3699, Validate Loss: 0.9081\n",
      "Epoch:82 \tValidation Acc:0.5909943714821764 \tTrain Acc:0.8348182883939038\n",
      "Epoch 83, Train Loss: 0.3709, Validate Loss: 0.9065\n",
      "Epoch:83 \tValidation Acc:0.5863039399624765 \tTrain Acc:0.8330597889800704\n",
      "Epoch 84, Train Loss: 0.3622, Validate Loss: 0.9624\n",
      "Epoch:84 \tValidation Acc:0.5872420262664165 \tTrain Acc:0.8364595545134819\n",
      "Epoch 85, Train Loss: 0.3581, Validate Loss: 0.9230\n",
      "Epoch:85 \tValidation Acc:0.5900562851782364 \tTrain Acc:0.8458382180539273\n",
      "Epoch 86, Train Loss: 0.3531, Validate Loss: 0.9154\n",
      "Epoch:86 \tValidation Acc:0.5956848030018762 \tTrain Acc:0.8471277842907385\n",
      "Epoch 87, Train Loss: 0.3480, Validate Loss: 0.9085\n",
      "Epoch:87 \tValidation Acc:0.5900562851782364 \tTrain Acc:0.8475967174677609\n",
      "Epoch 88, Train Loss: 0.3481, Validate Loss: 0.9535\n",
      "Epoch:88 \tValidation Acc:0.6013133208255159 \tTrain Acc:0.845369284876905\n",
      "Epoch 89, Train Loss: 0.3390, Validate Loss: 1.0026\n",
      "Epoch:89 \tValidation Acc:0.5844277673545967 \tTrain Acc:0.8474794841735053\n",
      "Epoch 90, Train Loss: 0.3401, Validate Loss: 1.0012\n",
      "Epoch:90 \tValidation Acc:0.5928705440900562 \tTrain Acc:0.8485345838218054\n",
      "Epoch 91, Train Loss: 0.3371, Validate Loss: 1.0113\n",
      "Epoch:91 \tValidation Acc:0.5797373358348968 \tTrain Acc:0.8498241500586167\n",
      "Epoch 92, Train Loss: 0.3346, Validate Loss: 1.0142\n",
      "Epoch:92 \tValidation Acc:0.5881801125703565 \tTrain Acc:0.8542790152403282\n",
      "Epoch 93, Train Loss: 0.3234, Validate Loss: 0.9915\n",
      "Epoch:93 \tValidation Acc:0.5900562851782364 \tTrain Acc:0.8637749120750293\n",
      "Epoch 94, Train Loss: 0.3237, Validate Loss: 1.0179\n",
      "Epoch:94 \tValidation Acc:0.6022514071294559 \tTrain Acc:0.8579132473622508\n",
      "Epoch 95, Train Loss: 0.3180, Validate Loss: 1.0886\n",
      "Epoch:95 \tValidation Acc:0.5797373358348968 \tTrain Acc:0.8623681125439625\n",
      "Epoch 96, Train Loss: 0.3150, Validate Loss: 1.0471\n",
      "Epoch:96 \tValidation Acc:0.5975609756097561 \tTrain Acc:0.865767878077374\n",
      "Epoch 97, Train Loss: 0.3060, Validate Loss: 1.0533\n",
      "Epoch:97 \tValidation Acc:0.5769230769230769 \tTrain Acc:0.8670574443141852\n",
      "Epoch 98, Train Loss: 0.3029, Validate Loss: 1.0706\n",
      "Epoch:98 \tValidation Acc:0.5872420262664165 \tTrain Acc:0.87010550996483\n",
      "Epoch 99, Train Loss: 0.3014, Validate Loss: 1.1181\n",
      "Epoch:99 \tValidation Acc:0.5816135084427767 \tTrain Acc:0.8688159437280187\n",
      "Epoch 100, Train Loss: 0.2990, Validate Loss: 1.0939\n",
      "Epoch:100 \tValidation Acc:0.5872420262664165 \tTrain Acc:0.8732708089097304\n"
     ]
    }
   ],
   "source": [
    "RNN_embeddings_model = VanillaRNNWithEmbedding(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=2, num_classes=1)\n",
    "optim = torch.optim.Adam(RNN_embeddings_model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "train_dataset_ed2 = EmbeddingsDataset2(train_dataset['text'], train_dataset['label'], num_tokens_per_sentence=25)\n",
    "validation_dataset_ed2 = EmbeddingsDataset2(validation_dataset['text'], validation_dataset['label'], num_tokens_per_sentence=25)\n",
    "# test_dataset_ed2 = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "\n",
    "train_dataloader2 = DataLoader(train_dataset_ed2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader2 = DataLoader(validation_dataset_ed2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "validation_acc = [] \n",
    "train_acc = []\n",
    "train_losses, validate_losses = [], []\n",
    "for i in range(NUM_EPOCHS):\n",
    "  train_loss, train_correct = train_loop2(train_dataloader2, RNN_embeddings_model, criterion, optim) \n",
    "  validate_loss, validate_correct = test_loop2(validation_dataloader2, RNN_embeddings_model, criterion)\n",
    "  validation_acc.append(validate_correct)\n",
    "  train_acc.append(train_correct)\n",
    "  train_losses.append(train_loss)\n",
    "  validate_losses.append(validate_loss)\n",
    "\n",
    "  print(f\"Epoch {i+1}, Train Loss: {train_loss:.4f}, Validate Loss: {validate_loss:.4f}\")\n",
    "  #if i%10 == 0:\n",
    "  print(f\"Epoch:{i+1} \\tValidation Acc:{validate_correct} \\tTrain Acc:{train_correct}\")\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_acc, label=\"train acc\")\n",
    "plt.plot(validation_acc, label=\"validation acc\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracies\")\n",
    "plt.title(\"train vs validation accs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(validate_losses, label=\"validation loss\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracies\")\n",
    "plt.title(\"train vs validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box(learning_rate, epoch, batch_size, num_layers): # A list of hyperparameters to optimize\n",
    "    validation_acc = [] \n",
    "    train_acc = []\n",
    "    train_correct, validate_correct = 0, 0\n",
    "\n",
    "    epoch = int(epoch)\n",
    "    batch_size = int(batch_size)\n",
    "    num_layers = int(num_layers)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    RNN_embeddings_model = VanillaRNNWithEmbedding(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=num_layers, num_classes=1)\n",
    "    optim = torch.optim.Adam(RNN_embeddings_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataset_ed2 = EmbeddingsDataset2(train_dataset['text'], train_dataset['label'])\n",
    "    validation_dataset_ed2 = EmbeddingsDataset2(validation_dataset['text'], validation_dataset['label'])\n",
    "    # test_dataset_ed2 = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "    train_dataloader2 = DataLoader(train_dataset_ed2, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataloader2 = DataLoader(validation_dataset_ed2, batch_size=batch_size, shuffle=True)\n",
    "    for _ in range(epoch):\n",
    "        train_loss, train_correct = train_loop(train_dataloader2, RNN_embeddings_model, criterion, optim) \n",
    "        validate_loss, validate_correct = test_loop(validation_dataloader2, RNN_embeddings_model, criterion)\n",
    "        #validation_acc.append(validate_correct)\n",
    "        #train_acc.append(train_correct)\n",
    "    return validate_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box,\n",
    "    pbounds={\"learning_rate\": (0.0001, 0.01), \"epoch\": (1, 60), \"batch_size\": (5, 64), \"num_layers\": (1, 8)},\n",
    "    random_state=1,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |   epoch   | learni... | num_la... |\n",
      "-------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5385   \u001b[39m | \u001b[39m29.6     \u001b[39m | \u001b[39m72.31    \u001b[39m | \u001b[39m0.0001114\u001b[39m | \u001b[39m3.116    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m13.66    \u001b[39m | \u001b[39m10.14    \u001b[39m | \u001b[39m0.01871  \u001b[39m | \u001b[39m3.419    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m28.41    \u001b[39m | \u001b[39m54.34    \u001b[39m | \u001b[39m0.04198  \u001b[39m | \u001b[39m5.797    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m17.06    \u001b[39m | \u001b[39m87.93    \u001b[39m | \u001b[39m0.002836 \u001b[39m | \u001b[39m5.693    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m29.62    \u001b[39m | \u001b[39m56.31    \u001b[39m | \u001b[39m0.01412  \u001b[39m | \u001b[39m2.387    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m52.24    \u001b[39m | \u001b[39m96.86    \u001b[39m | \u001b[39m0.03141  \u001b[39m | \u001b[39m5.846    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.4944   \u001b[39m | \u001b[39m56.71    \u001b[39m | \u001b[39m89.57    \u001b[39m | \u001b[39m0.008596 \u001b[39m | \u001b[39m1.273    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m15.02    \u001b[39m | \u001b[39m87.94    \u001b[39m | \u001b[39m0.009925 \u001b[39m | \u001b[39m3.948    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m61.52    \u001b[39m | \u001b[39m53.78    \u001b[39m | \u001b[39m0.06922  \u001b[39m | \u001b[39m3.209    \u001b[39m |\n",
      "| \u001b[35m10       \u001b[39m | \u001b[35m0.5394   \u001b[39m | \u001b[35m45.5     \u001b[39m | \u001b[35m83.63    \u001b[39m | \u001b[35m0.001927 \u001b[39m | \u001b[35m6.251    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5009   \u001b[39m | \u001b[39m38.32    \u001b[39m | \u001b[39m78.24    \u001b[39m | \u001b[39m0.03072  \u001b[39m | \u001b[39m5.91     \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m48.27    \u001b[39m | \u001b[39m82.71    \u001b[39m | \u001b[39m0.01068  \u001b[39m | \u001b[39m7.913    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m46.25    \u001b[39m | \u001b[39m83.35    \u001b[39m | \u001b[39m0.07759  \u001b[39m | \u001b[39m7.262    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m6.251    \u001b[39m | \u001b[39m80.98    \u001b[39m | \u001b[39m0.09649  \u001b[39m | \u001b[39m5.915    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m26.5     \u001b[39m | \u001b[39m3.217    \u001b[39m | \u001b[39m0.01533  \u001b[39m | \u001b[39m7.801    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m29.39    \u001b[39m | \u001b[39m71.92    \u001b[39m | \u001b[39m0.07659  \u001b[39m | \u001b[39m3.316    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m62.89    \u001b[39m | \u001b[39m27.77    \u001b[39m | \u001b[39m0.0809   \u001b[39m | \u001b[39m4.126    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m24.17    \u001b[39m | \u001b[39m63.94    \u001b[39m | \u001b[39m0.04011  \u001b[39m | \u001b[39m6.261    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m23.05    \u001b[39m | \u001b[39m67.99    \u001b[39m | \u001b[39m0.05518  \u001b[39m | \u001b[39m3.211    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m28.43    \u001b[39m | \u001b[39m20.22    \u001b[39m | \u001b[39m0.00538  \u001b[39m | \u001b[39m3.415    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.4991   \u001b[39m | \u001b[39m5.558    \u001b[39m | \u001b[39m55.47    \u001b[39m | \u001b[39m0.01628  \u001b[39m | \u001b[39m4.345    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m24.61    \u001b[39m | \u001b[39m48.99    \u001b[39m | \u001b[39m0.04174  \u001b[39m | \u001b[39m6.746    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m18.81    \u001b[39m | \u001b[39m45.25    \u001b[39m | \u001b[39m0.05506  \u001b[39m | \u001b[39m7.644    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m11.95    \u001b[39m | \u001b[39m55.5     \u001b[39m | \u001b[39m0.0492   \u001b[39m | \u001b[39m2.022    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5169   \u001b[39m | \u001b[39m20.5     \u001b[39m | \u001b[39m3.162    \u001b[39m | \u001b[39m0.01683  \u001b[39m | \u001b[39m5.491    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m16.57    \u001b[39m | \u001b[39m40.64    \u001b[39m | \u001b[39m0.05158  \u001b[39m | \u001b[39m1.491    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m15.67    \u001b[39m | \u001b[39m27.91    \u001b[39m | \u001b[39m0.08037  \u001b[39m | \u001b[39m5.536    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m12.89    \u001b[39m | \u001b[39m51.28    \u001b[39m | \u001b[39m0.08892  \u001b[39m | \u001b[39m4.97     \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m13.95    \u001b[39m | \u001b[39m21.22    \u001b[39m | \u001b[39m0.06728  \u001b[39m | \u001b[39m7.124    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[35m30       \u001b[39m | \u001b[35m0.5638   \u001b[39m | \u001b[35m53.26    \u001b[39m | \u001b[35m17.19    \u001b[39m | \u001b[35m0.001602 \u001b[39m | \u001b[35m2.127    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m63.17    \u001b[39m | \u001b[39m71.12    \u001b[39m | \u001b[39m0.05829  \u001b[39m | \u001b[39m6.223    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m58.04    \u001b[39m | \u001b[39m45.77    \u001b[39m | \u001b[39m0.03998  \u001b[39m | \u001b[39m3.181    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m46.4     \u001b[39m | \u001b[39m39.04    \u001b[39m | \u001b[39m0.02691  \u001b[39m | \u001b[39m4.424    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m63.66    \u001b[39m | \u001b[39m26.33    \u001b[39m | \u001b[39m0.0619   \u001b[39m | \u001b[39m3.589    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m55.21    \u001b[39m | \u001b[39m78.51    \u001b[39m | \u001b[39m0.09023  \u001b[39m | \u001b[39m1.654    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5028   \u001b[39m | \u001b[39m28.28    \u001b[39m | \u001b[39m65.22    \u001b[39m | \u001b[39m0.01577  \u001b[39m | \u001b[39m3.921    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m36.82    \u001b[39m | \u001b[39m39.82    \u001b[39m | \u001b[39m0.09373  \u001b[39m | \u001b[39m3.952    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.4991   \u001b[39m | \u001b[39m33.82    \u001b[39m | \u001b[39m7.355    \u001b[39m | \u001b[39m0.03303  \u001b[39m | \u001b[39m5.481    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5056   \u001b[39m | \u001b[39m28.93    \u001b[39m | \u001b[39m2.383    \u001b[39m | \u001b[39m0.04087  \u001b[39m | \u001b[39m6.515    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m43.65    \u001b[39m | \u001b[39m22.56    \u001b[39m | \u001b[39m0.0703   \u001b[39m | \u001b[39m7.349    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m41       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m22.78    \u001b[39m | \u001b[39m63.6     \u001b[39m | \u001b[39m0.0893   \u001b[39m | \u001b[39m4.529    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m42       \u001b[39m | \u001b[39m0.4991   \u001b[39m | \u001b[39m48.67    \u001b[39m | \u001b[39m98.4     \u001b[39m | \u001b[39m0.01612  \u001b[39m | \u001b[39m6.188    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m43       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m26.22    \u001b[39m | \u001b[39m51.19    \u001b[39m | \u001b[39m0.07251  \u001b[39m | \u001b[39m4.647    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m44       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m33.65    \u001b[39m | \u001b[39m69.15    \u001b[39m | \u001b[39m0.0648   \u001b[39m | \u001b[39m6.607    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m45       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m59.91    \u001b[39m | \u001b[39m91.34    \u001b[39m | \u001b[39m0.07552  \u001b[39m | \u001b[39m5.825    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m46       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m28.21    \u001b[39m | \u001b[39m94.5     \u001b[39m | \u001b[39m0.07652  \u001b[39m | \u001b[39m6.504    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m47       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m25.66    \u001b[39m | \u001b[39m94.35    \u001b[39m | \u001b[39m0.038    \u001b[39m | \u001b[39m3.026    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m48       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m30.48    \u001b[39m | \u001b[39m68.21    \u001b[39m | \u001b[39m0.09317  \u001b[39m | \u001b[39m5.729    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m49       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m43.05    \u001b[39m | \u001b[39m13.4     \u001b[39m | \u001b[39m0.06798  \u001b[39m | \u001b[39m2.798    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m50       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m29.86    \u001b[39m | \u001b[39m42.39    \u001b[39m | \u001b[39m0.08251  \u001b[39m | \u001b[39m3.506    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m51       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m61.73    \u001b[39m | \u001b[39m68.52    \u001b[39m | \u001b[39m0.05952  \u001b[39m | \u001b[39m5.858    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m52       \u001b[39m | \u001b[39m0.4962   \u001b[39m | \u001b[39m63.43    \u001b[39m | \u001b[39m64.76    \u001b[39m | \u001b[39m0.008872 \u001b[39m | \u001b[39m1.494    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m53       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m30.29    \u001b[39m | \u001b[39m18.37    \u001b[39m | \u001b[39m0.04175  \u001b[39m | \u001b[39m6.959    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m54       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m45.66    \u001b[39m | \u001b[39m12.67    \u001b[39m | \u001b[39m0.0544   \u001b[39m | \u001b[39m5.896    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m55       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m27.43    \u001b[39m | \u001b[39m84.99    \u001b[39m | \u001b[39m0.04115  \u001b[39m | \u001b[39m3.61     \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m56       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m50.99    \u001b[39m | \u001b[39m90.4     \u001b[39m | \u001b[39m0.05129  \u001b[39m | \u001b[39m3.067    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m57       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m44.15    \u001b[39m | \u001b[39m46.33    \u001b[39m | \u001b[39m0.03891  \u001b[39m | \u001b[39m1.568    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m58       \u001b[39m | \u001b[39m0.4991   \u001b[39m | \u001b[39m59.4     \u001b[39m | \u001b[39m31.25    \u001b[39m | \u001b[39m0.0541   \u001b[39m | \u001b[39m1.812    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m59       \u001b[39m | \u001b[39m0.5009   \u001b[39m | \u001b[39m21.91    \u001b[39m | \u001b[39m11.28    \u001b[39m | \u001b[39m0.08537  \u001b[39m | \u001b[39m1.988    \u001b[39m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_batch = torch.tensor(X_batch)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_101968\\4173132243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[39m60       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m47.47    \u001b[39m | \u001b[39m29.42    \u001b[39m | \u001b[39m0.02467  \u001b[39m | \u001b[39m2.23     \u001b[39m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(init_points=5, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': np.float64(0.5637898686679175), 'params': {'batch_size': np.float64(53.26447499168856), 'epoch': np.float64(17.18914449561826), 'learning_rate': np.float64(0.0016018184281454636), 'num_layers': np.float64(2.1269209601583134)}}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.set_bounds(new_bounds={\"learning_rate\": (0.0001, 0.01), \"epoch\": (1, 60), \"batch_size\": (5, 64), \"num_layers\": (1, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "logger = JSONLogger(path=\"./logs.log\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |   epoch   | learni... | num_la... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m62       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m33.12    \u001b[39m | \u001b[39m49.11    \u001b[39m | \u001b[39m0.006336 \u001b[39m | \u001b[39m7.38     \u001b[39m |\n",
      "| \u001b[39m63       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m47.49    \u001b[39m | \u001b[39m34.79    \u001b[39m | \u001b[39m0.006623 \u001b[39m | \u001b[39m7.939    \u001b[39m |\n",
      "| \u001b[39m64       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m14.38    \u001b[39m | \u001b[39m29.23    \u001b[39m | \u001b[39m0.005116 \u001b[39m | \u001b[39m1.848    \u001b[39m |\n",
      "| \u001b[39m65       \u001b[39m | \u001b[39m0.5028   \u001b[39m | \u001b[39m62.63    \u001b[39m | \u001b[39m32.79    \u001b[39m | \u001b[39m0.009846 \u001b[39m | \u001b[39m6.543    \u001b[39m |\n",
      "| \u001b[39m66       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m37.97    \u001b[39m | \u001b[39m12.79    \u001b[39m | \u001b[39m0.008052 \u001b[39m | \u001b[39m1.569    \u001b[39m |\n",
      "| \u001b[39m67       \u001b[39m | \u001b[39m0.485    \u001b[39m | \u001b[39m52.05    \u001b[39m | \u001b[39m11.81    \u001b[39m | \u001b[39m0.002247 \u001b[39m | \u001b[39m7.624    \u001b[39m |\n",
      "| \u001b[39m68       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m15.72    \u001b[39m | \u001b[39m53.75    \u001b[39m | \u001b[39m0.005095 \u001b[39m | \u001b[39m7.904    \u001b[39m |\n",
      "| \u001b[39m69       \u001b[39m | \u001b[39m0.4756   \u001b[39m | \u001b[39m45.07    \u001b[39m | \u001b[39m49.91    \u001b[39m | \u001b[39m0.003345 \u001b[39m | \u001b[39m3.658    \u001b[39m |\n",
      "| \u001b[39m70       \u001b[39m | \u001b[39m0.4841   \u001b[39m | \u001b[39m52.16    \u001b[39m | \u001b[39m17.05    \u001b[39m | \u001b[39m0.007017 \u001b[39m | \u001b[39m4.107    \u001b[39m |\n",
      "| \u001b[39m71       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m28.8     \u001b[39m | \u001b[39m59.24    \u001b[39m | \u001b[39m0.006845 \u001b[39m | \u001b[39m7.359    \u001b[39m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=0,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |   epoch   | learni... | num_la... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m72       \u001b[39m | \u001b[39m0.5272   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m37.04    \u001b[39m | \u001b[39m0.008657 \u001b[39m | \u001b[39m2.45     \u001b[39m |\n",
      "| \u001b[39m73       \u001b[39m | \u001b[39m0.5047   \u001b[39m | \u001b[39m36.86    \u001b[39m | \u001b[39m19.95    \u001b[39m | \u001b[39m0.008388 \u001b[39m | \u001b[39m7.927    \u001b[39m |\n",
      "| \u001b[39m74       \u001b[39m | \u001b[39m0.4934   \u001b[39m | \u001b[39m52.18    \u001b[39m | \u001b[39m23.63    \u001b[39m | \u001b[39m0.007858 \u001b[39m | \u001b[39m5.999    \u001b[39m |\n",
      "| \u001b[39m75       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m63.94    \u001b[39m | \u001b[39m35.5     \u001b[39m | \u001b[39m0.004804 \u001b[39m | \u001b[39m7.199    \u001b[39m |\n",
      "| \u001b[39m76       \u001b[39m | \u001b[39m0.5516   \u001b[39m | \u001b[39m63.59    \u001b[39m | \u001b[39m16.72    \u001b[39m | \u001b[39m0.0005364\u001b[39m | \u001b[39m4.827    \u001b[39m |\n",
      "| \u001b[39m77       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m38.62    \u001b[39m | \u001b[39m3.055    \u001b[39m | \u001b[39m0.008401 \u001b[39m | \u001b[39m4.782    \u001b[39m |\n",
      "| \u001b[39m78       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m12.55    \u001b[39m | \u001b[39m9.951    \u001b[39m | \u001b[39m0.006469 \u001b[39m | \u001b[39m2.334    \u001b[39m |\n",
      "| \u001b[39m79       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m61.25    \u001b[39m | \u001b[39m46.2     \u001b[39m | \u001b[39m0.009997 \u001b[39m | \u001b[39m3.355    \u001b[39m |\n",
      "| \u001b[39m80       \u001b[39m | \u001b[39m0.5066   \u001b[39m | \u001b[39m7.714    \u001b[39m | \u001b[39m15.33    \u001b[39m | \u001b[39m0.003516 \u001b[39m | \u001b[39m1.604    \u001b[39m |\n",
      "| \u001b[39m81       \u001b[39m | \u001b[39m0.4878   \u001b[39m | \u001b[39m16.89    \u001b[39m | \u001b[39m14.01    \u001b[39m | \u001b[39m0.008163 \u001b[39m | \u001b[39m6.2      \u001b[39m |\n",
      "| \u001b[39m82       \u001b[39m | \u001b[39m0.4981   \u001b[39m | \u001b[39m36.11    \u001b[39m | \u001b[39m48.68    \u001b[39m | \u001b[39m0.007921 \u001b[39m | \u001b[39m5.503    \u001b[39m |\n",
      "| \u001b[39m83       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m58.9     \u001b[39m | \u001b[39m43.39    \u001b[39m | \u001b[39m0.005368 \u001b[39m | \u001b[39m4.232    \u001b[39m |\n",
      "| \u001b[39m84       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m9.08     \u001b[39m | \u001b[39m58.55    \u001b[39m | \u001b[39m0.00938  \u001b[39m | \u001b[39m2.945    \u001b[39m |\n",
      "| \u001b[39m85       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m55.78    \u001b[39m | \u001b[39m19.41    \u001b[39m | \u001b[39m0.006811 \u001b[39m | \u001b[39m7.255    \u001b[39m |\n",
      "| \u001b[39m86       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m16.94    \u001b[39m | \u001b[39m48.3     \u001b[39m | \u001b[39m0.004414 \u001b[39m | \u001b[39m3.21     \u001b[39m |\n",
      "| \u001b[39m87       \u001b[39m | \u001b[39m0.4859   \u001b[39m | \u001b[39m40.64    \u001b[39m | \u001b[39m41.71    \u001b[39m | \u001b[39m0.006557 \u001b[39m | \u001b[39m3.906    \u001b[39m |\n",
      "| \u001b[39m88       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m16.09    \u001b[39m | \u001b[39m6.764    \u001b[39m | \u001b[39m0.005817 \u001b[39m | \u001b[39m7.31     \u001b[39m |\n",
      "| \u001b[39m89       \u001b[39m | \u001b[39m0.5197   \u001b[39m | \u001b[39m21.26    \u001b[39m | \u001b[39m24.18    \u001b[39m | \u001b[39m0.005725 \u001b[39m | \u001b[39m1.837    \u001b[39m |\n",
      "| \u001b[39m90       \u001b[39m | \u001b[39m0.5094   \u001b[39m | \u001b[39m24.79    \u001b[39m | \u001b[39m23.38    \u001b[39m | \u001b[39m0.002118 \u001b[39m | \u001b[39m5.262    \u001b[39m |\n",
      "| \u001b[39m91       \u001b[39m | \u001b[39m0.5075   \u001b[39m | \u001b[39m50.0     \u001b[39m | \u001b[39m39.84    \u001b[39m | \u001b[39m0.00306  \u001b[39m | \u001b[39m5.804    \u001b[39m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=0,\n",
    "    n_iter=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': np.float64(0.5637898686679175), 'params': {'batch_size': np.float64(53.26447499168856), 'epoch': np.float64(17.18914449561826), 'learning_rate': np.float64(0.0016018184281454636), 'num_layers': np.float64(2.1269209601583134)}}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Embeddings \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaRNNWithDropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix_torch=torch.tensor(embedding_matrix_np, dtype=torch.float)):\n",
    "        super(VanillaRNNWithDropout, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_torch, freeze=True, padding_idx=0)\n",
    "        self.num_layers = num_layers \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # this is the num rows of the input matrix \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, dtype=torch.float).to(x.device)\n",
    "        # Pass the embeddings through the RNN layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Only take the last output for each sequence\n",
    "        out = out[:, -1, :]\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        # Apply sigmoid activation (for binary classification)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box(learning_rate, epoch, batch_size, num_layers): # A list of hyperparameters to optimize\n",
    "    validation_acc = [] \n",
    "    train_acc = []\n",
    "    train_correct, validate_correct = 0, 0\n",
    "\n",
    "    epoch = int(epoch)\n",
    "    batch_size = int(batch_size)\n",
    "    num_layers = int(num_layers)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    RNN_embeddings_model = VanillaRNNWithDropout(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=num_layers, num_classes=1)\n",
    "    optim = torch.optim.Adam(RNN_embeddings_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataset_ed2 = EmbeddingsDataset2(train_dataset['text'], train_dataset['label'])\n",
    "    validation_dataset_ed2 = EmbeddingsDataset2(validation_dataset['text'], validation_dataset['label'])\n",
    "    # test_dataset_ed2 = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "    train_dataloader2 = DataLoader(train_dataset_ed2, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataloader2 = DataLoader(validation_dataset_ed2, batch_size=batch_size, shuffle=True)\n",
    "    for _ in range(epoch):\n",
    "        train_loss, train_correct = train_loop(train_dataloader2, RNN_embeddings_model, criterion, optim) \n",
    "        validate_loss, validate_correct = test_loop(validation_dataloader2, RNN_embeddings_model, criterion)\n",
    "        #validation_acc.append(validate_correct)\n",
    "        #train_acc.append(train_correct)\n",
    "    return validate_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box,\n",
    "    pbounds={\"learning_rate\": (0.0001, 0.01), \"epoch\": (1, 60), \"batch_size\": (5, 64), \"num_layers\": (1, 8)},\n",
    "    random_state=1,\n",
    "    verbose=2\n",
    ")\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "logger = JSONLogger(path=\"./logs_dropout.log\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.probe(\n",
    "    params={\"learning_rate\": 0.0016, \"epoch\": 17, \"batch_size\": 53, \"num_layers\": 2},\n",
    "    lazy = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0.5694183864915572\n",
      "test 0.5450281425891182\n",
      "test 0.5112570356472795\n",
      "test 0.5769230769230769\n",
      "test 0.5\n",
      "test 0.5\n",
      "test 0.5891181988742964\n",
      "test 0.5150093808630394\n",
      "test 0.5\n",
      "test 0.4896810506566604\n",
      "test 0.5234521575984991\n",
      "test 0.549718574108818\n",
      "test 0.5478424015009381\n",
      "test 0.5\n",
      "test 0.5046904315196998\n",
      "test 0.5\n",
      "test 0.49343339587242024\n",
      "test 0.5422138836772983\n",
      "test 0.5075046904315197\n",
      "test 0.5\n",
      "test 0.5\n",
      "test 0.5787992495309568\n",
      "test 0.5647279549718575\n",
      "test 0.5140712945590994\n",
      "test 0.5\n",
      "test 0.5\n",
      "test 0.49343339587242024\n",
      "test 0.5\n",
      "test 0.5797373358348968\n",
      "test 0.5\n",
      "test 0.4906191369606004\n",
      "test 0.5\n",
      "test 0.5103189493433395\n",
      "test 0.5121951219512195\n",
      "test 0.5\n",
      "test 0.5\n",
      "test 0.5553470919324578\n",
      "test 0.5\n",
      "test 0.50093808630394\n",
      "test 0.5\n",
      "test 0.5037523452157598\n",
      "test 0.50093808630394\n",
      "test 0.5\n",
      "test 0.5234521575984991\n",
      "test 0.5478424015009381\n",
      "test 0.5\n",
      "test 0.5\n",
      "test 0.49906191369606\n",
      "test 0.50187617260788\n",
      "test 0.5309568480300187\n",
      "test 0.49624765478424016\n",
      "test 0.5093808630393997\n",
      "test 0.5562851782363978\n",
      "test 0.5684803001876173\n",
      "test 0.5084427767354597\n",
      "test 0.5\n",
      "test 0.5\n",
      "test 0.5\n",
      "test 0.525328330206379\n",
      "test 0.5\n",
      "test 0.50093808630394\n",
      "test 0.5131332082551595\n",
      "test 0.5\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(init_points=2, n_iter=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': np.float64(0.5891181988742964),\n",
       " 'params': {'batch_size': np.float64(55.01782564205619),\n",
       "  'epoch': np.float64(17.216924152295988),\n",
       "  'learning_rate': np.float64(0.0029747238556237883),\n",
       "  'num_layers': np.float64(1.5953275626132635)}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\t{'target': np.float64(0.5694183864915572), 'params': {'batch_size': np.float64(53.0), 'epoch': np.float64(17.0), 'learning_rate': np.float64(0.0016), 'num_layers': np.float64(2.0)}}\n",
      "Iteration 1: \n",
      "\t{'target': np.float64(0.5450281425891182), 'params': {'batch_size': np.float64(29.604298277451868), 'epoch': np.float64(43.49914511308733), 'learning_rate': np.float64(0.00010113231069171439), 'num_layers': np.float64(3.1163280084228786)}}\n",
      "Iteration 2: \n",
      "\t{'target': np.float64(0.5112570356472795), 'params': {'batch_size': np.float64(13.658597558209669), 'epoch': np.float64(6.44797709135907), 'learning_rate': np.float64(0.0019439760926389421), 'num_layers': np.float64(3.418925089301334)}}\n",
      "Iteration 3: \n",
      "\t{'target': np.float64(0.5769230769230769), 'params': {'batch_size': np.float64(53.865065176555866), 'epoch': np.float64(16.021065076802277), 'learning_rate': np.float64(0.0022729686141519097), 'num_layers': np.float64(2.322985558014845)}}\n",
      "Iteration 4: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(59.38304080761811), 'epoch': np.float64(11.021215297921465), 'learning_rate': np.float64(0.009251637590092995), 'num_layers': np.float64(3.724442788339891)}}\n",
      "Iteration 5: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(51.159192195311036), 'epoch': np.float64(13.528971753093774), 'learning_rate': np.float64(0.0022502328188345705), 'num_layers': np.float64(4.1368048842024665)}}\n",
      "Iteration 6: \n",
      "\t{'target': np.float64(0.5891181988742964), 'params': {'batch_size': np.float64(55.01782564205619), 'epoch': np.float64(17.216924152295988), 'learning_rate': np.float64(0.0029747238556237883), 'num_layers': np.float64(1.5953275626132635)}}\n",
      "Iteration 7: \n",
      "\t{'target': np.float64(0.5150093808630394), 'params': {'batch_size': np.float64(57.28100498593709), 'epoch': np.float64(19.08778956020952), 'learning_rate': np.float64(0.001713709600651785), 'num_layers': np.float64(4.121717495939463)}}\n",
      "Iteration 8: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(56.32119319254185), 'epoch': np.float64(15.001044241283436), 'learning_rate': np.float64(0.005945720922266015), 'num_layers': np.float64(1.5976202858359598)}}\n",
      "Iteration 9: \n",
      "\t{'target': np.float64(0.4896810506566604), 'params': {'batch_size': np.float64(54.510736199928104), 'epoch': np.float64(17.263383732939324), 'learning_rate': np.float64(0.005326955872387043), 'num_layers': np.float64(2.821076942181391)}}\n",
      "Iteration 10: \n",
      "\t{'target': np.float64(0.5234521575984991), 'params': {'batch_size': np.float64(29.502342180516173), 'epoch': np.float64(43.093126104911626), 'learning_rate': np.float64(0.008963139268345402), 'num_layers': np.float64(3.622966064341308)}}\n",
      "Iteration 11: \n",
      "\t{'target': np.float64(0.549718574108818), 'params': {'batch_size': np.float64(53.83447218320983), 'epoch': np.float64(15.901444627141036), 'learning_rate': np.float64(0.0015358829494509507), 'num_layers': np.float64(2.37792621547061)}}\n",
      "Iteration 12: \n",
      "\t{'target': np.float64(0.5478424015009381), 'params': {'batch_size': np.float64(28.43388731841654), 'epoch': np.float64(12.456495888732382), 'learning_rate': np.float64(0.0006232741719578822), 'num_layers': np.float64(3.4146642182805804)}}\n",
      "Iteration 13: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(5.55835547735852), 'epoch': np.float64(33.46204977400653), 'learning_rate': np.float64(0.001703683935436564), 'num_layers': np.float64(4.344594394207118)}}\n",
      "Iteration 14: \n",
      "\t{'target': np.float64(0.5046904315196998), 'params': {'batch_size': np.float64(24.608807250518403), 'epoch': np.float64(29.602757884304054), 'learning_rate': np.float64(0.0042265804560144594), 'num_layers': np.float64(6.74581195410718)}}\n",
      "Iteration 15: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(18.809367377863072), 'epoch': np.float64(27.373109485044917), 'learning_rate': np.float64(0.0055460861591496245), 'num_layers': np.float64(7.644278811982074)}}\n",
      "Iteration 16: \n",
      "\t{'target': np.float64(0.49343339587242024), 'params': {'batch_size': np.float64(11.954392727801258), 'epoch': np.float64(33.481334650824024), 'learning_rate': np.float64(0.00496550489054661), 'num_layers': np.float64(2.0217239258016764)}}\n",
      "Iteration 17: \n",
      "\t{'target': np.float64(0.5422138836772983), 'params': {'batch_size': np.float64(54.050127077011894), 'epoch': np.float64(15.798545167862265), 'learning_rate': np.float64(0.0029554625567605837), 'num_layers': np.float64(2.670898832879527)}}\n",
      "Iteration 18: \n",
      "\t{'target': np.float64(0.5075046904315197), 'params': {'batch_size': np.float64(54.257069824709085), 'epoch': np.float64(15.542376909027555), 'learning_rate': np.float64(0.005851382019516044), 'num_layers': np.float64(2.563383024867282)}}\n",
      "Iteration 19: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(15.666522660985207), 'epoch': np.float64(17.034506375180204), 'learning_rate': np.float64(0.008055052114754305), 'num_layers': np.float64(5.536269805153662)}}\n",
      "Iteration 20: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(29.775444252135625), 'epoch': np.float64(43.6838659420433), 'learning_rate': np.float64(0.008901938788850941), 'num_layers': np.float64(2.996788711429435)}}\n",
      "Iteration 21: \n",
      "\t{'target': np.float64(0.5787992495309568), 'params': {'batch_size': np.float64(55.36505505119515), 'epoch': np.float64(16.98101242301825), 'learning_rate': np.float64(0.0016262741167851558), 'num_layers': np.float64(1.804861594835442)}}\n",
      "Iteration 22: \n",
      "\t{'target': np.float64(0.5647279549718575), 'params': {'batch_size': np.float64(53.26447499168856), 'epoch': np.float64(10.648076012540177), 'learning_rate': np.float64(0.00024882885323964055), 'num_layers': np.float64(2.1269209601583134)}}\n",
      "Iteration 23: \n",
      "\t{'target': np.float64(0.5140712945590994), 'params': {'batch_size': np.float64(63.174571676827135), 'epoch': np.float64(42.79133075399859), 'learning_rate': np.float64(0.00586687940135942), 'num_layers': np.float64(6.222870959423908)}}\n",
      "Iteration 24: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(58.04094296656058), 'epoch': np.float64(27.680947479642406), 'learning_rate': np.float64(0.00405222716050925), 'num_layers': np.float64(3.1806974647086266)}}\n",
      "Iteration 25: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(52.687802910120546), 'epoch': np.float64(16.82894860919011), 'learning_rate': np.float64(0.006493464666399643), 'num_layers': np.float64(1.7151762058313609)}}\n",
      "Iteration 26: \n",
      "\t{'target': np.float64(0.49343339587242024), 'params': {'batch_size': np.float64(53.473687486713146), 'epoch': np.float64(10.573351467453982), 'learning_rate': np.float64(0.004406503906259965), 'num_layers': np.float64(1.6560494574152687)}}\n",
      "Iteration 27: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(28.354787951366834), 'epoch': np.float64(12.238173609544518), 'learning_rate': np.float64(0.009976129504029332), 'num_layers': np.float64(3.1194151747727297)}}\n",
      "Iteration 28: \n",
      "\t{'target': np.float64(0.5797373358348968), 'params': {'batch_size': np.float64(54.41475333779778), 'epoch': np.float64(15.922196251708971), 'learning_rate': np.float64(0.001300766114221012), 'num_layers': np.float64(1.7708123185491411)}}\n",
      "Iteration 29: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(36.81660706756672), 'epoch': np.float64(24.132291614175806), 'learning_rate': np.float64(0.009378754148032577), 'num_layers': np.float64(3.951567491290912)}}\n",
      "Iteration 30: \n",
      "\t{'target': np.float64(0.4906191369606004), 'params': {'batch_size': np.float64(33.82144270224923), 'epoch': np.float64(4.78749560266928), 'learning_rate': np.float64(0.003363403665417363), 'num_layers': np.float64(5.481265768453364)}}\n",
      "Iteration 31: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(54.76259306542409), 'epoch': np.float64(16.88570178606179), 'learning_rate': np.float64(0.005463800135524024), 'num_layers': np.float64(1.827638051916722)}}\n",
      "Iteration 32: \n",
      "\t{'target': np.float64(0.5103189493433395), 'params': {'batch_size': np.float64(43.648735783025685), 'epoch': np.float64(13.847291190546368), 'learning_rate': np.float64(0.007056286014839863), 'num_layers': np.float64(7.348983765907969)}}\n",
      "Iteration 33: \n",
      "\t{'target': np.float64(0.5121951219512195), 'params': {'batch_size': np.float64(28.072618386422423), 'epoch': np.float64(12.537719219240062), 'learning_rate': np.float64(0.003154695163584881), 'num_layers': np.float64(3.5284063988922343)}}\n",
      "Iteration 34: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(54.518164535482406), 'epoch': np.float64(15.961678596424214), 'learning_rate': np.float64(0.00967284600389975), 'num_layers': np.float64(2.070589271859449)}}\n",
      "Iteration 35: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(26.220203476672843), 'epoch': np.float64(30.909631260171434), 'learning_rate': np.float64(0.007275879010741873), 'num_layers': np.float64(4.64651488454857)}}\n",
      "Iteration 36: \n",
      "\t{'target': np.float64(0.5553470919324578), 'params': {'batch_size': np.float64(29.491243836284966), 'epoch': np.float64(43.43227124967311), 'learning_rate': np.float64(0.0018174177669482985), 'num_layers': np.float64(2.7400468134273086)}}\n",
      "Iteration 37: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(59.91142868020695), 'epoch': np.float64(54.836934245553984), 'learning_rate': np.float64(0.007573933074583711), 'num_layers': np.float64(5.825441639687478)}}\n",
      "Iteration 38: \n",
      "\t{'target': np.float64(0.50093808630394), 'params': {'batch_size': np.float64(28.207727522495325), 'epoch': np.float64(56.72344481903019), 'learning_rate': np.float64(0.007672900022050824), 'num_layers': np.float64(6.503638567666743)}}\n",
      "Iteration 39: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(25.658993128262654), 'epoch': np.float64(56.63219204044854), 'learning_rate': np.float64(0.003855537910819301), 'num_layers': np.float64(3.025724920277924)}}\n",
      "Iteration 40: \n",
      "\t{'target': np.float64(0.5037523452157598), 'params': {'batch_size': np.float64(28.752754588834378), 'epoch': np.float64(12.254120056855655), 'learning_rate': np.float64(0.002521905577946622), 'num_layers': np.float64(3.534198961935899)}}\n",
      "Iteration 41: \n",
      "\t{'target': np.float64(0.50093808630394), 'params': {'batch_size': np.float64(29.716373142471813), 'epoch': np.float64(43.455720836597074), 'learning_rate': np.float64(0.008459821982152102), 'num_layers': np.float64(3.2197069275670462)}}\n",
      "Iteration 42: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(29.863303632405174), 'epoch': np.float64(25.66525650383247), 'learning_rate': np.float64(0.00826683239834308), 'num_layers': np.float64(3.5056895212573127)}}\n",
      "Iteration 43: \n",
      "\t{'target': np.float64(0.5234521575984991), 'params': {'batch_size': np.float64(53.93305241019763), 'epoch': np.float64(15.840013416439685), 'learning_rate': np.float64(0.003129862484795453), 'num_layers': np.float64(2.314090646899075)}}\n",
      "Iteration 44: \n",
      "\t{'target': np.float64(0.5478424015009381), 'params': {'batch_size': np.float64(63.428690592207765), 'epoch': np.float64(38.998345839653915), 'learning_rate': np.float64(0.0009693208403092987), 'num_layers': np.float64(1.4943316324673876)}}\n",
      "Iteration 45: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(53.98552905654276), 'epoch': np.float64(16.146887130086398), 'learning_rate': np.float64(0.0075797606791300496), 'num_layers': np.float64(2.778510061203441)}}\n",
      "Iteration 46: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(54.55050103942847), 'epoch': np.float64(15.691746747115712), 'learning_rate': np.float64(0.008347934771067534), 'num_layers': np.float64(1.70456344881278)}}\n",
      "Iteration 47: \n",
      "\t{'target': np.float64(0.49906191369606), 'params': {'batch_size': np.float64(27.429607948826593), 'epoch': np.float64(51.05535410766693), 'learning_rate': np.float64(0.004168285921199728), 'num_layers': np.float64(3.6098658130378727)}}\n",
      "Iteration 48: \n",
      "\t{'target': np.float64(0.50187617260788), 'params': {'batch_size': np.float64(29.28172672807823), 'epoch': np.float64(43.37118910522206), 'learning_rate': np.float64(0.00469213871957369), 'num_layers': np.float64(2.750705126300632)}}\n",
      "Iteration 49: \n",
      "\t{'target': np.float64(0.5309568480300187), 'params': {'batch_size': np.float64(29.67436599897116), 'epoch': np.float64(43.686757492290724), 'learning_rate': np.float64(0.0021300546288068227), 'num_layers': np.float64(2.5312599045587723)}}\n",
      "Iteration 50: \n",
      "\t{'target': np.float64(0.49624765478424016), 'params': {'batch_size': np.float64(63.321528970446785), 'epoch': np.float64(38.62071551483721), 'learning_rate': np.float64(0.007284755559346624), 'num_layers': np.float64(1.7630634822978415)}}\n",
      "Iteration 51: \n",
      "\t{'target': np.float64(0.5093808630393997), 'params': {'batch_size': np.float64(21.907355421248123), 'epoch': np.float64(7.128159552934867), 'learning_rate': np.float64(0.008550307427662612), 'num_layers': np.float64(1.9880678285051365)}}\n",
      "Iteration 52: \n",
      "\t{'target': np.float64(0.5562851782363978), 'params': {'batch_size': np.float64(47.46838147616159), 'epoch': np.float64(17.935208185059892), 'learning_rate': np.float64(0.002535354205965862), 'num_layers': np.float64(2.23000343280743)}}\n",
      "Iteration 53: \n",
      "\t{'target': np.float64(0.5684803001876173), 'params': {'batch_size': np.float64(60.41216426930936), 'epoch': np.float64(58.34160437124727), 'learning_rate': np.float64(0.002556975315957607), 'num_layers': np.float64(1.9692042502554803)}}\n",
      "Iteration 54: \n",
      "\t{'target': np.float64(0.5084427767354597), 'params': {'batch_size': np.float64(29.300156583860684), 'epoch': np.float64(43.27087178116993), 'learning_rate': np.float64(0.002870632448446767), 'num_layers': np.float64(3.109370046794006)}}\n",
      "Iteration 55: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(33.12206266648725), 'epoch': np.float64(49.1116070056182), 'learning_rate': np.float64(0.00633582643485553), 'num_layers': np.float64(7.379959151655335)}}\n",
      "Iteration 56: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(53.756925703625626), 'epoch': np.float64(15.825142997568493), 'learning_rate': np.float64(0.007003038655803392), 'num_layers': np.float64(2.7818105062339558)}}\n",
      "Iteration 57: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(14.382118786113574), 'epoch': np.float64(29.22775059065728), 'learning_rate': np.float64(0.005116244937372004), 'num_layers': np.float64(1.8479687778061822)}}\n",
      "Iteration 58: \n",
      "\t{'target': np.float64(0.525328330206379), 'params': {'batch_size': np.float64(55.37740759401987), 'epoch': np.float64(16.730190982225754), 'learning_rate': np.float64(0.008371839155655349), 'num_layers': np.float64(1.57420015448823)}}\n",
      "Iteration 59: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(53.30329377570177), 'epoch': np.float64(10.98043528184546), 'learning_rate': np.float64(0.006616610552740382), 'num_layers': np.float64(2.6451270815821477)}}\n",
      "Iteration 60: \n",
      "\t{'target': np.float64(0.50093808630394), 'params': {'batch_size': np.float64(60.47117521308193), 'epoch': np.float64(58.517296365420655), 'learning_rate': np.float64(0.0068766716813766495), 'num_layers': np.float64(2.2944149233516735)}}\n",
      "Iteration 61: \n",
      "\t{'target': np.float64(0.5131332082551595), 'params': {'batch_size': np.float64(55.39305694538), 'epoch': np.float64(16.92486020366672), 'learning_rate': np.float64(0.008903260325981438), 'num_layers': np.float64(1.6667159507617118)}}\n",
      "Iteration 62: \n",
      "\t{'target': np.float64(0.5), 'params': {'batch_size': np.float64(45.070464489581916), 'epoch': np.float64(49.907524483573525), 'learning_rate': np.float64(0.003344593947284045), 'num_layers': np.float64(3.6577114541604296)}}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN with dropout & more token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_with_token(learning_rate, epoch, batch_size, num_layers, num_tokens): # A list of hyperparameters to optimize\n",
    "    validation_acc = [] \n",
    "    train_acc = []\n",
    "    train_correct, validate_correct = 0, 0\n",
    "\n",
    "    # make sure the parameters are integers, since bayesian optimization return float\n",
    "    # refer to: https://colab.research.google.com/github/bayesian-optimization/BayesianOptimization/blob/master/examples/advanced-tour.ipynb#scrollTo=QWqhKqCnvZmn\n",
    "    epoch = int(epoch)\n",
    "    batch_size = int(batch_size)\n",
    "    num_layers = int(num_layers)\n",
    "    num_tokens = int(num_tokens)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    RNN_embeddings_model = VanillaRNNWithDropout(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=num_layers, num_classes=1)\n",
    "    optim = torch.optim.Adam(RNN_embeddings_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataset_ed2 = EmbeddingsDataset2(train_dataset['text'], train_dataset['label'], num_tokens_per_sentence=num_tokens)\n",
    "    validation_dataset_ed2 = EmbeddingsDataset2(validation_dataset['text'], validation_dataset['label'], num_tokens_per_sentence=num_tokens)\n",
    "    # test_dataset_ed2 = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "    train_dataloader2 = DataLoader(train_dataset_ed2, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataloader2 = DataLoader(validation_dataset_ed2, batch_size=batch_size, shuffle=True)\n",
    "    for _ in range(epoch):\n",
    "        train_loss, train_correct = train_loop(train_dataloader2, RNN_embeddings_model, criterion, optim) \n",
    "        validate_loss, validate_correct = test_loop(validation_dataloader2, RNN_embeddings_model, criterion)\n",
    "        #validation_acc.append(validate_correct)\n",
    "        #train_acc.append(train_correct)\n",
    "    return validate_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_with_token,\n",
    "    pbounds={\"learning_rate\": (0.0001, 0.01), \"epoch\": (1, 60), \"batch_size\": (5, 64), \"num_layers\": (1, 8), \"num_tokens\": (16, 26)},\n",
    "    random_state=1,\n",
    "    verbose=2\n",
    ")\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "logger = JSONLogger(path=\"./logs_dropout_token.log\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.probe(\n",
    "    params={\"learning_rate\": 0.0016, \"epoch\": 17, \"batch_size\": 53, \"num_layers\": 2, \"num_tokens\": 20},\n",
    "    lazy = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.maximize(init_points=5, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': np.float64(0.5600375234521576),\n",
       " 'params': {'batch_size': np.float64(63.02362224905339),\n",
       "  'epoch': np.float64(6.191456753164534),\n",
       "  'learning_rate': np.float64(0.0006502875347645216),\n",
       "  'num_layers': np.float64(3.530788193240186),\n",
       "  'num_tokens': np.float64(24.5764309091781)}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': np.float64(0.5440900562851783),\n",
       "  'params': {'batch_size': np.float64(29.604298277451868),\n",
       "   'epoch': np.float64(43.49914511308733),\n",
       "   'learning_rate': np.float64(0.00010113231069171439),\n",
       "   'num_layers': np.float64(3.1163280084228786),\n",
       "   'num_tokens': np.float64(16.46755890817113)}},\n",
       " {'target': np.float64(0.5487804878048781),\n",
       "  'params': {'batch_size': np.float64(17.895131677247917),\n",
       "   'epoch': np.float64(17.931185536260774),\n",
       "   'learning_rate': np.float64(0.00019660957174913565),\n",
       "   'num_layers': np.float64(3.2833213709768443),\n",
       "   'num_tokens': np.float64(18.976001443018905)}},\n",
       " {'target': np.float64(0.551594746716698),\n",
       "  'params': {'batch_size': np.float64(62.28906779058867),\n",
       "   'epoch': np.float64(19.139926846575833),\n",
       "   'learning_rate': np.float64(0.00010016845106199955),\n",
       "   'num_layers': np.float64(7.370817526373162),\n",
       "   'num_tokens': np.float64(19.92278535746045)}},\n",
       " {'target': np.float64(0.5525328330206379),\n",
       "  'params': {'batch_size': np.float64(27.6912884571598),\n",
       "   'epoch': np.float64(32.40564746517945),\n",
       "   'learning_rate': np.float64(0.00040390272796158146),\n",
       "   'num_layers': np.float64(1.9354739940028254),\n",
       "   'num_tokens': np.float64(15.617791110282242)}},\n",
       " {'target': np.float64(0.5600375234521576),\n",
       "  'params': {'batch_size': np.float64(63.02362224905339),\n",
       "   'epoch': np.float64(6.191456753164534),\n",
       "   'learning_rate': np.float64(0.0006502875347645216),\n",
       "   'num_layers': np.float64(3.530788193240186),\n",
       "   'num_tokens': np.float64(24.5764309091781)}}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(optimizer.res, key=lambda x: x['target'])[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout + learnable hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Embeddings \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DropoutInitialRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix_torch=torch.tensor(embedding_matrix_np, dtype=torch.float)):\n",
    "        super(DropoutInitialRNN, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_torch, freeze=True, padding_idx=0)\n",
    "        self.num_layers = num_layers \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # this is the num rows of the input matrix \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.h0 = nn.Parameter(torch.zeros(self.num_layers, 1, self.hidden_size, dtype=torch.float))\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        h0 = self.h0.expand(-1, x.size(0), -1).contiguous().to(x.device)\n",
    "        # Pass the embeddings through the RNN layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Only take the last output for each sequence\n",
    "        out = out[:, -1, :]\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        # Apply sigmoid activation (for binary classification)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_with_token(learning_rate, epoch, batch_size, num_layers, num_tokens): # A list of hyperparameters to optimize\n",
    "    validation_acc = [] \n",
    "    train_acc = []\n",
    "    train_correct, validate_correct = 0, 0\n",
    "\n",
    "    # make sure the parameters are integers, since bayesian optimization return float\n",
    "    # refer to: https://colab.research.google.com/github/bayesian-optimization/BayesianOptimization/blob/master/examples/advanced-tour.ipynb#scrollTo=QWqhKqCnvZmn\n",
    "    epoch = int(epoch)\n",
    "    batch_size = int(batch_size)\n",
    "    num_layers = int(num_layers)\n",
    "    num_tokens = int(num_tokens)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    RNN_embeddings_model = DropoutInitialRNN(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=num_layers, num_classes=1)\n",
    "    optim = torch.optim.Adam(RNN_embeddings_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataset_ed2 = EmbeddingsDataset2(train_dataset['text'], train_dataset['label'], num_tokens_per_sentence=num_tokens)\n",
    "    validation_dataset_ed2 = EmbeddingsDataset2(validation_dataset['text'], validation_dataset['label'], num_tokens_per_sentence=num_tokens)\n",
    "    # test_dataset_ed2 = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "    train_dataloader2 = DataLoader(train_dataset_ed2, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataloader2 = DataLoader(validation_dataset_ed2, batch_size=batch_size, shuffle=True)\n",
    "    for _ in range(epoch):\n",
    "        train_loss, train_correct = train_loop(train_dataloader2, RNN_embeddings_model, criterion, optim) \n",
    "        validate_loss, validate_correct = test_loop(validation_dataloader2, RNN_embeddings_model, criterion)\n",
    "        #validation_acc.append(validate_correct)\n",
    "        #train_acc.append(train_correct)\n",
    "    return validate_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "optimizer_dropout_initial = BayesianOptimization(\n",
    "    f=black_box_with_token,\n",
    "    pbounds={\"learning_rate\": (0.0001, 0.01), \"epoch\": (5, 60), \"batch_size\": (10, 64), \"num_layers\": (1, 4), \"num_tokens\": (16, 26)},\n",
    "    random_state=1,\n",
    "    verbose=2\n",
    ")\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "logger = JSONLogger(path=\"./logs_dropout_token_initial.log\")\n",
    "optimizer_dropout_initial.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dropout_initial.probe(\n",
    "    params={\"learning_rate\": 0.00019, \"epoch\": 17, \"batch_size\": 17, \"num_layers\": 3, \"num_tokens\": 18},\n",
    "    lazy = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dropout_initial.maximize(init_points=4, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(optimizer_dropout_initial.res, key=lambda x: x['target'])[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Embeddings \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VarDropoutInitialRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix_torch=torch.tensor(embedding_matrix_np, dtype=torch.float), drop_out = 0.3):\n",
    "        super(VarDropoutInitialRNN, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_torch, freeze=True, padding_idx=0)\n",
    "        self.num_layers = num_layers \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # this is the num rows of the input matrix \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.h0 = nn.Parameter(torch.zeros(self.num_layers, 1, self.hidden_size, dtype=torch.float))\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        h0 = self.h0.expand(-1, x.size(0), -1).contiguous().to(x.device)\n",
    "        # Pass the embeddings through the RNN layer\n",
    "        out, hidden = self.rnn(x, h0)\n",
    "        # Only take the last output for each sequence\n",
    "        res = hidden[-1]\n",
    "        # Dropout\n",
    "        res = self.dropout(res)\n",
    "        # Pass through the fully connected layer\n",
    "        res = self.fc(res)\n",
    "        # Apply sigmoid activation (for binary classification)\n",
    "        res = self.sigmoid(res)\n",
    "        \n",
    "        return res\n",
    "\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "# TODO: change the num_tokens \n",
    "class EmbeddingsDataset3(Dataset):\n",
    "  def __init__(self, X, y, num_tokens_per_sentence=8, word_embeddings=word_embeddings):\n",
    "    self.num_tokens_per_sentence = num_tokens_per_sentence\n",
    "    self.word_embeddings = word_embeddings\n",
    "\n",
    "    sentence_lengths = [len(nltk.word_tokenize(x)) for x in X]\n",
    "    sorted_indices = sorted(range(len(X)), key=lambda i: sentence_lengths[i])\n",
    "    \n",
    "    self.X = [X[i] for i in sorted_indices] # train_dataset['text']\n",
    "    self.y = [y[i] for i in sorted_indices] # train_dataset['label']\n",
    "\n",
    "    self.len = len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # tokenize the sentence \n",
    "    tokens = self.tokenize_sentence(self.X[index])\n",
    "    # convert each token to embeddings \n",
    "    sentence_tensor = self.convert_sentence_into_indices(tokens)\n",
    "    label = torch.tensor(self.y[index], dtype=torch.float)\n",
    "    return sentence_tensor, label \n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len \n",
    "\n",
    "  def tokenize_sentence(self, x): \n",
    "    '''\n",
    "    returns a list containing the embeddings of each token \n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    return tokens \n",
    "  \n",
    "  def convert_sentence_into_indices(self, tokens):\n",
    "    indices = []\n",
    "    num_tokens_used = 0 \n",
    "    for token in tokens:\n",
    "      if num_tokens_used == self.num_tokens_per_sentence:\n",
    "        break # we have enough of tokens from the sentence \n",
    "      if token in word2idx:\n",
    "        indices.append(word2idx[token])\n",
    "        num_tokens_used += 1 \n",
    "    # # if not enough tokens in the sentence, use index of ?? \n",
    "    if len(indices) < self.num_tokens_per_sentence:\n",
    "      padding = [0 for _ in range(self.num_tokens_per_sentence - len(indices))]\n",
    "      indices.extend(padding)\n",
    "\n",
    "    indices = torch.tensor(indices, dtype=torch.long)\n",
    "    return indices\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ed3 = EmbeddingsDataset3(train_dataset['text'], train_dataset['label'])\n",
    "validation_dataset_ed3 = EmbeddingsDataset3(validation_dataset['text'], validation_dataset['label'])\n",
    "# test_dataset_ed2 = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "\n",
    "train_dataloader3 = DataLoader(train_dataset_ed3, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validation_dataloader3 = DataLoader(validation_dataset_ed3, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_with_token(learning_rate, epoch, batch_size, num_layers, num_tokens, drop_out): # A list of hyperparameters to optimize\n",
    "    validation_acc = [] \n",
    "    train_acc = []\n",
    "    train_correct, validate_correct = 0, 0\n",
    "\n",
    "    # make sure the parameters are integers, since bayesian optimization return float\n",
    "    # refer to: https://colab.research.google.com/github/bayesian-optimization/BayesianOptimization/blob/master/examples/advanced-tour.ipynb#scrollTo=QWqhKqCnvZmn\n",
    "    epoch = int(epoch)\n",
    "    batch_size = int(batch_size)\n",
    "    num_layers = int(num_layers)\n",
    "    num_tokens = int(num_tokens)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    RNN_embeddings_model = VarDropoutInitialRNN(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=num_layers, num_classes=1, drop_out=drop_out)\n",
    "    optim = torch.optim.Adam(RNN_embeddings_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataset_ed = EmbeddingsDataset3(train_dataset['text'], train_dataset['label'], num_tokens_per_sentence=num_tokens)\n",
    "    validation_dataset_ed = EmbeddingsDataset3(validation_dataset['text'], validation_dataset['label'], num_tokens_per_sentence=num_tokens)\n",
    "    # test_dataset_ed2 = EmbeddingsDataset(test_dataset['text'], test_dataset['label'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset_ed, batch_size=batch_size, shuffle=False)\n",
    "    validation_dataloader = DataLoader(validation_dataset_ed, batch_size=batch_size, shuffle=False)\n",
    "    for _ in range(epoch):\n",
    "        train_loss, train_correct = train_loop(train_dataloader, RNN_embeddings_model, criterion, optim) \n",
    "        validate_loss, validate_correct = test_loop(validation_dataloader, RNN_embeddings_model, criterion)\n",
    "        #validation_acc.append(validate_correct)\n",
    "        #train_acc.append(train_correct)\n",
    "    return validate_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "optimizer_dropout_initial = BayesianOptimization(\n",
    "    f=black_box_with_token,\n",
    "    pbounds={\"learning_rate\": (0.0001, 0.01), \"epoch\": (5, 60), \"batch_size\": (10, 64), \"num_layers\": (1, 4), \"num_tokens\": (16, 26), \"drop_out\": (0.1, 0.5)},\n",
    "    random_state=1,\n",
    "    verbose=2\n",
    ")\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "logger = JSONLogger(path=\"./logs_var_dropout_token_initial_sorted.log\")\n",
    "optimizer_dropout_initial.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43moptimizer_dropout_initial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:312\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter)\u001b[0m\n\u001b[0;32m    310\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest()\n\u001b[0;32m    311\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:245\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mappend(params)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\bayes_opt\\target_space.py:418\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    416\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo target function has been provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m--> 418\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdict_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[1;32mIn[128], line 25\u001b[0m, in \u001b[0;36mblack_box_with_token\u001b[1;34m(learning_rate, epoch, batch_size, num_layers, num_tokens, drop_out)\u001b[0m\n\u001b[0;32m     23\u001b[0m validation_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(validation_dataset_ed, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[1;32m---> 25\u001b[0m     train_loss, train_correct \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRNN_embeddings_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     26\u001b[0m     validate_loss, validate_correct \u001b[38;5;241m=\u001b[39m test_loop(validation_dataloader, RNN_embeddings_model, criterion)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m#validation_acc.append(validate_correct)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m#train_acc.append(train_correct)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[122], line 24\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(train_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches \n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\Github\\school\\nlp-2\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer_dropout_initial.maximize(init_points=10, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
