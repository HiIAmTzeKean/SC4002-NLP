{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snorl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       "  'effective but too-tepid biopic',\n",
       "  'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .',\n",
       "  \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"],\n",
       " 'label': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Preparing Word Embeddings   \n",
    "As the first step of building your model, you need to prepare the word embeddings to form the\n",
    "input layer of your model. You are required to choose only from Word2vec or Glove to initialize\n",
    "your word embedding matrix. The word embedding matrix stores the pre-trained word vectors\n",
    "(taken from Word2vec or Glove) where each row corresponds to a vector for a specific word in the\n",
    "vocabulary formed from your task dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 16570\n",
      "Vocabulary saved to ./result/vocab.json\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import regex as re\n",
    "import json\n",
    "\n",
    "vocab = set()\n",
    "# Pattern is adapted from GPT2: https://github.com/huggingface/transformers/blob/4fb28703adc2b44ed66a44dd04740787010c5e11/src/transformers/models/gpt2/tokenization_gpt2.py#L167\n",
    "pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d|\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "for example in train_dataset:\n",
    "    tokens = pattern.findall(example['text']) \n",
    "    vocab.update(tokens)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "with open('./result/vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(list(vocab), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Vocabulary saved to ./result/vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize Word Embedding Matrix with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "GloVe embeddings loaded.\n",
      "Number of OOV Words: 638\n"
     ]
    }
   ],
   "source": [
    "glove_file = './source/glove.6B.50d.txt'\n",
    "\n",
    "glove_vocab = set()\n",
    "\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word = line.split()[0]\n",
    "        glove_vocab.add(word)\n",
    "\n",
    "print(\"GloVe embeddings loaded.\")\n",
    "\n",
    "oov_words = vocab - glove_vocab\n",
    "num_oov = len(oov_words)\n",
    "print(f\"Number of OOV Words: {num_oov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix...\n",
      "Total GloVe words loaded: 400000\n",
      "Embedding matrix built successfully.\n",
      "Embedding matrix saved as './result/embedding_matrix.npy'.\n",
      "Mapping 'word2idx' saved as './result/word2idx.json'.\n",
      "Mapping 'idx2word' saved as './result/idx2word.json'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "GLOVE_FILE = './source/glove.6B.50d.txt'  \n",
    "EMBEDDING_DIM = 50\n",
    "SAVE_DIR = './result/'\n",
    "EMBEDDING_MATRIX_PATH = os.path.join(SAVE_DIR, 'embedding_matrix.npy')\n",
    "WORD2IDX_PATH = os.path.join(SAVE_DIR, 'word2idx.json')\n",
    "IDX2WORD_PATH = os.path.join(SAVE_DIR, 'idx2word.json')\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "extended_vocab = vocab.copy()  #\n",
    "extended_vocab.add(UNK_TOKEN)\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted(extended_vocab))}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size_extended = len(extended_vocab)\n",
    "embedding_matrix = np.zeros((vocab_size_extended, EMBEDDING_DIM))\n",
    "\n",
    "print(\"Building embedding matrix...\")\n",
    "\n",
    "glove_dict = {}\n",
    "with open(GLOVE_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        word = split_line[0]\n",
    "        vector = np.array(split_line[1:], dtype='float32')\n",
    "        glove_dict[word] = vector\n",
    "\n",
    "print(f\"Total GloVe words loaded: {len(glove_dict)}\")\n",
    "\n",
    "for word in extended_vocab:\n",
    "    idx = word2idx[word]\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[idx] = glove_dict[word]\n",
    "    else:\n",
    "        if word == UNK_TOKEN:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "        else:\n",
    "            embedding_matrix[idx] = embedding_matrix[word2idx[UNK_TOKEN]]\n",
    "\n",
    "print(\"Embedding matrix built successfully.\")\n",
    "\n",
    "np.save(EMBEDDING_MATRIX_PATH, embedding_matrix)\n",
    "print(f\"Embedding matrix saved as '{EMBEDDING_MATRIX_PATH}'.\")\n",
    "\n",
    "with open(WORD2IDX_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'word2idx' saved as '{WORD2IDX_PATH}'.\")\n",
    "\n",
    "with open(IDX2WORD_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'idx2word' saved as '{IDX2WORD_PATH}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1. Word Embedding\n",
    "(a) What is the size of the vocabulary formed from your training data?   \n",
    "`16570`\n",
    "\n",
    "(b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?    \n",
    "`638`\n",
    "   \n",
    "(c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove).\n",
    "Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you\n",
    "think is the best strategy to mitigate such limitation? Implement your solution in your source\n",
    "code. Show the corresponding code snippet.     \n",
    "`Using an <UNK> Token, with its Embeddings randomized. Map any OOV words to the <UNK> Token`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Model Training & Evaluation - RNN   \n",
    "Now with the pretrained word embeddings acquired from Part 1 and the dataset acquired from\n",
    "Part 0, you need to train a deep learning model for sentiment classification using the training set,\n",
    "conforming to these requirements:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Use the pretrained word embeddings from Part 1 as inputs; do not update them during training\n",
    "(they are “frozen”).   \n",
    "\n",
    "• Design a simple recurrent neural network (RNN), taking the input word embeddings, and\n",
    "predicting a sentiment label for each sentence. To do that, you need to consider how to\n",
    "aggregate the word representations to represent a sentence.   \n",
    "\n",
    "• Use the validation set to gauge the performance of the model for each epoch during training.\n",
    "You are required to use accuracy as the performance metric during validation and evaluation. \n",
    "   \n",
    "• Use the mini-batch strategy during training. You may choose any preferred optimizer (e.g.,\n",
    "SGD, Adagrad, Adam, RMSprop). Be careful when you choose your initial learning rate and\n",
    "mini-batch size. (You should use the validation set to determine the optimal configuration.)\n",
    "Train the model until the accuracy score on the validation set is not increasing for a few\n",
    "epochs.\n",
    "   \n",
    "• Evaluate your trained model on the test dataset, observing the accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. RNN\n",
    "(a) Report the final configuration of your best model, namely the number of training epochs,\n",
    "learning rate, optimizer, batch size.   \n",
    "\n",
    "(b) Report the accuracy score on the test set, as well as the accuracy score on the validation\n",
    "set for each epoch during training.   \n",
    "\n",
    "(c) RNNs produce a hidden vector for each word, instead of the entire sentence. Which methods\n",
    "have you tried in deriving the final sentence representation to perform sentiment classification?\n",
    "Describe all the strategies you have implemented, together with their accuracy scores on the\n",
    "test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Enhancement\n",
    "The RNN model used in Part 2 is a basic model to perform the task of sentiment classification. In\n",
    "this section, you will design strategies to improve upon the previous model you have built. You are\n",
    "required to implement the following adjustments:\n",
    "\n",
    "1. Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "2. As discussed in Question 1(c), apply your solution in mitigating the influence of OOV words\n",
    "and train your model again.\n",
    "3. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a biLSTM model and a biGRU model, incorporating recurrent computations in both directions and\n",
    "stacking multiple layers if possible.\n",
    "4. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a Convolutional Neural Network (CNN) to produce sentence representations and perform sentiment\n",
    "classification.\n",
    "5. Further improve your model. You are free to use any strategy other than the above mentioned solutions. Changing hyper-parameters or stacking more layers is not counted towards\n",
    "a meaningful improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Enhancement\n",
    "(a) Report the accuracy score on the test set when the word embeddings are updated (Part 3.1).\n",
    "   \n",
    "(b) Report the accuracy score on the test set when applying your method to deal with OOV words\n",
    "in Part 3.2.\n",
    "   \n",
    "(c) Report the accuracy scores of biLSTM and biGRU on the test set (Part 3.3).\n",
    "   \n",
    "(d) Report the accuracy scores of CNN on the test set (Part 3.4).\n",
    "   \n",
    "(e) Describe your final improvement strategy in Part 3.5. Report the accuracy on the test set\n",
    "using your improved model.\n",
    "   \n",
    "(f) Compare the results across different solutions above and describe your observations with possible discussions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
