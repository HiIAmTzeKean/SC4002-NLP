{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Enhancement\n",
    "The RNN model used in Part 2 is a basic model to perform the task of sentiment classification. In\n",
    "this section, you will design strategies to improve upon the previous model you have built. You are\n",
    "required to implement the following adjustments:\n",
    "\n",
    "1. Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "2. As discussed in Question 1(c), apply your solution in mitigating the influence of OOV words\n",
    "and train your model again.\n",
    "3. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a biLSTM model and a biGRU model, incorporating recurrent computations in both directions and\n",
    "stacking multiple layers if possible.\n",
    "4. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a Convolutional Neural Network (CNN) to produce sentence representations and perform sentiment\n",
    "classification.\n",
    "5. Further improve your model. You are free to use any strategy other than the above mentioned solutions. Changing hyper-parameters or stacking more layers is not counted towards\n",
    "a meaningful improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "\n",
    "### Approach\n",
    "\n",
    "We will use the same model as in part 2 notebook, but now we will also back propagate\n",
    "the loss into the word embeddings itself. This will mean that as the model learns,\n",
    "the word embeddings would also update, causing the encoding of the words to change.\n",
    "\n",
    "We keep all other variables constant such as not handling the OOV words and using\n",
    "the same hyperparameters as in part 2 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from common_utils import EmbeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import CustomDatasetPreparer, BATCH_SIZE\n",
    "\n",
    "\n",
    "dataset_preparer = CustomDatasetPreparer(\n",
    "    dataset_name=\"rotten_tomatoes\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "train_loader, val_loader, test_loader = dataset_preparer.get_dataloaders(ignore_unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x,_,lenghts, sample_y = next(dataiter)\n",
    "\n",
    "# print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)\n",
    "print()\n",
    "print('Sample label size: ', lenghts.size()) # batch_size\n",
    "print('Sample label: \\n', lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        embedding_dim: int,\n",
    "        word_embeddings: torch.Tensor,\n",
    "        pad_idx,\n",
    "        num_layers=1,\n",
    "        output_size=1,\n",
    "        dropout_rate=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings, freeze=False, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim, hidden_dim, num_layers, batch_first=True\n",
    "        )  # this is the num rows of the input matrix\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, text_lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        if self.dropout_rate > 0:\n",
    "            embedded = self.dropout(self.embedding(x)).float()\n",
    "        else:\n",
    "            embedded = self.embedding(x).float()\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted = False)\n",
    "        packed_output, _ = self.rnn(packed_embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        if self.dropout_rate > 0:\n",
    "            output = self.dropout(output)\n",
    "        \n",
    "        output, _ = torch.max(output, 1)\n",
    "\n",
    "        out = self.fc(output)  # Use the last output of the RNN for classification\n",
    "\n",
    "        sig_out = self.sigmoid(out)\n",
    "        sig_out = sig_out.squeeze(1)\n",
    "        # print(\"Sig_out shape: \", sig_out.shape)\n",
    "        return sig_out\n",
    "\n",
    "from common_utils import HIDDEN_SIZE, EMBEDDING_DIM, LEARNING_RATE\n",
    "# initialize word embeddings\n",
    "word_embeddings = EmbeddingMatrix.load()\n",
    "word_embeddings.add_padding()\n",
    "\n",
    "print(\"The index of <PAD> is: \", word_embeddings.pad_idx)\n",
    "\n",
    "\n",
    "basic_RNN = RNN(\n",
    "    hidden_dim=HIDDEN_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    word_embeddings=word_embeddings.to_tensor,\n",
    "    dropout_rate=0.3,\n",
    "    pad_idx=word_embeddings.pad_idx,\n",
    "    num_layers=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 100\n",
    "es_patience = 15\n",
    "\n",
    "model = basic_RNN\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.BCELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.05)\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label).item()\n",
    "\n",
    "# training\n",
    "def train_loop(train_loader, model, loss_fn, optimizer, scheduler, max_norm = 5, device='cpu'):\n",
    "    train_loss = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    for X, extra_features, lengths, Y in train_loader:\n",
    "        X, Y = X.to(device), Y.to(device)   \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X, lengths)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = loss_fn(output.squeeze(), Y.float())\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,Y)\n",
    "        train_acc += accuracy\n",
    "        \n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "\n",
    "    return epoch_train_loss, epoch_train_acc\n",
    "\n",
    "\n",
    "def test_loop(test_loader, model, loss_fn, optimizer, device='cpu'):\n",
    "    test_loss = []\n",
    "    test_acc = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X,extra_features, lengths, Y in test_loader:\n",
    "            X, Y = X.to(device), Y.to(device)   \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X, lengths)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = loss_fn(output.squeeze(), Y.float())\n",
    "\n",
    "            test_loss.append(loss.item())\n",
    "            # calculating accuracy\n",
    "            accuracy = acc(output,Y)\n",
    "            test_acc += accuracy\n",
    "\n",
    "    epoch_test_loss = np.mean(test_loss)\n",
    "    epoch_test_acc = test_acc/len(test_loader.dataset)\n",
    "\n",
    "    return epoch_test_loss, epoch_test_acc\n",
    "\n",
    "def train_model(train_loader, val_loader, model, loss_fn, optimizer, scheduler, epochs, es_patience):\n",
    "    best_val_loss = np.inf\n",
    "    best_acc = 0\n",
    "    train_loss_, train_acc_, val_loss_, val_acc_ = [], [], [], []\n",
    "    from tqdm import tqdm\n",
    "    # start training\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_loop(train_loader, model, loss_fn, optimizer, scheduler)\n",
    "        val_loss, val_acc = test_loop(val_loader, model, loss_fn, optimizer)\n",
    "\n",
    "        train_loss_.append(train_loss), train_acc_.append(train_acc)\n",
    "        val_loss_.append(val_loss), val_acc_.append(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "        # early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= es_patience:\n",
    "                print(f'early stopping after {epoch+1} epochs')\n",
    "                print(f'best val loss: {best_val_loss}')\n",
    "                print(f'best accuracy on val set: {best_acc}')\n",
    "                break\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"epoch {epoch+1}, train_loss {train_loss:>7f} train_acc {train_acc:>4f}, val_loss {val_loss:>7f}, val_acc {val_acc:>4f}\")\n",
    "\n",
    "    return train_loss_, train_acc_, val_loss_, val_acc_\n",
    "\n",
    "train_loss_, train_acc_, val_loss_, val_acc_ = train_model(train_loader, val_loader, model, loss_fn, optimizer, scheduler, epochs, es_patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "(a) We use the same RNN model with the same parameters and we found that by unfreezing the weights, the performance of the model went for the test set. We hypothesize this is the case as the word embeddings are\n",
    "generalized and not specific to the task at hand. By updating the word embeddings, the model can learn the specific embeddings for the small dataset that was given to us.\n",
    "\n",
    "We show our results in the following 2 cells where we obtained a much better accuracy on the validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_accuracy(train_loss_, train_acc_, val_loss_, val_acc_):\n",
    "    fig = plt.figure(figsize = (20, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_acc_, label='Train Acc')\n",
    "    plt.plot(val_acc_, label='Validation Acc')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "        \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_loss_, label='Train loss')\n",
    "    plt.plot(val_loss_, label='Validation loss')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_accuracy(train_loss_, train_acc_, val_loss_, val_acc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test_loop(test_loader, model, loss_fn)\n",
    "print(f\"test_loss {test_loss:>7f}, test_acc {test_acc:>4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Approach\n",
    "\n",
    "As discussed in part 1, we have mentioned 2 approaches to handling of the\n",
    "OOV words. We will now demonstrate the first approach, which is to replace the OOV\n",
    "words with a special token. We will replace the OOV words with a special token\n",
    "`<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import EMBEDDING_DIM, UNK_TOKEN, load_glove_embeddings\n",
    "\n",
    "\n",
    "w2v_model = EmbeddingMatrix.load()\n",
    "extended_vocab = w2v_model.vocab\n",
    "extended_vocab.add(UNK_TOKEN)\n",
    "\n",
    "glove_dict = load_glove_embeddings()\n",
    "\n",
    "# Collect words to be removed\n",
    "missing_words = []\n",
    "for word in extended_vocab:\n",
    "    if word not in glove_dict:\n",
    "        missing_words.append(word)\n",
    "\n",
    "# Remove missing words from vocab\n",
    "for word in missing_words:\n",
    "    extended_vocab.remove(word)\n",
    "        \n",
    "print(f\"Number of missing words: {len(missing_words)}\")\n",
    "print(f\"The missing words are: {missing_words}\")\n",
    "\n",
    "# mapping of words to indices and vice versa\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted(extended_vocab))}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(\"Building embedding matrix...\")\n",
    "vocab_size = len(word2idx)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    embedding_matrix[idx] = glove_dict[word]\n",
    "\n",
    "# add random vector for unknown words\n",
    "embedding_matrix[word2idx[UNK_TOKEN]] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "\n",
    "print(\"Embedding matrix built successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid us in the downstream task for the rest of the analysis that we will be\n",
    "doing, we will update the EmbeddingMatrix class that was earlier defined to\n",
    "also handle the case of unknown words.\n",
    "\n",
    "We define the class using an API interface which can be easily called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import EMBEDDING_MATRIX_PATH, WORD2IDX_PATH, IDX2WORD_PATH\n",
    "\n",
    "class EmbeddingMatrix:\n",
    "    def __init__(self, unk_token=UNK_TOKEN, handle_unknown=True) -> None:\n",
    "        self.d = 0\n",
    "        self.v = 0\n",
    "        self.pad_idx: int\n",
    "        self.unk_idx: int\n",
    "        self.embedding_matrix: np.ndarray\n",
    "        self.word2idx: dict\n",
    "        self.idx2word: dict\n",
    "        self.unk_token = unk_token\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls) -> \"EmbeddingMatrix\":\n",
    "        # load vectors from file\n",
    "        embedding_matrix: np.ndarray = np.load(EMBEDDING_MATRIX_PATH)\n",
    "        # set attributes\n",
    "        em = cls()\n",
    "        em.embedding_matrix = embedding_matrix\n",
    "\n",
    "        with open(WORD2IDX_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            word2idx: dict = json.load(f)\n",
    "            em.word2idx = word2idx\n",
    "        with open(IDX2WORD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            idx2word: dict = json.load(f)\n",
    "            em.idx2word = idx2word\n",
    "            \n",
    "        em.v, em.d = embedding_matrix.shape\n",
    "        return em\n",
    "\n",
    "    def save(self) -> None:\n",
    "        np.save(EMBEDDING_MATRIX_PATH, self.embedding_matrix)\n",
    "        \n",
    "        with open(WORD2IDX_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.word2idx, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        with open(IDX2WORD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.idx2word, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    @property\n",
    "    def to_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(self.embedding_matrix, dtype=torch.float)\n",
    "\n",
    "    def add_padding(self) -> None:\n",
    "        if \"<PAD>\" in self.word2idx:\n",
    "            return\n",
    "        padding = np.zeros((1, self.d), dtype=\"float32\")\n",
    "        self.embedding_matrix = np.vstack((self.embedding_matrix, padding))\n",
    "\n",
    "        self.v += 1\n",
    "        self.pad_idx = self.v - 1\n",
    "        self.word2idx[\"<PAD>\"] = self.pad_idx\n",
    "\n",
    "    def add_unk_token(self) -> None:\n",
    "        if self.unk_token in self.word2idx:\n",
    "            return\n",
    "        unk_vector = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "        self.embedding_matrix = np.vstack((self.embedding_matrix, unk_vector))\n",
    "\n",
    "        self.v += 1\n",
    "        self.unk_idx = self.v - 1\n",
    "        self.word2idx[self.unk_token] = self.unk_idx\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        \"\"\"Dimension of the embedding matrix\n",
    "\n",
    "        :return: The dimension of the embedding matrix\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.d\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Vocabulary size of the embedding matrix\n",
    "\n",
    "        :return: The vocabulary size of the embedding matrix\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.v\n",
    "\n",
    "    @property\n",
    "    def vocab(self) -> set[str]:\n",
    "        \"\"\"Vocabulary of the embedding matrix\n",
    "\n",
    "        Set of words in the embedding matrix\n",
    "\n",
    "        :return: The vocabulary of the embedding matrix\n",
    "        :rtype: set[str]\n",
    "        \"\"\"\n",
    "        return set(self.word2idx.keys())\n",
    "\n",
    "    def __getitem__(self, word: str) -> np.ndarray:\n",
    "        return self.embedding_matrix[self.word2idx[word]]\n",
    "\n",
    "    def get_idx(self, word: str) -> int:\n",
    "        # if word not in vocab, return None\n",
    "        if self.handle_unknown:\n",
    "            return self.word2idx.get(word, self.unk_idx)\n",
    "\n",
    "        return self.word2idx.get(word, None)\n",
    "    \n",
    "    def is_in_vocab(self, word: str) -> bool:\n",
    "        return word in self.word2idx\n",
    "\n",
    "    def is_in_index(self, idx: int) -> bool:\n",
    "        return idx in self.idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second approach, we will use the FastText embeddings, which are trained on\n",
    "subword information. This will help in encoding the OOV words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of FastText for word embedding\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "from common_utils import EMBEDDING_DIM\n",
    "# Create a FastText model with the same dimensions as the Word2Vec model\n",
    "fasttext_model = FastText(\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=5, # context window size \n",
    "    min_count=1, # threshold for word frequency\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from common_utils import EmbeddingMatrix, tokenize\n",
    "import nltk\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "corpus = []\n",
    "for example in train_dataset:\n",
    "    tokens = nltk.word_tokenize(example['text'])\n",
    "    corpus.append(tokens)\n",
    "print(\"The corpus has {} documents.\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model.build_vocab(corpus_iterable=corpus)\n",
    "# fasttext_model.build_vocab(corpus_iterable=corpus, update=True)\n",
    "\n",
    "print(\"Length of vocabulary:\", len(fasttext_model.wv.key_to_index))\n",
    "\n",
    "\n",
    "fasttext_model.train(\n",
    "    corpus_iterable=corpus, epochs=fasttext_model.epochs,\n",
    "    total_examples=fasttext_model.corpus_count, total_words=fasttext_model.corpus_total_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"computer\" in fasttext_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_vocab = set(fasttext_model.wv.key_to_index.keys())\n",
    "w2v_model = EmbeddingMatrix.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.vocab - fasttext_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_vocab - w2v_model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The FastText model has {} words.\".format(len(fasttext_model.wv.key_to_index)))\n",
    "print(\"The Word2Vec model has {} words.\".format(len(w2v_model.vocab)))\n",
    "print(\"The FastText model has {} words that are not in the Word2Vec model.\".format(len(fasttext_vocab - w2v_model.vocab)))\n",
    "print(\"The Word2Vec model has {} words that are not in the FastText model.\".format(len(w2v_model.vocab - fasttext_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the FastText model\n",
    "fasttext_model.save(\"fasttext_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OOV handling defined, we now define the handing of the dataloader\n",
    "with the new embedding containing the `<UNK>` token. We will also now\n",
    "define an RNN model with the updated embedding matrix and run it on the\n",
    "data loader to test the performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import CustomDatasetPreparer, BATCH_SIZE\n",
    "\n",
    "\n",
    "dataset_preparer = CustomDatasetPreparer(\n",
    "    dataset_name=\"rotten_tomatoes\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "train_loader, val_loader, test_loader = dataset_preparer.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize word embeddings\n",
    "word_embeddings = EmbeddingMatrix.load()\n",
    "word_embeddings.add_padding()\n",
    "word_embeddings.add_unk_token()\n",
    "\n",
    "print(\"The index of <PAD> is: \", word_embeddings.pad_idx)\n",
    "\n",
    "\n",
    "basic_RNN = RNN(\n",
    "    hidden_dim=HIDDEN_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    word_embeddings=word_embeddings.to_tensor,\n",
    "    dropout_rate=0.3,\n",
    "    pad_idx=word_embeddings.pad_idx,\n",
    "    num_layers=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "(b) The following code shows the performance of the model with OOV handling\n",
    "where the model would have the learn the embeddings for the unknown words\n",
    "that it might occur and subsequently how to classify an `<UNK>` token that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 100\n",
    "es_patience = 15\n",
    "\n",
    "model = basic_RNN\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.BCELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.05)\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "train_loss_, train_acc_, val_loss_, val_acc_ = train_model(train_loader, val_loader, model, loss_fn, optimizer, scheduler, epochs, es_patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(train_loss_, train_acc_, val_loss_, val_acc_)\n",
    "\n",
    "test_loss, test_acc = test_loop(test_loader, model, loss_fn, optimizer)\n",
    "print(f\"test_loss {test_loss:>7f}, test_acc {test_acc:>4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
