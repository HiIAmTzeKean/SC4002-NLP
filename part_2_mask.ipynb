{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# We omit warnings to keep the output clean\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "\n",
    "from common_utils import load_glove_embeddings, set_seed, EmbeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed \n",
    "set_seed()\n",
    "\n",
    "# initialize parameters\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = 100 # word embedding size \n",
    "HIDDEN_SIZE = 128 # just as a starter to see \n",
    "NUM_EPOCHS = 100 \n",
    "EMBEDDING_DIM=100\n",
    "GRADIENT_CLIP=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from huggingface first \n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "with open('result/word2idx.json', \"r\") as file:\n",
    "    word2idx = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of <PAD> is:  18030\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# initialize word embeddings\n",
    "word_embeddings = EmbeddingMatrix.load()\n",
    "word_embeddings.add_padding()\n",
    "\n",
    "print(\"The index of <PAD> is: \", word_embeddings.pad_idx)\n",
    "\n",
    "print(word_embeddings.to_tensor[word_embeddings.pad_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validate and test datasets and dataloaders\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, X, y, word_embeddings:EmbeddingMatrix =word_embeddings):\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.X = X # train_dataset['text']\n",
    "        self.y = y # train_dataset['label']\n",
    "        self.len = len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # tokenize the sentence\n",
    "        tokens = self.tokenize_sentence(self.X[index])\n",
    "        return tokens, self.y[index] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len \n",
    "\n",
    "    def tokenize_sentence(self, x): \n",
    "        '''\n",
    "    returns a list containing the embeddings of each token \n",
    "    '''\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        # word tokens to index, skip if token is not in the word embeddings\n",
    "        tokens = [self.word_embeddings.get_idx(token) for token in tokens if self.word_embeddings.get_idx(token) is not None]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def pad_collate(batch, pad_value):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    # convert xx to a tensor\n",
    "    xx = [torch.tensor(x, dtype=torch.int64) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
    "    return xx_pad, torch.tensor(yy, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ed = EmbeddingsDataset(\n",
    "    train_dataset[\"text\"], train_dataset[\"label\"]\n",
    ")\n",
    "validation_dataset_ed = EmbeddingsDataset(\n",
    "    validation_dataset[\"text\"], validation_dataset[\"label\"]\n",
    ")\n",
    "test_dataset_ed = EmbeddingsDataset(test_dataset[\"text\"], test_dataset[\"label\"])\n",
    "\n",
    "pad_value = word_embeddings.pad_idx\n",
    "# implement minibatch training\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Embeddings \n",
    "class VanillaRNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,  embedding_matrix_torch:torch.tensor, padding_idx: int):\n",
    "        super(VanillaRNNWithEmbedding, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_torch, freeze=True, padding_idx=padding_idx)\n",
    "        self.num_layers = num_layers \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # this is the num rows of the input matrix \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x).float()\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, dtype=torch.float).to(x.device)\n",
    "        # Pass the embeddings through the RNN layer\n",
    "        out, hidden = self.rnn(x, h0)\n",
    "        # Max pooling\n",
    "        #out, _ = torch.max(out, dim=1)\n",
    "        # Only take the last output for each sequence\n",
    "        res = hidden[-1]\n",
    "        # Pass through the fully connected layer\n",
    "        res = self.fc(res)\n",
    "        # Apply sigmoid activation (for binary classification)\n",
    "        res = self.sigmoid(res)\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_bce(train_dataloader, model, loss_fn, optimizer):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "    size = len(train_dataloader.dataset)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch_no, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        if train_on_gpu:\n",
    "            X_batch = X_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        \n",
    "        pred = pred.squeeze(1)\n",
    "        pred_binary = (pred >= 0.5).long()\n",
    "        loss = loss_fn(pred, y_batch.float())\n",
    "        train_loss += loss.item() \n",
    "        train_correct += (pred_binary==y_batch.long()).sum().item() \n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # TODO add main branch\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= num_batches \n",
    "    train_correct /= size \n",
    "\n",
    "    return train_loss, train_correct \n",
    "   \n",
    "\n",
    "def test_loop_bce(validate_dataloader, model, loss_fn):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    num_batches = len(validate_dataloader)\n",
    "    size = len(validate_dataloader.dataset)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in validate_dataloader:\n",
    "            if train_on_gpu:\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            pred = model(X_batch)\n",
    "            pred = pred.squeeze(1)\n",
    "            pred_binary = (pred >= 0.5).long()\n",
    "            test_loss += loss_fn(pred, y_batch.float()).item()\n",
    "            test_correct += (pred_binary == y_batch.long()).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    return test_loss, test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, criterion, training_dataloader, validation_dataloader, epoch = NUM_EPOCHS):\n",
    "  validation_acc = [] \n",
    "  train_acc = []\n",
    "  train_losses, validate_losses = [], []\n",
    "  for i in range(epoch):\n",
    "    train_loss, train_correct = train_loop_bce(training_dataloader, model, criterion, optim) \n",
    "    validate_loss, validate_correct = test_loop_bce(validation_dataloader, model, criterion)\n",
    "    validation_acc.append(validate_correct)\n",
    "    train_acc.append(train_correct)\n",
    "    train_losses.append(train_loss)\n",
    "    validate_losses.append(validate_loss)\n",
    "\n",
    "    print(f\"Epoch {i+1}, Train Loss: {train_loss:.4f}, Validate Loss: {validate_loss:.4f}\")\n",
    "    #if i%10 == 0:\n",
    "    print(f\"Epoch:{i+1} \\t Train Acc:{train_correct} \\t Validation Acc:{validate_correct}\")\n",
    "  return train_acc, validation_acc, train_losses, validate_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6929, Validate Loss: 0.6939\n",
      "Epoch:1 \t Train Acc:0.5089097303634232 \t Validation Acc:0.48874296435272047\n",
      "Epoch 2, Train Loss: 0.6934, Validate Loss: 0.6944\n",
      "Epoch:2 \t Train Acc:0.5048065650644783 \t Validation Acc:0.50093808630394\n",
      "Epoch 3, Train Loss: 0.6932, Validate Loss: 0.6940\n",
      "Epoch:3 \t Train Acc:0.5094958968347011 \t Validation Acc:0.50187617260788\n",
      "Epoch 4, Train Loss: 0.6930, Validate Loss: 0.6925\n",
      "Epoch:4 \t Train Acc:0.4978898007033998 \t Validation Acc:0.5\n",
      "Epoch 5, Train Loss: 0.6925, Validate Loss: 0.6932\n",
      "Epoch:5 \t Train Acc:0.500820633059789 \t Validation Acc:0.5\n",
      "Epoch 6, Train Loss: 0.6922, Validate Loss: 0.6931\n",
      "Epoch:6 \t Train Acc:0.5044548651817116 \t Validation Acc:0.5056285178236398\n",
      "Epoch 7, Train Loss: 0.6452, Validate Loss: 0.6050\n",
      "Epoch:7 \t Train Acc:0.6345838218053927 \t Validation Acc:0.6885553470919324\n",
      "Epoch 8, Train Loss: 0.5896, Validate Loss: 0.5929\n",
      "Epoch:8 \t Train Acc:0.700820633059789 \t Validation Acc:0.6904315196998124\n",
      "Epoch 9, Train Loss: 0.5757, Validate Loss: 0.5837\n",
      "Epoch:9 \t Train Acc:0.7096131301289567 \t Validation Acc:0.7073170731707317\n",
      "Epoch 10, Train Loss: 0.5646, Validate Loss: 0.5913\n",
      "Epoch:10 \t Train Acc:0.7179366940211019 \t Validation Acc:0.698874296435272\n",
      "Epoch 11, Train Loss: 0.5617, Validate Loss: 0.5877\n",
      "Epoch:11 \t Train Acc:0.7221570926143025 \t Validation Acc:0.6979362101313321\n",
      "Epoch 12, Train Loss: 0.5588, Validate Loss: 0.5635\n",
      "Epoch:12 \t Train Acc:0.7219226260257913 \t Validation Acc:0.7129455909943715\n",
      "Epoch 13, Train Loss: 0.5526, Validate Loss: 0.5752\n",
      "Epoch:13 \t Train Acc:0.7279015240328254 \t Validation Acc:0.7045028142589118\n",
      "Epoch 14, Train Loss: 0.5484, Validate Loss: 0.5564\n",
      "Epoch:14 \t Train Acc:0.7305978898007034 \t Validation Acc:0.7148217636022514\n",
      "Epoch 15, Train Loss: 0.5462, Validate Loss: 0.5805\n",
      "Epoch:15 \t Train Acc:0.7325908558030481 \t Validation Acc:0.7091932457786116\n",
      "Epoch 16, Train Loss: 0.5506, Validate Loss: 0.5634\n",
      "Epoch:16 \t Train Acc:0.7317702227432591 \t Validation Acc:0.7166979362101313\n",
      "Epoch 17, Train Loss: 0.5476, Validate Loss: 0.5730\n",
      "Epoch:17 \t Train Acc:0.731535756154748 \t Validation Acc:0.7213883677298312\n",
      "Epoch 18, Train Loss: 0.5478, Validate Loss: 0.5594\n",
      "Epoch:18 \t Train Acc:0.7329425556858148 \t Validation Acc:0.7213883677298312\n",
      "Epoch 19, Train Loss: 0.5419, Validate Loss: 0.5576\n",
      "Epoch:19 \t Train Acc:0.7336459554513481 \t Validation Acc:0.7204502814258912\n",
      "Epoch 20, Train Loss: 0.5389, Validate Loss: 0.5590\n",
      "Epoch:20 \t Train Acc:0.7396248534583821 \t Validation Acc:0.7110694183864915\n",
      "Epoch 21, Train Loss: 0.5417, Validate Loss: 0.5609\n",
      "Epoch:21 \t Train Acc:0.7389214536928488 \t Validation Acc:0.7082551594746717\n",
      "Epoch 22, Train Loss: 0.5428, Validate Loss: 0.5662\n",
      "Epoch:22 \t Train Acc:0.7388042203985932 \t Validation Acc:0.7120075046904315\n",
      "Epoch 23, Train Loss: 0.5389, Validate Loss: 0.5730\n",
      "Epoch:23 \t Train Acc:0.7377491207502931 \t Validation Acc:0.7110694183864915\n",
      "Epoch 24, Train Loss: 0.5393, Validate Loss: 0.5538\n",
      "Epoch:24 \t Train Acc:0.7404454865181712 \t Validation Acc:0.7157598499061913\n",
      "Epoch 25, Train Loss: 0.5391, Validate Loss: 0.5803\n",
      "Epoch:25 \t Train Acc:0.741852286049238 \t Validation Acc:0.701688555347092\n",
      "Epoch 26, Train Loss: 0.5390, Validate Loss: 0.5672\n",
      "Epoch:26 \t Train Acc:0.7396248534583821 \t Validation Acc:0.7270168855534709\n",
      "Epoch 27, Train Loss: 0.5325, Validate Loss: 0.5657\n",
      "Epoch:27 \t Train Acc:0.7423212192262603 \t Validation Acc:0.7195121951219512\n",
      "Epoch 28, Train Loss: 0.5340, Validate Loss: 0.5549\n",
      "Epoch:28 \t Train Acc:0.741852286049238 \t Validation Acc:0.7213883677298312\n",
      "Epoch 29, Train Loss: 0.5280, Validate Loss: 0.5572\n",
      "Epoch:29 \t Train Acc:0.7424384525205159 \t Validation Acc:0.7148217636022514\n",
      "Epoch 30, Train Loss: 0.5310, Validate Loss: 0.5595\n",
      "Epoch:30 \t Train Acc:0.7437280187573271 \t Validation Acc:0.7185741088180112\n",
      "Epoch 31, Train Loss: 0.5262, Validate Loss: 0.5454\n",
      "Epoch:31 \t Train Acc:0.7449003516998828 \t Validation Acc:0.726078799249531\n",
      "Epoch 32, Train Loss: 0.5274, Validate Loss: 0.5525\n",
      "Epoch:32 \t Train Acc:0.7417350527549824 \t Validation Acc:0.7363977485928705\n",
      "Epoch 33, Train Loss: 0.5289, Validate Loss: 0.5501\n",
      "Epoch:33 \t Train Acc:0.7438452520515827 \t Validation Acc:0.7270168855534709\n",
      "Epoch 34, Train Loss: 0.5275, Validate Loss: 0.5438\n",
      "Epoch:34 \t Train Acc:0.7432590855803048 \t Validation Acc:0.7317073170731707\n",
      "Epoch 35, Train Loss: 0.5265, Validate Loss: 0.5871\n",
      "Epoch:35 \t Train Acc:0.7480656506447831 \t Validation Acc:0.7138836772983115\n",
      "Epoch 36, Train Loss: 0.5327, Validate Loss: 0.5792\n",
      "Epoch:36 \t Train Acc:0.7438452520515827 \t Validation Acc:0.700750469043152\n",
      "Epoch 37, Train Loss: 0.5203, Validate Loss: 0.6014\n",
      "Epoch:37 \t Train Acc:0.7463071512309496 \t Validation Acc:0.6941838649155723\n",
      "Epoch 38, Train Loss: 0.5276, Validate Loss: 0.5515\n",
      "Epoch:38 \t Train Acc:0.7458382180539274 \t Validation Acc:0.701688555347092\n",
      "Epoch 39, Train Loss: 0.5201, Validate Loss: 0.5647\n",
      "Epoch:39 \t Train Acc:0.7511137162954279 \t Validation Acc:0.724202626641651\n",
      "Epoch 40, Train Loss: 0.5173, Validate Loss: 0.5594\n",
      "Epoch:40 \t Train Acc:0.7480656506447831 \t Validation Acc:0.7120075046904315\n",
      "Epoch 41, Train Loss: 0.5198, Validate Loss: 0.5431\n",
      "Epoch:41 \t Train Acc:0.7456037514654161 \t Validation Acc:0.7307692307692307\n",
      "Epoch 42, Train Loss: 0.5215, Validate Loss: 0.5672\n",
      "Epoch:42 \t Train Acc:0.7531066822977726 \t Validation Acc:0.7279549718574109\n",
      "Epoch 43, Train Loss: 0.5129, Validate Loss: 0.5452\n",
      "Epoch:43 \t Train Acc:0.7519343493552169 \t Validation Acc:0.7195121951219512\n",
      "Epoch 44, Train Loss: 0.5138, Validate Loss: 0.5401\n",
      "Epoch:44 \t Train Acc:0.7525205158264947 \t Validation Acc:0.7298311444652908\n",
      "Epoch 45, Train Loss: 0.5096, Validate Loss: 0.5383\n",
      "Epoch:45 \t Train Acc:0.7562719812426729 \t Validation Acc:0.726078799249531\n",
      "Epoch 46, Train Loss: 0.5088, Validate Loss: 0.5902\n",
      "Epoch:46 \t Train Acc:0.7568581477139508 \t Validation Acc:0.6885553470919324\n",
      "Epoch 47, Train Loss: 0.5140, Validate Loss: 0.5417\n",
      "Epoch:47 \t Train Acc:0.7502930832356389 \t Validation Acc:0.723264540337711\n",
      "Epoch 48, Train Loss: 0.5057, Validate Loss: 0.5318\n",
      "Epoch:48 \t Train Acc:0.7534583821805393 \t Validation Acc:0.7204502814258912\n",
      "Epoch 49, Train Loss: 0.5054, Validate Loss: 0.5326\n",
      "Epoch:49 \t Train Acc:0.7540445486518171 \t Validation Acc:0.7392120075046904\n",
      "Epoch 50, Train Loss: 0.4980, Validate Loss: 0.5578\n",
      "Epoch:50 \t Train Acc:0.7601406799531066 \t Validation Acc:0.7101313320825516\n"
     ]
    }
   ],
   "source": [
    "vanilla_rnn = VanillaRNNWithEmbedding(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=2, num_classes=1, embedding_matrix_torch=word_embeddings.to_tensor, padding_idx=word_embeddings.pad_idx)\n",
    "optim = torch.optim.Adam(vanilla_rnn.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_acc_vanilla_rnn, validation_acc_vanilla_rnn, train_loss_vanilla_rnn, validation_loss_vanilla_rnn = train(vanilla_rnn, optim, criterion, train_dataloader, validation_dataloader, epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
