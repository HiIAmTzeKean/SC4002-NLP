{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Enhancement\n",
    "The RNN model used in Part 2 is a basic model to perform the task of sentiment classification. In\n",
    "this section, you will design strategies to improve upon the previous model you have built. You are\n",
    "required to implement the following adjustments:\n",
    "\n",
    "1. Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "2. As discussed in Question 1(c), apply your solution in mitigating the influence of OOV words\n",
    "and train your model again.\n",
    "3. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a biLSTM model and a biGRU model, incorporating recurrent computations in both directions and\n",
    "stacking multiple layers if possible.\n",
    "4. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a Convolutional Neural Network (CNN) to produce sentence representations and perform sentiment\n",
    "classification.\n",
    "5. Further improve your model. You are free to use any strategy other than the above mentioned solutions. Changing hyper-parameters or stacking more layers is not counted towards\n",
    "a meaningful improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "\n",
    "### Approach\n",
    "\n",
    "We will use the same model as in part 2 notebook, but now we will also back propagate\n",
    "the loss into the word embeddings itself. This will mean that as the model learns,\n",
    "the word embeddings would also update, causing the encoding of the words to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\sc4002-nlp-LiAJfnhK-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from common_utils import EmbeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, word_embedding:np.ndarray):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Word2Vec embedding layer\n",
    "        # freeze=False to enable updates to embeddings\n",
    "        self.word2vec_embeddings = nn.Embedding.from_pretrained(torch.tensor(word_embedding), freeze=False)\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Look up word embeddings\n",
    "        x = self.word2vec_embeddings(x)\n",
    "\n",
    "        # Pass through RNN\n",
    "        x, _ = self.rnn(x)\n",
    "\n",
    "        # Take the last hidden state\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "# Load EmbeddingMatrix\n",
    "w2v_model = EmbeddingMatrix.load()\n",
    "\n",
    "# Create RNN model\n",
    "model = RNNModel(w2v_model.vocab_size,\n",
    "                 w2v_model.dimension,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 w2v_model.embedding_matrix)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Approach\n",
    "\n",
    "We can implement FastText and the <UNK> token handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of FastText for word embedding\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Load Word2Vec model\n",
    "w2v_model = EmbeddingMatrix.load()\n",
    "\n",
    "# Create a list of words from the Word2Vec model\n",
    "words = list(w2v_model.vocab)\n",
    "\n",
    "# Create a FastText model with the same dimensions as the Word2Vec model\n",
    "fasttext_model = FastText(vector_size=w2v_model.dimension, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Build the FastText vocabulary\n",
    "fasttext_model.build_vocab(words)\n",
    "\n",
    "# Initialize embeddings with Word2Vec embeddings\n",
    "for word in words:\n",
    "    fasttext_model.wv[word] = w2v_model[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3. Enhancement\n",
    "(a) Report the accuracy score on the test set when the word embeddings are updated (Part 3.1).\n",
    "   \n",
    "(b) Report the accuracy score on the test set when applying your method to deal with OOV words\n",
    "in Part 3.2.\n",
    "   \n",
    "(c) Report the accuracy scores of biLSTM and biGRU on the test set (Part 3.3).\n",
    "   \n",
    "(d) Report the accuracy scores of CNN on the test set (Part 3.4).\n",
    "   \n",
    "(e) Describe your final improvement strategy in Part 3.5. Report the accuracy on the test set\n",
    "using your improved model.\n",
    "   \n",
    "(f) Compare the results across different solutions above and describe your observations with possible discussions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002-nlp-LiAJfnhK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
