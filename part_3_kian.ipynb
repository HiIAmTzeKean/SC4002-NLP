{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ac2ef87f70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from common_utils import EmbeddingMatrix, UNK_TOKEN, EMBEDDING_DIM, tokenize, CustomDatasetPreparer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make sure to generate with part_3_tk.ipynb first\n",
    "# todo merge with previous parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec embedding matrix\n",
    "w2v_model = EmbeddingMatrix.load()\n",
    "word2idx = w2v_model.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset_preparer = CustomDatasetPreparer(\n",
    "    dataset_name=\"rotten_tomatoes\",\n",
    "    word2idx=word2idx,\n",
    "    unk_token=UNK_TOKEN,\n",
    "    max_len=512,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_loader, val_loader, test_loader = dataset_preparer.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load datasets\n",
    "# from datasets import load_dataset\n",
    "# # From assignment PDF\n",
    "# dataset = load_dataset(\"rotten_tomatoes\")\n",
    "# train_dataset = dataset ['train']\n",
    "# validation_dataset = dataset ['validation']\n",
    "# test_dataset = dataset ['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# import nltk\n",
    "\n",
    "# def tokenize(texts, word2idx, max_len=512):\n",
    "#     tokenized = []\n",
    "#     for text in texts:\n",
    "#         tokens = nltk.word_tokenize(text.lower())\n",
    "#         token_ids = [word2idx.get(word, word2idx[UNK_TOKEN]) for word in tokens]\n",
    "#         tokenized.append(torch.tensor(token_ids[:max_len], dtype=torch.int))\n",
    "#     return tokenized\n",
    "\n",
    "\n",
    "# def prepare_dataset(dataset: datasets.DatasetDict, batch_size=50) -> torch.utils.data:\n",
    "#     set_tokenized = tokenize(dataset['text'], word2idx)\n",
    "\n",
    "#     set_tokenized = pad_sequence(set_tokenized, batch_first=True)\n",
    "\n",
    "#     set_labels = torch.tensor(dataset['label'])\n",
    "\n",
    "#     set_data = data.TensorDataset(set_tokenized, set_labels)\n",
    "\n",
    "#     return data.DataLoader(set_data, batch_size, shuffle=True)\n",
    "# train_loader = prepare_dataset(train_dataset)\n",
    "# val_loader = prepare_dataset(validation_dataset)\n",
    "# test_loader = prepare_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Enhancement\n",
    "The RNN model used in Part 2 is a basic model to perform the task of sentiment classification. In\n",
    "this section, you will design strategies to improve upon the previous model you have built. You are\n",
    "required to implement the following adjustments:\n",
    "\n",
    "1. Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "2. As discussed in Question 1(c), apply your solution in mitigating the influence of OOV words\n",
    "and train your model again.\n",
    "3. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a biLSTM model and a biGRU model, incorporating recurrent computations in both directions and\n",
    "stacking multiple layers if possible.\n",
    "4. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a Convolutional Neural Network (CNN) to produce sentence representations and perform sentiment\n",
    "classification.\n",
    "5. Further improve your model. You are free to use any strategy other than the above mentioned solutions. Changing hyper-parameters or stacking more layers is not counted towards\n",
    "a meaningful improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(c) biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, word_embedding, hidden_dim, embedding_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(word_embedding, dtype=torch.float))\n",
    "\n",
    "        # Bi-directional LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # Fully connected layer for binary classification\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the input sequence\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Apply the BiLSTM layer\n",
    "        emb_output, (hidden_state,cell_state) = self.lstm(embedded)\n",
    "\n",
    "         # Concatenating the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.sigmoid(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (embedding): Embedding(16164, 100)\n",
      "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 64\n",
    "# batch_size = 64\n",
    "max_norm = 5\n",
    "learning_rate = 0.0001\n",
    "epochs = 100\n",
    "es_patience = 15\n",
    "\n",
    "# Create LSTM model\n",
    "model = BiLSTM(w2v_model.embedding_matrix,hidden_dim,w2v_model.dimension)\n",
    "print(model)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Create loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label).item()\n",
    "\n",
    "# training\n",
    "def train_loop(train_loader, model, loss_fn, optimizer, max_norm = 3):\n",
    "    train_loss = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    for X, extra_features, lengths, Y in train_loader:\n",
    "        X, Y = X.to(device), Y.to(device)   \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = loss_fn(output.squeeze(), Y.float())\n",
    "        loss.backward()\n",
    "        train_loss.append(loss.item())\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,Y)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "\n",
    "    return epoch_train_loss, epoch_train_acc\n",
    "\n",
    "# testing\n",
    "def test_loop(test_loader, model, loss_fn):\n",
    "\n",
    "    test_loss = []\n",
    "    test_acc = 0.0\n",
    "    model.eval() # inform no dropout and fix bn during testing\n",
    "    with torch.no_grad():\n",
    "        for X,extra_features, lengths, Y in test_loader:\n",
    "            X, Y = X.to(device), Y.to(device)   \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = loss_fn(output.squeeze(), Y.float())\n",
    "\n",
    "            test_loss.append(loss.item())\n",
    "            # calculating accuracy\n",
    "            accuracy = acc(output,Y)\n",
    "            test_acc += accuracy\n",
    "\n",
    "    epoch_test_loss = np.mean(test_loss)\n",
    "    epoch_test_acc = test_acc/len(test_loader.dataset)\n",
    "\n",
    "    return epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:13<22:22, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 0.691380 train_acc 0.545252, val_loss 0.690256, val_acc 0.574109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 6/100 [00:53<11:10,  7.13s/it]"
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "best_acc = 0\n",
    "train_loss_, train_acc_, val_loss_, val_acc_ = [], [], [], []\n",
    "from tqdm import tqdm\n",
    "# start training\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    val_loss, val_acc = test_loop(val_loader, model, loss_fn)\n",
    "\n",
    "    train_loss_.append(train_loss), train_acc_.append(train_acc)\n",
    "    val_loss_.append(val_loss), val_acc_.append(val_acc)\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "    # early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= es_patience:\n",
    "            print(f'early stopping after {epoch+1} epochs')\n",
    "            print(f'best val loss: {best_val_loss}')\n",
    "            print(f'best accuracy on val set: {best_acc}')\n",
    "            break\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch+1}, train_loss {train_loss:>7f} train_acc {train_acc:>4f}, val_loss {val_loss:>7f}, val_acc {val_acc:>4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_, label='Train Acc')\n",
    "plt.plot(val_acc_, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_, label='Train loss')\n",
    "plt.plot(val_loss_, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test_loop(test_loader, model, loss_fn)\n",
    "print(f\"test_loss {test_loss:>7f}, test_acc {test_acc:>4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3. Enhancement\n",
    "(a) Report the accuracy score on the test set when the word embeddings are updated (Part 3.1).\n",
    "   \n",
    "(b) Report the accuracy score on the test set when applying your method to deal with OOV words\n",
    "in Part 3.2.\n",
    "   \n",
    "(c) Report the accuracy scores of biLSTM and biGRU on the test set (Part 3.3).\n",
    "   \n",
    "(d) Report the accuracy scores of CNN on the test set (Part 3.4).\n",
    "   \n",
    "(e) Describe your final improvement strategy in Part 3.5. Report the accuracy on the test set\n",
    "using your improved model.\n",
    "   \n",
    "(f) Compare the results across different solutions above and describe your observations with possible discussions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002-nlp-LiAJfnhK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
