{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time to load dataset: 8.2316 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time to load dataset: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       "  'effective but too-tepid biopic',\n",
       "  'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .',\n",
       "  \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"],\n",
       " 'label': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preparing Word Embeddings   \n",
    "As the first step of building your model, you need to prepare the word embeddings to form the\n",
    "input layer of your model. You are required to choose only from Word2vec or Glove to initialize\n",
    "your word embedding matrix. The word embedding matrix stores the pre-trained word vectors\n",
    "(taken from Word2vec or Glove) where each row corresponds to a vector for a specific word in the\n",
    "vocabulary formed from your task dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "EMBEDDING_DIM = 100 # glove embedding are usually 50,100,200,300\n",
    "SAVE_DIR = './result/'\n",
    "VOCAB_PATH = os.path.join(SAVE_DIR, 'vocab.json')\n",
    "EMBEDDING_MATRIX_PATH = os.path.join(SAVE_DIR, 'embedding_matrix.npy')\n",
    "WORD2IDX_PATH = os.path.join(SAVE_DIR, 'word2idx.json')\n",
    "IDX2WORD_PATH = os.path.join(SAVE_DIR, 'idx2word.json')\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_string = train_dataset[4][\"text\"]\n",
    "train_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " \"'s\",\n",
       " \"'t\",\n",
       " ',',\n",
       " '.',\n",
       " 'an',\n",
       " 'and',\n",
       " 'as',\n",
       " 'doesn',\n",
       " 'emerges',\n",
       " 'feel',\n",
       " 'honest',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'keenly',\n",
       " 'like',\n",
       " 'movie',\n",
       " 'observed',\n",
       " 'one',\n",
       " 'rare',\n",
       " 'so',\n",
       " 'something',\n",
       " 'that'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pattern is adapted from GPT2: https://github.com/huggingface/transformers/blob/4fb28703adc2b44ed66a44dd04740787010c5e11/src/transformers/models/gpt2/tokenization_gpt2.py#L167\n",
    "pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d|\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "tokens_re = pattern.findall(train_string)\n",
    "tokens_re = set(tokens_re)\n",
    "tokens_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'internals' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt_tab\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokens_nltk \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(train_string)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/nltk/__init__.py:180\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/nltk/downloader.py:2475\u001b[0m\n\u001b[1;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[1;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \n\u001b[1;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[0;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m Downloader()\n\u001b[1;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/nltk/downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_download_dir()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/nltk/downloader.py:1069\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m-> 1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;66;03m# On Windows, use %APPDATA%\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'internals' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "tokens_nltk = nltk.word_tokenize(train_string)\n",
    "tokens_nltk = set(tokens_nltk)\n",
    "tokens_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'does', \"n't\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_nltk - tokens_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', \"'t\", 'doesn'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_re - tokens_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does seem like NLTK tokenizer performs a better job at tokenizing the overall dataset.\n",
    "In light of that, we will use the tokenizer that was introduced in the lectures\n",
    "to obtain an overall more comprehensive tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset: Dataset) -> set:\n",
    "    \"\"\"Tokenize the text in the dataset using NTLK\n",
    "\n",
    "    :param dataset: The dataset to tokenize\n",
    "    :type dataset: Dataset\n",
    "    :return: The set of tokens in the dataset\n",
    "    :rtype: set\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    \n",
    "    for example in dataset:\n",
    "        tokens = nltk.word_tokenize(example['text'])\n",
    "        vocab.update(tokens)\n",
    "    \n",
    "    print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "\n",
    "    with open(VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(list(vocab), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Vocabulary saved to {VOCAB_PATH}\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab \u001b[38;5;241m=\u001b[39m tokenize(train_dataset)\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 12\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(tokens)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocab)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "vocab = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Word Embedding Matrix with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vocab():\n",
    "    \"\"\"Load Glove vocab\n",
    "\n",
    "    :return: GloVe vocab\n",
    "    :rtype: Set\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe vocab...\")\n",
    "    glove_vocab:set = set()\n",
    "    # https://huggingface.co/datasets/SLU-CSCI4750/glove.6B.100d.txt\n",
    "    dataset = load_dataset(\"SLU-CSCI4750/glove.6B.100d.txt\")\n",
    "    dataset = dataset['train']\n",
    "    \n",
    "    for example in dataset:\n",
    "        word = example[\"text\"].split()[0]\n",
    "        glove_vocab.add(word)\n",
    "    print(\"GloVe vocab loaded.\")\n",
    "    return glove_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe vocab loaded.\n",
      "Size of GloVe vocab: 400000\n",
      "Number of OOV Words: 1867\n"
     ]
    }
   ],
   "source": [
    "glove_vocab = load_glove_vocab()\n",
    "print(f\"Size of GloVe vocab: {len(glove_vocab)}\")\n",
    "\n",
    "oov_words = vocab - glove_vocab\n",
    "print(f\"Number of OOV Words: {len(oov_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings() -> dict:\n",
    "    \"\"\"Load GloVe embeddings\n",
    "\n",
    "    :return: GloVe embeddings\n",
    "    :rtype: Dict\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    glove_dict = {}\n",
    "    word_embedding_glove = load_dataset(\"SLU-CSCI4750/glove.6B.100d.txt\")\n",
    "    word_embedding_glove = word_embedding_glove['train']\n",
    "    \n",
    "    for example in word_embedding_glove:\n",
    "        split_line = example[\"text\"].strip().split()\n",
    "        word = split_line[0]\n",
    "        vector = np.array(split_line[1:], dtype='float32')\n",
    "        glove_dict[word] = vector\n",
    "\n",
    "    print(f\"Total GloVe words loaded: {len(glove_dict)}\")\n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix...\n",
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GloVe words loaded: 400000\n",
      "Embedding matrix built successfully.\n",
      "Embedding matrix saved as './result/embedding_matrix.npy'.\n",
      "Mapping 'word2idx' saved as './result/word2idx.json'.\n",
      "Mapping 'idx2word' saved as './result/idx2word.json'.\n"
     ]
    }
   ],
   "source": [
    "# mapping of words to indices and vice versa\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted(vocab))}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "print(\"Building embedding matrix...\")\n",
    "\n",
    "glove_dict = load_glove_embeddings()\n",
    "\n",
    "for word in vocab:\n",
    "    idx = word2idx[word]\n",
    "    # if word is in glove vocab, use glove vector\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[idx] = glove_dict[word]\n",
    "    else:\n",
    "    # word is not in glove vocab, we remove it from our vocab\n",
    "        # embedding_matrix[idx] = np.zeros(EMBEDDING_DIM)\n",
    "        word2idx.pop(word)\n",
    "        idx2word.pop(idx)\n",
    "\n",
    "print(\"Embedding matrix built successfully.\")\n",
    "\n",
    "np.save(EMBEDDING_MATRIX_PATH, embedding_matrix)\n",
    "print(f\"Embedding matrix saved as '{EMBEDDING_MATRIX_PATH}'.\")\n",
    "\n",
    "with open(WORD2IDX_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'word2idx' saved as '{WORD2IDX_PATH}'.\")\n",
    "\n",
    "with open(IDX2WORD_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'idx2word' saved as '{IDX2WORD_PATH}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(a) What is the size of the vocabulary formed from your training data?   \n",
    "\n",
    "Answer:\n",
    "`16570`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?\n",
    "\n",
    "Answer:\n",
    "`638`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove).\n",
    "Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you\n",
    "think is the best strategy to mitigate such limitation? Implement your solution in your source\n",
    "code. Show the corresponding code snippet. \n",
    "\n",
    "Answer:\n",
    "\n",
    "(1) Using an <UNK> Token, with its Embeddings randomized. Map any OOV words to the <UNK> Token\n",
    "\n",
    "We explore the code snippet below\n",
    "\n",
    "```python\n",
    "for word in extended_vocab:\n",
    "    idx = word2idx[word]\n",
    "    # if word is in glove vocab, use glove vector\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[idx] = glove_dict[word]\n",
    "    else:\n",
    "        # use random vector for unknown words\n",
    "        if word == UNK_TOKEN:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "        else:\n",
    "            embedding_matrix[idx] = embedding_matrix[word2idx[UNK_TOKEN]]\n",
    "```\n",
    "\n",
    "This is a useful strategy as we can use the embeddings of the <UNK> token to\n",
    "represent any unknown words. Thus, now for any unknown words, we can use the\n",
    "<UNK> token to represent them and for the vocabulary words that are not in the\n",
    "pretrained embeddings, we can use the embeddings of the <UNK> token to represent\n",
    "it.\n",
    "\n",
    "(2) There are many kinds of static embeddings. An extension of word2vec, fasttext (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words—words that appear in a test corpus but were unseen in the training corpus.\n",
    "\n",
    "A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols < and > added to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
