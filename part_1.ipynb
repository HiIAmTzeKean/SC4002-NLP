{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time to load dataset: 7.5930 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time to load dataset: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       "  'effective but too-tepid biopic',\n",
       "  'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .',\n",
       "  \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"],\n",
       " 'label': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preparing Word Embeddings   \n",
    "As the first step of building your model, you need to prepare the word embeddings to form the\n",
    "input layer of your model. You are required to choose only from Word2vec or Glove to initialize\n",
    "your word embedding matrix. The word embedding matrix stores the pre-trained word vectors\n",
    "(taken from Word2vec or Glove) where each row corresponds to a vector for a specific word in the\n",
    "vocabulary formed from your task dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "EMBEDDING_DIM = 100 # glove embedding are usually 50,100,200,300\n",
    "SAVE_DIR = './result/'\n",
    "VOCAB_PATH = os.path.join(SAVE_DIR, 'vocab.json')\n",
    "EMBEDDING_MATRIX_PATH = os.path.join(SAVE_DIR, 'embedding_matrix.npy')\n",
    "WORD2IDX_PATH = os.path.join(SAVE_DIR, 'word2idx.json')\n",
    "IDX2WORD_PATH = os.path.join(SAVE_DIR, 'idx2word.json')\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_string = train_dataset[4][\"text\"]\n",
    "train_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " \"'s\",\n",
       " \"'t\",\n",
       " ',',\n",
       " '.',\n",
       " 'an',\n",
       " 'and',\n",
       " 'as',\n",
       " 'doesn',\n",
       " 'emerges',\n",
       " 'feel',\n",
       " 'honest',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'keenly',\n",
       " 'like',\n",
       " 'movie',\n",
       " 'observed',\n",
       " 'one',\n",
       " 'rare',\n",
       " 'so',\n",
       " 'something',\n",
       " 'that'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pattern is adapted from GPT2: https://github.com/huggingface/transformers/blob/4fb28703adc2b44ed66a44dd04740787010c5e11/src/transformers/models/gpt2/tokenization_gpt2.py#L167\n",
    "pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d|\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "tokens_re = pattern.findall(train_string)\n",
    "tokens_re = set(tokens_re)\n",
    "tokens_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from nltk) (4.66.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ngtzekean/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"'s\",\n",
       " ',',\n",
       " '.',\n",
       " 'an',\n",
       " 'and',\n",
       " 'as',\n",
       " 'does',\n",
       " 'emerges',\n",
       " 'feel',\n",
       " 'honest',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'keenly',\n",
       " 'like',\n",
       " 'movie',\n",
       " \"n't\",\n",
       " 'observed',\n",
       " 'one',\n",
       " 'rare',\n",
       " 'so',\n",
       " 'something',\n",
       " 'that'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "tokens_nltk = nltk.word_tokenize(train_string)\n",
    "tokens_nltk = set(tokens_nltk)\n",
    "tokens_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'does', \"n't\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_nltk - tokens_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', \"'t\", 'doesn'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_re - tokens_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does seem like NLTK tokenizer performs a better job at tokenizing the overall dataset.\n",
    "In light of that, we will use the tokenizer that was introduced in the lectures\n",
    "to obtain an overall more comprehensive tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset: Dataset) -> set:\n",
    "    \"\"\"Tokenize the text in the dataset using NTLK\n",
    "\n",
    "    :param dataset: The dataset to tokenize\n",
    "    :type dataset: Dataset\n",
    "    :return: The set of tokens in the dataset\n",
    "    :rtype: set\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    \n",
    "    for example in dataset:\n",
    "        tokens = nltk.word_tokenize(example['text'])\n",
    "        vocab.update(tokens)\n",
    "    \n",
    "    print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "\n",
    "    with open(VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(list(vocab), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Vocabulary saved to {VOCAB_PATH}\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18030\n",
      "Vocabulary saved to ./result/vocab.json\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Word Embedding Matrix with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vocab():\n",
    "    \"\"\"Load Glove vocab\n",
    "\n",
    "    :return: GloVe vocab\n",
    "    :rtype: Set\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe vocab...\")\n",
    "    glove_vocab:set = set()\n",
    "    # https://huggingface.co/datasets/SLU-CSCI4750/glove.6B.100d.txt\n",
    "    dataset = load_dataset(\"SLU-CSCI4750/glove.6B.100d.txt\")\n",
    "    dataset = dataset['train']\n",
    "    \n",
    "    for example in dataset:\n",
    "        word = example[\"text\"].split()[0]\n",
    "        glove_vocab.add(word)\n",
    "    print(\"GloVe vocab loaded.\")\n",
    "    return glove_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe vocab loaded.\n",
      "Size of GloVe vocab: 400000\n",
      "Number of OOV Words: 1867\n"
     ]
    }
   ],
   "source": [
    "glove_vocab = load_glove_vocab()\n",
    "print(f\"Size of GloVe vocab: {len(glove_vocab)}\")\n",
    "\n",
    "oov_words = vocab - glove_vocab\n",
    "print(f\"Number of OOV Words: {len(oov_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings() -> dict:\n",
    "    \"\"\"Load GloVe embeddings\n",
    "\n",
    "    :return: GloVe embeddings\n",
    "    :rtype: Dict\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    glove_dict = {}\n",
    "    word_embedding_glove = load_dataset(\"SLU-CSCI4750/glove.6B.100d.txt\")\n",
    "    word_embedding_glove = word_embedding_glove['train']\n",
    "    \n",
    "    for example in word_embedding_glove:\n",
    "        split_line = example[\"text\"].strip().split()\n",
    "        word = split_line[0]\n",
    "        vector = np.array(split_line[1:], dtype='float32')\n",
    "        glove_dict[word] = vector\n",
    "\n",
    "    print(f\"Total GloVe words loaded: {len(glove_dict)}\")\n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GloVe words loaded: 400000\n",
      "Number of missing words: 1867\n",
      "The missing words are: [\"'artístico\", 'kalesniko', 'old-movie', 'sugar-sweet', 'daneses', \"'bowling\", 'beseechingly', 'musicais', 'stuporously', 'pulpiness', 'gay-niche', \"'under\", 'clarke-williams', 'gender-bender-baller', 'good-bad', 'últimos', 'sermonize', 'cliche-drenched', 'sogginess', 'not-so-bright', 'cinema-besotted', 'flck', 'sock-you-in-the-eye', 'sugar-coating', 'jirí', 'crime-film', 'out-of-kilter', 'marine/legal', 'smashups', 'pseudo-educational', 'out-bad-act', 'blighter', 'mcklusky', 'glass-shattering', 'beat-charged', 'family-film', 'direção', \"'no\", 'multi-layers', 'resultan', 'obligada', 'coriat', \"'wayne\", \"'sophisticated\", \"'some\", 'torna-se', 'thekids', 'nebrida', 'everlyn', 'anti-hollywood', 'bug-eye', 'travel-agency', 'navel-gazing', 'over-indulgent', 'mid-film', 'socio-histo-political', 'runteldat', 'again-courage', 'well-meaningness', 'bling-bling', \"'rare\", \"'worse\", 'stortelling', 'idoosyncratic', 'sequer', 'toe-tapping', 'wewannour', 'long-dreaded', 'ages-old', 'snoots', 'bjorkness', 'montied', 'flex-a-thon', \"'topless\", \"'scream\", 'highly-praised', 'self-', 'rough-around-the-edges', 'makmalbaf', 'bollywood/hollywood', 'hard-to-predict', 'be-all-end-all', 'lump-in-the-throat', 'mini-mod-madness', 'super-cool', 'ballistic-pyrotechnic', 'sitcom-cute', 'self-mutilating', 'camera-work', 'memória', 's1m0ne', 'monster-in-the-', 'shoplifts', 'dialed-up', 'meets-new', 'ultra-cheesy', 'ultra-provincial', 'unsuspenseful', 'crowd-pleaser', 'pep-talk', 'pressure-cooker', 'slowtime', 'condensada', 'substitutable', 'pincel', 'already-shallow', 'heidegger-', 'também', 'hard-to-swallow', 'middle-fingered', 'badly-rendered', 'romantic/comedy', 'often-mined', 'cold-fish', 'stop-and-start', 'complementares', 'racial-issues', 'scene-stealing', 'intelectualmente', 'hamfisted', 'non-god', 'halfwit', 'brazil-like', 'drippiness', 'unemotive', 'something-borrowed', 'dolphin-gasm', 'ultra-loud', 'star-making', 'squaddie', 'well-edited', 'am-radio', 'lame-old', 'back-stabbing', 'non-shakespeare', 'delibrately', 'margolo', 'modern-office', 'skeeved', \"'linklater\", 'entretiene', \"'all\", 'disfrutable', 'oscar-sweeping', 'candy-like', \"'nicholas\", 'hollywood-action', 'emocionante', 'less-than-thrilling', \"'types\", 'wind-in-the-hair', 'doofus-on-', '\\x91direct-to-video', 'expresar', '-greaseballs', 'vintage-tv', 'self-regarding', 'outré', 'and-miss', 'damon/bourne', \"'truth\", 'altman-esque', 'poke-mania', 'bargain-basement', 'às', 'arriesgado', 'not-so-small', \"'twist\", 'phoned-in', 'schneidermeister', 'desnudo', 'not-at-all-good', 'movie-starring', 'out-to-change-the-world', 'updatings', 'else-', \"'get\", 'soul-killing', 'cannier', \"it's-surreal\", 'drama/action', 'live-style', \"'belgium\", 'head-trip', \"'jackie\", \"'right-thinking\", 'sequel-for-the-sake-', 'bang-the-drum', 'two-wrongs-make-a-right', 'quasi-original', 'style-free', 'baseball-playing', 'koshashvili', 'young-guns', 'tongue-tied', 'anteing', 'whoopee-cushion', \"mid-'70s\", 'mesmos', 'un-bear-able', 'possui', 'loosely-connected', 'odd-couple', \"'who\", 'amuse-bouche', 'female-bonding', 'two-drink-minimum', 'então', 'trash-cinema', 'fulford-wierzbicki', 'extra-dry', 'bierbichler', 'pop-cyber', 'docu-makers', 'unsalvageability', 'buddy-comedy', 'ham-fisted', \"'like\", 'love-jealousy-', 'neo-hitchcockianism', \"'masterpiece\", 'wild-and-woolly', 'memory-as-identity', 'focus-grouped', 'perde-se', 'fascinantes', 'money-grubbing', 'cheap-looking', 'atacar', \"'t\", \"'wow\", 'middle-agers', 'co-dependence', 'instante', 'denlopp', 'jean-claud', 'mystery/thriller', 'roteiro', 'blutarsky', \"'aunque\", 'finding-herself', 'period-perfect', 'soul-stripping', 'gosto', 'one-joke', 'bigger-than-life', 'aburrido', 'weapon-derived', 'eye-rolling', 'pseudo-serious', 'mediocridade', 'over-familiarity', 'snake-down-the-throat', 'kafka-inspired', 'string-pulling', 'prejuicios', 'ball-and-chain', 'monster/science', 'medium-grade', 'fuhgeddaboutit', 'oh-those-wacky-brits', 'strafings', 'puportedly', 'only-in', 'shuck-and-jive', 'not-so-hot', 'of-a-sequel', 'rip-roaring', 'little-remembered', 'b-ball', 'drek', 'often-funny', 'spy-action', 'democracie', 'gilliam-esque', 'pigeonhole-resisting', 'direct-to-void', 'gut-busting', 'bolly-holly', 'image-mongering', 'shimmeringly', '1/2-hour', 'talking-head', 'unforgivingly', 'self-infatuated', \"'cq\", 'decades-spanning', 'grandiosa', 'cameo-packed', 'hard-', \"fun-for-fun's-sake\", 'outgag', 'colocar', 'guessable', 'picture-perfect', 'líquido', 'estudo', \"'stock\", 'teen-gang', 'campaign-trail', 'not-so-funny', 'premisa', 'ga-zillionth', 'grunge-pirate', 'shmear', 'cliché-laden', 'emocionalmente', 'snail-like', 'rah-rah', 'dogwalker', 'no-bull', \"'blue\", 'good-naturedness', 'rat-a-tat', 'witch-style', 'dog-paddle', 'bone-chilling', 'gorefests', 'fine-looking', \"'are\", 'hobnail', 'war/adventure', 'oh-so-important', 'derivativeness', 'critic-proof', 'south-of-the-border', \"'60s-homage\", 'stand-up-comedy', 'policiales', 'all-too', 'emotiva', 'director-chef', 'action-fantasy', \"'empowerment\", 'twinkly-eyed', 'splat-man', 'down-and-dirty', 'rintarô', 'video-viewing', 'cyber-horror', 'disappearing/reappearing', 'capturou', 'superficiale', 'sweet-and-sour', 'repellantly', 'unhibited', 'witlessness', 'guilt-free', 'nonchallenging', \"'tradition\", 'ventually', \"'suits\", 'demented-funny', 'janklowicz-mann', 'pathos-filled', 'old-form', 'action-movie', 'waters-like', 'miscasts', 'movie-specific', 'representando', \"so-bad-it's-funny\", 'próprio', 'unreligious', 'director-writer', 'truck-loving', 'low-', 'efteriades', 'actress-producer', 'dull-witted', 'merchant-ivory', 'sub-sophomoric', 'personagens', \"'dragonfly\", 'co-writer/director', 'audience-abuse', 'kidlets', 'meets-john', 'clutchy', 'bliss-less', 'decent-enough', \"'inspirational\", 'audacious-impossible', 'têm', 'entreter', 'ruh-roh', 'age-inspired', 'crash-and-bash', 'scuzbag', 'disney-style', 'tissue-thin', 'too-tepid', 'je-gyu', 'gender-provoking', 'anti-date', 'two-actor', 'long-on-the-shelf', 'super-serious', 'fish-out-of-water', 'independent-community', 'collosum', \"early-'80s\", 'no-surprise', \"'lick\", 'contrária', 'white-trash', 'gone-to-seed', 'precisa', 'wise-beyond-her-years', 'explicados', 'nolden', \"'easier\", 'mean-spiritedness', 'for-fans', 'evolução', 'old-hat', 'melodic/', 'wifty', 'pokepie', 'visualmente', 'reeboir', 'maelström', 'future-world', \"'artistically\", 'episódio', \"'bad\", 'oscar-caliber', 'crass-a-thons', 'graças', 'trend-hoppy', 'light-heartedness', 'spy-on-the-run', 'shlockmeister', 'decidiram', 'dearly-loved', 'action-filled', 'Último', 'sailboaters', \"'interesante\", 'movie-of-the-week', \"'it\", 'intacto', 'excrescence', 'coming-of-age/coming-out', \"'tadpole\", \"'red\", 'single-mindedly', \"'in\", 'steinis', 'humor-seeking', 'award-worthy', 'throat-singing', 'b+', \"'comedian\", 'qatsi', 'haphazardness', 'industrial-model', 'musclefest', 'overstylized', 'uplifter', 'crime-land', \"'top\", 'actuación', 'sillified', 'hyper-realistic', 'playboy-mansion', 'fleshed-out', 'thrift-shop', 'guilt-trip', 'disaffected-indie-film', \"'german-expressionist\", 'majority-oriented', 'chick-flick', 'the-loose', 'dumbfoundingly', 'four-', 'sports-movie', 'prewarned', 'well-contructed', 'done-that', \"'literary\", 'green-guts', 'greasiest', 'often-deadly', 'melodramáticos', 'out-depress', 'touchy-feely', 'logra', 'japanimator', \"'hungry-man\", 'diretor', 'sytle', 'heartwarmingly', 'transforma', 'character-who-shall-', 'premissa', 'ninguém', 'worldly-wise', \"'love\", 'elegiacally', 'buddy-cop', 'eighth-grader', 'boom-bam', 'inconsequentiality', 'cineasts', 'time-vaulting', 'ótimo', 'social/economic/urban', 'pretention', 'projeção', '3-year-olds', 'star/producer', \"'de\", 'kid-empowerment', \"'issues\", 'banderas-lucy', 'holiday-season', 'well-told', 'manga-like', 'hoity-toity', 'incoloro', 'hell-jaunt', 'suspenser', \"'seeking\", 'portent-heavy', 'næs', 'apesar', 'quasi-shakespearean', 'clichês', 'police-oriented', 'work-hours', 'code-talk', 'pretty-boy', 'girl-power', \"'blundering\", 'cat-and-cat', 'espectáculo', 'snow-and-stuntwork', 'felinni', 'universos', 'cinema-and-self', 'french-produced', 'middle-brow', 'bad-luck', 'lástima', 'pop-induced', 'give-a-damn', 'artnering', 'transfigures', 'kaputschnik', 'new-agey', 'lived-in', 'misty-eyed', 'slash-and-hack', 'ápice', \"'great\", 'juiceless', 'off-the-rack', 'gone-to-pot', 'tone-deaf', 'unclassifiably', 'necessidade', 'live-honed', 'pun-laden', 'unembarrassing', 'connect-the-dots', 'jump-in-your-seat', 'half-bad', 'risk-takers', 'digno', 'laser-projected', 'right-thinking', 'silbersteins', \"'small\", 'fill-in-', \"'yes\", 'party-hearty', 're-assess', 'armenian-canadian', 'half-sleep', 'late-twenty-somethings', 'american-russian', 'gaï', 'mind-bending', 'cletis', \"'frailty\", 'retadora', 'pre-shooting', 'gut-buster', '170-minute', 'not-quite-suburban', 'drop-dead', 'ankle-deep', \"so-bad-it's-good\", \"'tonight\", \"'moore\", 'puréed', 'sandlerian', 'foot-age', 'cutting-room', 'blue-light-special', 'oh-so-hollywood', 'genre-busting', '\\x96', 'deadeningly', 'old-world-', 'pollyana', 'book-on-tape', 'life-embracing', 'lovers-on-the-run', \"keep-'em-guessing\", 'reeses', 'materalism', 'movilizador', 'sweet-tempered', 'mass-murdering', 'laissez-passer', 'ningún', 'pussy-ass', 'time-it-is', 'minute-by-minute', 'espite', 'achronological', 'satirizado', 'port-of-call', 'paródia', '-the-cash', 'dialogue-heavy', 'been-there', \"shoot-'em-up\", 'out-of-field', 'idemoto', 'well-trod', 'black-', 'pell-mell', 'captivatingly', 'hammily', 'hyper-artificiality', 'beloved-major-', 'goombah', 'still-contemporary', 'director/co-writer', 'lovable-loser', 'screwed-up', 'kid-movie', 'eye-opener', 'unslick', 'thing-type', 'tardier', \"'easily\", \"'barbershop\", 'super-stupid', 'sleep-inducingly', 'artsploitation', 'chopsocky', 'demencial', \"'jason\", 'wife/colleague', 'sinais', 'swordfights', \"'epic\", '10th-grade', \"'dumb\", 'collage-form', 'melodrama/character', \"'unfaithful\", 'shayamalan', 'quizá', 'waydowntown', 'enfrentados', 'ganha', 'head-turner', 'versión', 'kinetically-charged', 'splatterfests', 'andamento', 'semi-coherent', \"'chick\", \"'stoked\", 'non-firsthand', 'brain-slappingly', 'desee', 'ming-liang', '99-minute', 'channel-style', 'culminando', 'low-heat', 'out-sized', 'drenched-in-the-', 'boom-box', 'constatação', 'not-great', 'profundamente', 'consciousness-raiser', 'brothers/abrahams', 'corporate-sports', 'bait-and-tackle', \"'comedy\", 'costumey', 'patriotero', 'shiver-inducing', 'flame-like', 'solondzian', 'bodice-ripper', 'hidden-agenda', 'incompetência', 'oscar-worthy', 're-hash', \"'dog\", \"'home\", '\\x9170s', \"'swimfan\", 'pseudo-hip', 'action-thriller/dark', 'mind-destroying', 'achival', 'friday-night', 'from-television', \"'very\", 'yawn-provoking', 'shapable', 'video-shot', 'hayseeds-vs', 'redneck-versus-blueblood', 'not-quite-urban', 'large-frame', 'loosey-goosey', 'unamusing', 'criar', 'thesps', 'divertida', 'junk-food', \"'christian\", 'bottomlessly', \"'let\", 'unplundered', 'tv-style', 'skid-row', 'stunt-hungry', 'quick-cut', 'meanspirited', \"'fish\", 'untugged', 'obviation', '4/5ths', 'big-bug', 'hack-artist', \"'evelyn\", 'patient/doctor', 'single-mindedness', \"'cultural\", 'strip-mined', 'singer-turned', 'docu-dogma', 'ya-yas', \"'tron\", 'debuter', 'teen-catholic-movie', 'aproveitar', 'i-2-spoofing', 'murder-on-campus', 'self-revealing', 'bizzarre', 'techno-sex', \"'synthetic\", 'accordion/harmonica/banjo', 'inside-show-biz', 'too-facile', 'warmed-over', 'sub-aquatic', \"'picture\", \"'baran\", 'mood-altering', \"'divertida\", 're-fried', 'ho-tep', \"'korean\", 'overmanipulative', 'trailer-trash', 'fear-inducing', 'just-above-average', 'remain-nameless', 'auteil', 'allodi', 'verete', 'sense-of-humour', 'kid-pleasing', '-doing-it-for', 'raw-nerved', 'shapelessly', 'tear-stained', 'still-inestimable', 'fleet-footed', 'vulakoro', 'destinees', 'fast-edit', 'conan-esque', 'self-promoter', 'spell-casting', 'open-hearted', 'years/', 'eroti-comedy', \"soul's-eye\", 'unlaughable', 'self-amused', 'half-asleep', 'sitcomishly', 'cat-and-mouser', 'enviará', 'cheap-shot', 'spook-a-rama', 'certamente', 'less-is-more', 'stable-full', 'siuation', \"'snow\", 'equlibrium', 'wince-inducing', 'company-style', '-stunning', 'imponderably', 'cliche-bound', 'hopped-up', 'war-movie', 'actuada', 'nit-picky', 'botch-jobs', 'stop-go', \"'god\", 'nickelodeon-esque', 'spaniel-eyed', 'saímos', 'laser-beam', 'guy-in-a-dress', \"'brazil\", 'third-act', \"'face\", 'originalidad', 'nerfs', 'kibbitzes', 'accomodates', 'poster-boy', 'same-old', 'off-', 'escapa', 'freak-out', 'hyper-cliched', '-hollywood', 'marveilleux', 'difficult-to-swallow', 'nail-biter', 'food-for-thought', \"'urban\", 'small-budget', 'guión', 'heart-on-its-sleeve', 'superlarge', 'grade-grubbers', 'punch-drunk', 'tear-jerking', 'dreadfulness', 'title-bout', 'less-than-objective', 'sub-formulaic', 'scene-by-scene', 'petin', 'post-9/11', 'whip-crack', 'mothball-y', 'curse-free', 'gangster/crime', 'teen-speak', 'adventues', 'adapted-', 'stumblings', 'stalk-and-slash', 'baca-asay', 'foot-dragging', 'lhe', 'sincera', 'suspeito', 'near-masterpiece', 'self-dramatizing', 'existência', 'uberviolence', 'car-wreck', \"'bartleby\", 'bond-inspired', \"'ejemplo\", 'ihops', 'over-amorous', \"'blood\", 'smile-button', 'snazziness', \"'abandon\", 'well-honed', 'age-wise', \"'grandeur\", 'well-put-together', 'challenge-hungry', 'super-dooper-adorability', \"'film\", \"'assassin\", 'not-so-big', 'glad-handing', \"'classic\", 'hard-to-believe', 'character-oriented', '-after', 'goose-pimple', 'prefeminist', 'happily-ever', 'gabbiest', 'laugh-filled', 'in-the-ring', 'jae-eun', \"'funny\", 'half-wit', '-white', 'diferença', 'girl-woman', 're-', 'true-blue', \"'life\", \"'analyze\", 'clung-to', 'ryanovich', 'lifetime-channel', 'corruscating', \"show-don't-tell\", 'government/', \"'ah\", \"'compleja\", 'wisegirls', 'real-live', 'kosashvili', 'meet-cute', 'eye-filling', 'sorprenderá', 'giggle-inducing', 'slice-of-depression', 'unconned', 'upper-crust', 'spring-break', 'candy-coat', 'paint-by-number', '-a', 'ever-watchful', 'toolbags', 'oscar-winners', 'chou-chou', \"ain't-\", 'voices-from-the-other-side', 'pseudo-intellectual', 'brit-com', 'over-25s', 'travil', 'predecesora', 'disease-of-the-week', 'continuação', 'prechewed', 'ultra-manipulative', 'aborbing', \"'safe\", 'oh-so', 'mix-and-', '179-minute', 'nutjob', 'mother/daughter', '\\x91what', \"'drumline\", 'high-wattage', 'art-conscious', 'bone-crushing', 'two-hour-and-fifteen-minute', 'soaringly', 'cringe-inducing', 'deseos', 'b-scene', 'ink-and-paint', 'superada', 'dridi', \"'tap\", 'psychodramatics', 'overplotted', \"'ace\", 'off-puttingly', 'covardia', 'diciness', 'tv-insider', 'qutting', 'well-detailed', 'do-over', 'video-game-based', 'kahlories', 'preocupe', 'strainingly', \"'memento\", 'direto', 'silly-looking', 'fever-pitched', 'dust-caked', 're-do', 'russos', \"'why\", 'disney-fied', 'repulsively', 'shrieky', 'orquídeas', 'bump-in', 'retro-refitting', 'disease-of-', 'nohe', 'well-observed', 'jar-jar', \"'chan\", 'soft-porn', 'fear-reducing', 'monkeyfun', \"'edgy\", 'cor-blimey-luv-a-duck', 'raunch-fests', 'mullinski', 'gooeyness', 'handbag-clutching', 'easter-egg-colored', 'twist-and-turn', 'ugly-duckling', 'show-stoppingly', 'animated-movie', 'queasy-stomached', \"'zany\", 'opera-to-film', 'grato', 'unencouraging', 'whimsicality', 'turkey-on-rolls', 'wishy-washy', 'body-switching', 'six-packs', 'deutchland', 'inquestionável', 'retrata', \"'men\", 'shakesperean', 'one-sidedness', 'dateflick', 'pee-related', 'fuddled', 'too-spectacular', 'over-romanticize', 'complejos', 'responsável', 'hellstenius', 'recoing', 'sumamente', 'kid-vid', 'narcotizing', 'mergulha', 'italianas', 'roisterous', 'male-ridden', 'walking-dead', \"'girls\", 'tear-drenched', \"'this\", 'pg-rated', 'enrapturing', 'rocky-like', 'overcoming-obstacles', 'bar-scrapping', 'claustrophic', 'conflict-powered', 'cadness', 'dysfunctionally', 'cinemantic', 'gender-war', 'consegue', 'buzz-obsessed', 'shoe-loving', 'junk-calorie', \"'amateur\", 'field-sized', 'kilt-wearing', 'dominatrixes', \"'spectacular\", 'blood-curdling', 'b-film', 'fang-baring', 'phonce', 'double-', 'strung-together', 'precollegiate', 'crummles', 'apallingly', 'unrecommendable', 'ill-starred', 'dumbed-down', 'gantzes', \"'jackass\", 'overly-familiar', 'self-glorified', 'stadium-seat', '-inevitable', 'perfervid', 'datedness', 'sixties-style', 'hyper-real', '112-minute', 'girl-on-girl', '79-minute', 'teen-exploitation', \"'hannibal\", 'funny/gritty', 'opera-ish', 'fire-red', 'show-tunes', 'step-printing', \"'laughing\", 'timewaster', 'fully-written', 'sweaty-palmed', 'preciosista', 'contemplarse', 'time-switching', 'out-shock', 'heart-tugging', 'seventy-minute', 'hard-sell', 'ozpetek', 'seldahl', 'self-exploitation', 'brain-deadening', 'kiddie-oriented', 'alientation', 'vidgame', 'sticky-sweet', 'warm-milk', 'migraine-inducing', \"'there\", 'stuffiest', \"'scratch\", 'caper-comedy', 'bore-athon', 'autocritique', 'lynch-like', \"'charly\", 'ineptitudes', 'close-to-solid', 'east-vs', 'triple-espresso', 'contando', 'lascivious-minded', 'triple-crosses', \"'fatal\", 'teen-targeted', 'go-round', 'killer-thrillers', 'must-own', 'conmovedora', 'heart-affecting', 'long-faced', 'real\\x97she', \"'they\", 'uncinematic', 'lesser-praised', \"'terrible\", 'by-the-numbers', 'creature-feature', '8217', 'surface-effect', 'once-over', 'wish-fulfilling', 'film-culture', 'sober-minded', 'actorish', 'estranhos', 'pouty-lipped', 'sex-reassignment', 'oídos', 'makeup-deep', \"'garth\", 'second-guess', 'thriller-noir', \"'enigma\", 'impotentes', \"'blade\", 'dead-undead', 'atreve', 'well-wrought', 'sychowski', 'techno-saturation', 'off-hollywood', 'writer/director/producer', \"'uhhh\", 'igualmente', 'post-colonialist', 'teeth-clenching', \"'significant\", 'stage-trained', 'near-xenophobic', '28k', 'hollywood-predictable', 'cirulnick', \"bull's-eye\", \"'alabama\", \"'real\", 'powaqqatsi', '\\x97', 'saído', 'a-knocking', 'románticas', \"'best\", \"'carente\", 'wham-bam', 'edge-of-your-seat', 'all-in-all', 'freak-outs', 'overemphatic', 'kickass', 'star-power', 'schlock-filled', 'gore-free', 'well-meant', 'pie-type', 'natural-seeming', 'slam-bam', 'preocupar', 'scary-funny', 'leatherbound', 'human-scale', 'sarcástica', 'pie-like', 'hyper-time', \"'do\", 'gut-bustingly', 'smarty-pants', \"'you\", 'kiddie-flick', 'too-long', 'save-the-planet', 'divertingly', 'munchausen-by-proxy', \"'triumph\", 'jaglomized', \"'requiem\", 'blown-out', 'comedy-deficient', 'sappier', 'lip-non-synching', 'interações', \"'bold\", 'wise-cracker', 'brush-up', 'estafeta', 'snap-crackle', \"'alternate\", 'old-fashioned-movie', \"'surprises\", 'heartbeat-like', 'huge-screen', 'glacier-paced', 'like-themed', 'well-realized', 'fluxing', 'brother-man', 'bottom-feeder', 'smart-aleck', 'street-realist', 'inhospitability', 'college-spawned', 'teeny-bopper', \"'magnifique\", 'pseudo-bio', 'euro-film', 'tolerable-to-adults', 'date-night', 'rápidamente', 'surfacey', 'dewy-eyed', 'emptily', 'dead-eyed', 'acabamos', 'romething', 'spy-movie', '129-minute', 'barn-burningly', 'she-cute', \"'intro\", 'b-flick', 'hirosue', 'all-wise-guys-all-the-time', 'semi-surrealist', 'rambo-', 'still-raw', 'narcotized', 'so-inept-', 'dog-tag', 'sitcom-worthy', 'half-formed', \"'deadly\", \"'action\", 'assistir', 'péssima', 'estava', 'snore-fest', 'convencional', 'cool-j', 'the-blanks', 'live-wire', 'character-', 'self-congratulatory', 'cativante', 'elemento', '3/4th', 'copmovieland', 'interspliced', 'consumer-advice', 'mibii', 'slap-happy', 'espetáculo', 'low-wattage', 'masterpeice', \"'james\", 'non-britney', 'narrativa', 'bowel-curdling', 'star-splashed', 'hawk-style', 'wound-licking', 'likableness', 'neo-fascism', 'começamos', \"'lovely\", 'light-footed', 'exotic-looking', 'girl-meets-girl', 'ever-escalating', \"'children\", 'out-outrage', 'quasi-improvised', 'stultifyingly', \"'anyone\", 'bruckheimeresque', 'rusted-out', 'downy-cheeked', 'side-splittingly', 'cliche-riddled', 'malfitano-domingo', 'half-lit', 'not-so-stock', 'thinly-conceived', 'engaña', 'best-foreign-film', 'grace-in-rebellion', 'techno-horror', 'full-throated', 'ho-ho-ho', 'bait-and-switch', \"'possession\", 'hard-partying', 'amusedly', 're-voiced', 'luvvies', 'haunted-house', 'zinger-filled', 'lower-wit', 'well-conceived', \"'credit\", 'well-mounted', 'self-caricature', 'slo-mo', 'double-pistoled', 'dullingly', 'white-knuckled', 'flatula', \"'perfection\", 'esfera', 'desfecho', 'choquart', 'speeds/', 'pro-fat', 'pencil-thin', \"'vain\", 'eyeball-to-eyeball', 'media-constructed', 'talancón', 'romance-novel', 'dark-as-pitch', 'christmas-tree', 'the-week', 'sun-drenched', 'crime-busting', \"'chops\", \"'reality\", 'headbangingly', 'kasem-furnished', 'stomach-turning', 'delicia', 'class-', \"'have-yourself-a-happy-little-holocaust\", 'than-likely', 'revigorates', 'semi-stable', 'brûlée', 'entretenida', \"'solaris\", 'drama/character', 'over-dramatic', 'joylessly', 'pretenciosas', 'boundary-hopping', 'minac', 'miedos', 'enfrentará', 'soon-to-be-forgettable', 'first-timer', 'dime-store', 'esquerdo', 'heremakono', 'decirles', 'all-too-familiar', '-west', 'absolutamente', 'sad-sack', 'luv-spreading', 'screen-eating', 'multi-character', 'pile-ups', \"'sacre\", 'rose-tinted', 'wonder-what-', \"'si\", \"'hosts\", 'neo-augustinian', \"'solid\", 'hour-and-a-half', 'nrelentingly', 'funcionar', 'super-wealthy', 'rubber-face', 'kiddie-friendly', \"'qatsi\", 'beast-within', 'komediant', 'única', 'frissons', 'bergmanesque', 'crowdpleaser', 'control-alt-delete', 'early-on', 'animé', 'pseudo-sophisticated', 'kitchen-sink', 'valley-girl', 'made-for-movie', 'meat-and-potatoes', 'romijn-stamos', 'media-soaked', 'plaintiveness', 'all-over-the-map', 'italicizes', 'hanky-panky', \"'juvenile\", 'simbolizando', 'frankenstein-monster', 'kill-by-numbers', 'unentertaining', \"'what\", 'phoney-feeling', 'not-quite', 'utilizar', 'under-rehearsed', 'affectation-free', '-the-night', 'bazadona', 'watstein', 'indie-heads', \"'estupendamente\", \"'swept\", 'aceitou', 'not-so-divine', 'non-bondish', 'fuddy-duddy', 'unfakable', 'poorly-constructed', 'birot', 'movie-star', 'hoo-ha', 'djeinaba', 'a-bornin', 'anti-darwinian', 'water-bound', 'término', 'kids-and-family-oriented', 'white-empowered', 'squirm-inducing', 'front-loaded', 'reconceptualize', 'forgettably', 'volletta', 'bible-study', \"'butterfingered\", 'post-camp', 'head-banging', 'low-cal', \"'praise\", \"'tweener\", 'grand-scale', 'nietzsche-referencing', 'writer-producer-director', \"'frankly\", 'good-deed/bad-deed', 'documentary-making', 'flim-flam', 'mollà', \"'videodrome\", 'go-for-broke', 'skippable', 'unimpressively', 'pseudo-rock-video', 'laugh-a-minute', 'tv-to-movie', 'ourside', 'sleep-inducing', 'addessi', 'mouglalis', 'beat-the-clock', 'none-too-original', 'not-being', 'jaw-droppingly', 'fílmica', 'mid-section', 'bottom-rung', 'by-the-books', \"'manhunter\", 'not-quite-dead', 'handsome-looking', 'corniest', 'dead-center', 'manqué', 'brothers-style', 'farewell-to-innocence', 'hit-hungry', 'mistaken-identity', 'chloroform-soaked', 'actory', 'epic-horror', 'push-the-limits', 'gasp-inducing', 'papai', 'affirmational', 'ricture', \"cam'ron\", \"'laugh\", 'semi-humorous', 'self-congratulation', 'headline-fresh', 'pin-like', 'shagster', 'pianista', 'prescinde', 'hastier', 'landbound', 'decasia', \"'how\", 'roteirista', 'surface-obsession', 'irony-free', 'hit-', 'sex-as-war', 'played-out', 'exporing', 'zzzzzzzzz', 'action/effects', 'b-minus', 'post-full', 'less-than-magic', 'faux-contemporary', 'new/old', 'ooky-spookies', 'higuchinsky', 'indieflick', \"'ick\", 'gymkata', 'spiritual-uplift', 'saucer-eyed', 'dead-eye', 'digital-video', 'dogma-like', 'plot-wise', 'bone-dry', 'chabrolian', 'talk-heavy', 'skyscraper-trapeze', 'non-reactionary', 'tough-man', 'intentando', 'homo-eroticism', 'dès', 'well-shot', 'fresh-squeezed', 'estrogen-free', 'imaginación', 'action/comedy', 'leplouff', \"'drama\", 'rap-metal', \"aren't-kids-cute\", \"'uplifting\", 'ho-hum', \"'too\", 'fustily', 'soberbio', 'predecible', 'flick-knife', 'vietnamese-born', 'open-endedness', 'message-mongering', 'manipulador', 'peace-and-love', 'one-trick', 'blank-faced', \"hors-d'oeuvre\", 'lapdance', 'cliff-notes', 'colosal', 'stiff-upper-lip', 'ultra-low-budget', 'stagecrafts', \"'angels\", 'kalvert', 'money-oriented', 'provocatuers', \"'santa\", 'often-hilarious', 'kazmierski', \"'performance\", 'slam-bang', 'mcbeal-style', 'and-', 'dudsville', 'wafer-thin', 'heart-string', \"'just\", 'art-directed', 'self-glorification', 'disposible', 'non-mystery', 'fantasti', 'realidade', 'annie-mary', 'slasher-movie', 'nonethnic', 'penotti', 'prep-school', 'acting-workshop', 'amoses', 'rough-trade', 'guilty-pleasure', 'you-are-there', 'less-compelling', 'morning-glory', \"'naturalistic\", \"'challenging\", \"mid-'90s\", 'crappola', 'weirded-', 'cliche-ridden', \"'refreshing\", 'snazzy-looking', 'É', 'tv-cops', 'gang-infested', 'frustrado', 'digital-effects-heavy', 'under-inspired', 'início', \"'orphans\", 'coma-like', 'recurre', 'pseudo-witty', \"'hey\", \"'shindler\", 'european-set', 'slash-dash', \"stalk'n'slash\", 'too-conscientious', 'sheerly', \"'scooby\", \"'co-stars\", 'jazz-playing', \"'my\", \"'been\", 'post-saving', \"'the\", 'humbuggery', 'television-', 'groan-inducing', 'flower-power', 'self-destructiveness', \"'guests\", 'less-than-compelling', 'smack-dab', 'razor-sided', 'genial-rogue', 'daytime-drama', 'wankery', 'self-defeatingly', 'eardrum-dicing', 'gator-bashing', 'hitler-study', 'acontecimentos', 'esforço', 'half-dimensional', 'uncharismatically', 'surehanded', '-of-the-week', 'giant-screen', 'cliché-riddled', 'hour-and-a-half-long', 'paint-by-numbers', 'laboriousness', 'non-exploitive', 'fato', 'teary-eyed', 'flakeball', 'surfer-girl', 'rise-and-fall', 'mind-numbingly', 'duración', 'movie\\x97if', 'colonics', 'cop-flick', 'singer/composer', 'jeong-hyang', 'desaponta', 'involvingly', 'fluff-ball', 'gal-pal', 'perseguição', 'cipherlike', 'community-therapy', 'razzle-dazzle', 'blade-thin', 'gutterball', 'super-', 'stress-reducing', 'self-serious', 'abandone', 'messing-about', 'mid-to-low', 'truncheoning', 'screeching-metal', 'esteticamente', 'mad-libs', 'peter/spider-man', 'camareras', 'therapy-dependent', 'shock-you-into-laughter', 'exhilarate', 'havia', 'sex-soaked', 'salaciously', \"matrix'-style\", 'sucker-punch', 'hat-in-hand', 'fizzability', 'self-involved', \"'true\", 'poo-poo', 'hotdogging', 'near-disaster', 'force-feed', 'hjelje', 'even-toned', 'ear-splitting', 'enternecedora', 'desarrollarse', 'poor-me', \"'if\", 'hit-to-miss', \"'silence\", 'out-stealth', 'aqueles', 'atacarse', 'clericks', 'slam-dunk', 'groan-to-guffaw', 'nerve-rattling', 'ego-destroying', 'technology-of-the-moment', 'woman-', 'oft-described', 'slummy', 'otto-sallies', 'non-porn', 'horror/action', \"'thank\", 'witch-', 'hubac', 'super-simple', 'amped-up', 'cheese-laced', 'montaje', 'big-fisted', 'exibi-lo', \"'we\", 'who-wrote-shakespeare', 'montias', 'heavy-handedness', 'pouquíssimos', '102-minute', 'jacked-up', 'pérdida', \"'guy\", 'semi-improvised', 'fillm', 'anti-kieslowski', \"'difficult\", 'girls-behaving-badly', 'villians', 'bio-doc', 'skate/surf', 'francamente', 'substance-free', \"'date\", \"'quit\", 'sequel-itis', 'self-flagellation', 'girl-', 'ki-deok', 'plot-lines', 'creepy-crawly', 'sparklingly', 'glizty', 'less-', 'feardotcom', 'mock-tarantino', 'perdona', 'get-out', 'dictator-madman', 'not-exactly', 'techno-tripe', 'often-cute', 'underdramatized', 'half-assed', 'right-on', 'political-action', 'ill-wrought', 'comedy/thriller', 'hotsies', 'clunk-on-the-head', 'comic/thriller', 'globetrotters-generals', 'wollter', 'marcken', 'movie-esque', 'cutesy-pie', 'parka-wrapped', 'nuttgens', 'bibbidy-bobbidi-bland', \"'woods\", 'gerbosi', 'rape-payback', 'thrill-kill', 'culture-clash']\n",
      "Building embedding matrix...\n",
      "Vocab size: 16163\n",
      "\n",
      "Embedding matrix built successfully.\n",
      "Embedding matrix saved as './result/embedding_matrix.npy'.\n",
      "Mapping 'word2idx' saved as './result/word2idx.json'.\n",
      "Mapping 'idx2word' saved as './result/idx2word.json'.\n"
     ]
    }
   ],
   "source": [
    "glove_dict = load_glove_embeddings()\n",
    "\n",
    "# Collect words to be removed\n",
    "missing_words = []\n",
    "for word in vocab:\n",
    "    if word not in glove_dict:\n",
    "        missing_words.append(word)\n",
    "\n",
    "# Remove missing words from vocab\n",
    "for word in missing_words:\n",
    "    vocab.remove(word)\n",
    "        \n",
    "print(f\"Number of missing words: {len(missing_words)}\")\n",
    "print(f\"The missing words are: {missing_words}\")\n",
    "\n",
    "# mapping of words to indices and vice versa\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted(vocab))}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(\"Building embedding matrix...\")\n",
    "vocab_size = len(word2idx)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    embedding_matrix[idx] = glove_dict[word]\n",
    "\n",
    "print()\n",
    "print(\"Embedding matrix built successfully.\")\n",
    "\n",
    "np.save(EMBEDDING_MATRIX_PATH, embedding_matrix)\n",
    "print(f\"Embedding matrix saved as '{EMBEDDING_MATRIX_PATH}'.\")\n",
    "\n",
    "with open(WORD2IDX_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'word2idx' saved as '{WORD2IDX_PATH}'.\")\n",
    "\n",
    "with open(IDX2WORD_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'idx2word' saved as '{IDX2WORD_PATH}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding matrix:  (16163, 100)\n",
      "Data type of the elements:  float64\n",
      "First few rows of the embedding matrix: \n",
      "[ 0.38472     0.49351001  0.49096    -1.54340005 -0.33614001  0.62220001\n",
      "  0.32264999  0.075331    0.65591002 -0.23517001  1.21140003  0.06193\n",
      " -0.62004     0.31371     0.38947999 -0.24381    -0.065643    0.58797002\n",
      " -0.86382002  0.63165998  0.68362999  0.39647001 -0.62388003 -0.25094\n",
      "  0.92830998  1.51520002 -0.43917     0.22249     1.36950004 -0.53097999\n",
      "  0.39811     0.77113998  0.49043     0.58853     0.2376      0.31619999\n",
      " -0.011962   -0.047074    0.34584999 -1.29439998  0.18596999  0.27002001\n",
      " -0.70602    -0.20652001 -0.25194001 -0.48679999 -0.71538001 -0.23886999\n",
      " -0.041612   -0.55488002 -0.54225999  0.21235999  0.025341    0.96517003\n",
      " -0.88182998 -1.86810005  0.32657     1.16890001  1.17589998 -0.17393\n",
      " -0.3371      0.87535    -1.01139998 -0.61809999  1.00800002  0.31505999\n",
      "  0.24417     0.064393    0.33678001  0.33632001  0.45975     0.22813\n",
      " -0.37505001 -0.37507999  0.089301    0.53862     0.039714   -0.0036392\n",
      " -0.25023001 -0.18223999  0.42730999 -0.79118001 -0.29409    -0.40693\n",
      " -1.09080005 -0.16475999 -0.41457999 -0.67899001  0.28319001  0.30937001\n",
      "  0.49304    -0.067002    0.50221997  0.73958999 -0.47350001 -0.47341999\n",
      " -0.20242     0.026263    0.39052001  0.52217001]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Load the embedding matrix\n",
    "embedding_matrix = np.load('result/embedding_matrix.npy')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Shape of the embedding matrix: \", embedding_matrix.shape)\n",
    "print(\"Data type of the elements: \", embedding_matrix.dtype)\n",
    "\n",
    "# Display the first few entries\n",
    "print(\"First few rows of the embedding matrix: \")\n",
    "print(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try to define a class that will help us to handle the loading and the\n",
    "handling of the embedding matrix! This will help with the downstream tasks as well.\n",
    "\n",
    "We define core APIs that will help us to load the embeddings and also to get the\n",
    "embeddings for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class EmbeddingMatrix():\n",
    "    def __init__(self) -> None:\n",
    "        self.d = 0 \n",
    "        self.v = 0\n",
    "        self.pad_idx:int\n",
    "        self.embedding_matrix:np.ndarray\n",
    "        self.word2idx:dict\n",
    "    @classmethod\n",
    "    def load(cls) -> \"EmbeddingMatrix\":\n",
    "        # load vectors from file\n",
    "        embedding_matrix:np.ndarray = np.load(EMBEDDING_MATRIX_PATH)\n",
    "        # set attributes\n",
    "        em = cls()\n",
    "        em.embedding_matrix = embedding_matrix\n",
    "        \n",
    "        with open(WORD2IDX_PATH, 'r', encoding='utf-8') as f:\n",
    "            word2idx:dict = json.load(f)\n",
    "            em.word2idx = word2idx\n",
    "        em.v, em.d = embedding_matrix.shape\n",
    "        return em\n",
    "    @property\n",
    "    def to_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(self.embedding_matrix, dtype=torch.float64)\n",
    "    def add_padding(self) -> None:\n",
    "        if \"<PAD>\" in self.word2idx:\n",
    "            return\n",
    "        padding = np.zeros((1, self.d), dtype='float32')\n",
    "        self.embedding_matrix = np.vstack((self.embedding_matrix, padding))\n",
    "        \n",
    "        self.v += 1\n",
    "        self.pad_idx = self.v - 1\n",
    "        self.word2idx[\"<PAD>\"] = self.pad_idx \n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        \"\"\"Dimension of the embedding matrix\n",
    "        \n",
    "        :return: The dimension of the embedding matrix\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.d\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Vocabulary size of the embedding matrix\n",
    "\n",
    "        :return: The vocabulary size of the embedding matrix\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.v\n",
    "    @property\n",
    "    def vocab(self) -> set[str]:\n",
    "        \"\"\"Vocabulary of the embedding matrix\n",
    "        \n",
    "        Set of words in the embedding matrix\n",
    "\n",
    "        :return: The vocabulary of the embedding matrix\n",
    "        :rtype: set[str]\n",
    "        \"\"\"\n",
    "        return set(self.word2idx.keys())\n",
    "    def __getitem__(self, word:str) -> np.ndarray:\n",
    "        return self.embedding_matrix[self.word2idx[word]]\n",
    "    def get_idx(self, word:str) -> int:\n",
    "        # if word not in vocab, return None\n",
    "        return self.word2idx.get(word, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1. Word Embedding\n",
    "(a) What is the size of the vocabulary formed from your training data?   \n",
    "The size of the vocabulary is `18030`, after removing the OOV words from Glove embeddings, the\n",
    "final size of the vocabulary is `16163`.\n",
    "\n",
    "(b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?    \n",
    "`1867`\n",
    "   \n",
    "(c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove).\n",
    "Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you\n",
    "think is the best strategy to mitigate such limitation? Implement your solution in your source\n",
    "code. Show the corresponding code snippet. \n",
    "\n",
    "Answer:\n",
    "\n",
    "(1) Using an <UNK> Token, with its Embeddings randomized. Map any OOV words to the <UNK> Token\n",
    "\n",
    "We explore the code snippet below\n",
    "\n",
    "```python\n",
    "for word in extended_vocab:\n",
    "    idx = word2idx[word]\n",
    "    # if word is in glove vocab, use glove vector\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[idx] = glove_dict[word]\n",
    "    else:\n",
    "        # use random vector for unknown words\n",
    "        if word == UNK_TOKEN:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "        else:\n",
    "            embedding_matrix[idx] = embedding_matrix[word2idx[UNK_TOKEN]]\n",
    "```\n",
    "\n",
    "This is a useful strategy as we can use the embeddings of the <UNK> token to\n",
    "represent any unknown words. Thus, now for any unknown words, we can use the\n",
    "<UNK> token to represent them and for the vocabulary words that are not in the\n",
    "pretrained embeddings, we can use the embeddings of the <UNK> token to represent\n",
    "it.\n",
    "\n",
    "(2) There are many kinds of static embeddings. An extension of word2vec, fasttext (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words—words that appear in a test corpus but were unseen in the training corpus.\n",
    "\n",
    "A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols < and > added to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
