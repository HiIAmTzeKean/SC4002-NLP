{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-macosx_10_9_x86_64.whl (31 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
<<<<<<< Updated upstream
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
=======
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\SC4002-NLP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Elapsed time to load dataset: 8.2316 seconds\n"
=======
      "Elapsed time to load dataset: 8.7754 seconds\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time to load dataset: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 10,
=======
   "execution_count": 3,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 10,
=======
     "execution_count": 3,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 11,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       "  'effective but too-tepid biopic',\n",
       "  'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .',\n",
       "  \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"],\n",
       " 'label': [1, 1, 1, 1, 1]}"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 11,
=======
     "execution_count": 4,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preparing Word Embeddings   \n",
    "As the first step of building your model, you need to prepare the word embeddings to form the\n",
    "input layer of your model. You are required to choose only from Word2vec or Glove to initialize\n",
    "your word embedding matrix. The word embedding matrix stores the pre-trained word vectors\n",
    "(taken from Word2vec or Glove) where each row corresponds to a vector for a specific word in the\n",
    "vocabulary formed from your task dataset.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "EMBEDDING_DIM = 100 # glove embedding are usually 50,100,200,300\n",
    "SAVE_DIR = './result/'\n",
    "VOCAB_PATH = os.path.join(SAVE_DIR, 'vocab.json')\n",
    "EMBEDDING_MATRIX_PATH = os.path.join(SAVE_DIR, 'embedding_matrix.npy')\n",
    "WORD2IDX_PATH = os.path.join(SAVE_DIR, 'word2idx.json')\n",
    "IDX2WORD_PATH = os.path.join(SAVE_DIR, 'idx2word.json')\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_string = train_dataset[4][\"text\"]\n",
    "train_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " \"'s\",\n",
       " \"'t\",\n",
       " ',',\n",
       " '.',\n",
       " 'an',\n",
       " 'and',\n",
       " 'as',\n",
       " 'doesn',\n",
       " 'emerges',\n",
       " 'feel',\n",
       " 'honest',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'keenly',\n",
       " 'like',\n",
       " 'movie',\n",
       " 'observed',\n",
       " 'one',\n",
       " 'rare',\n",
       " 'so',\n",
       " 'something',\n",
       " 'that'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pattern is adapted from GPT2: https://github.com/huggingface/transformers/blob/4fb28703adc2b44ed66a44dd04740787010c5e11/src/transformers/models/gpt2/tokenization_gpt2.py#L167\n",
    "pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d|\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "tokens_re = pattern.findall(train_string)\n",
    "tokens_re = set(tokens_re)\n",
    "tokens_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/nicoleyap/opt/anaconda3/envs/nn/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
=======
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nicoleyap/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'internals' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt_tab\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokens_nltk \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(train_string)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/nltk/__init__.py:180\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/nltk/downloader.py:2475\u001b[0m\n\u001b[1;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[1;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \n\u001b[1;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[0;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m Downloader()\n\u001b[1;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/nltk/downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_download_dir()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nn/lib/python3.12/site-packages/nltk/downloader.py:1069\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m-> 1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;66;03m# On Windows, use %APPDATA%\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'internals' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "tokens_nltk = nltk.word_tokenize(train_string)\n",
    "tokens_nltk = set(tokens_nltk)\n",
    "tokens_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'does', \"n't\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_nltk - tokens_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', \"'t\", 'doesn'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_re - tokens_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does seem like NLTK tokenizer performs a better job at tokenizing the overall dataset.\n",
    "In light of that, we will use the tokenizer that was introduced in the lectures\n",
    "to obtain an overall more comprehensive tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset: Dataset) -> set:\n",
    "    \"\"\"Tokenize the text in the dataset using NTLK\n",
    "\n",
    "    :param dataset: The dataset to tokenize\n",
    "    :type dataset: Dataset\n",
    "    :return: The set of tokens in the dataset\n",
    "    :rtype: set\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    \n",
    "    for example in dataset:\n",
    "        tokens = nltk.word_tokenize(example['text'])\n",
    "        vocab.update(tokens)\n",
    "    \n",
    "    print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "\n",
    "    with open(VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(list(vocab), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Vocabulary saved to {VOCAB_PATH}\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab \u001b[38;5;241m=\u001b[39m tokenize(train_dataset)\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 12\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(tokens)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocab)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "vocab = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Word Embedding Matrix with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vocab():\n",
    "    \"\"\"Load Glove vocab\n",
    "\n",
    "    :return: GloVe vocab\n",
    "    :rtype: Set\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe vocab...\")\n",
    "    glove_vocab:set = set()\n",
    "    # https://huggingface.co/datasets/SLU-CSCI4750/glove.6B.100d.txt\n",
    "    dataset = load_dataset(\"SLU-CSCI4750/glove.6B.100d.txt\")\n",
    "    dataset = dataset['train']\n",
    "    \n",
    "    for example in dataset:\n",
    "        word = example[\"text\"].split()[0]\n",
    "        glove_vocab.add(word)\n",
    "    print(\"GloVe vocab loaded.\")\n",
    "    return glove_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████| 400000/400000 [00:02<00:00, 163480.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe vocab loaded.\n",
      "Size of GloVe vocab: 400000\n",
      "Number of OOV Words: 1867\n"
     ]
    }
   ],
   "source": [
    "glove_vocab = load_glove_vocab()\n",
    "print(f\"Size of GloVe vocab: {len(glove_vocab)}\")\n",
    "\n",
    "oov_words = vocab - glove_vocab\n",
    "print(f\"Number of OOV Words: {len(oov_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings() -> dict:\n",
    "    \"\"\"Load GloVe embeddings\n",
    "\n",
    "    :return: GloVe embeddings\n",
    "    :rtype: Dict\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    glove_dict = {}\n",
    "    word_embedding_glove = load_dataset(\"SLU-CSCI4750/glove.6B.100d.txt\")\n",
    "    word_embedding_glove = word_embedding_glove['train']\n",
    "    \n",
    "    for example in word_embedding_glove:\n",
    "        split_line = example[\"text\"].strip().split()\n",
    "        word = split_line[0]\n",
    "        vector = np.array(split_line[1:], dtype='float32')\n",
    "        glove_dict[word] = vector\n",
    "\n",
    "    print(f\"Total GloVe words loaded: {len(glove_dict)}\")\n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix...\n",
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GloVe words loaded: 400000\n",
      "Embedding matrix built successfully.\n",
      "Embedding matrix saved as './result/embedding_matrix.npy'.\n",
      "Mapping 'word2idx' saved as './result/word2idx.json'.\n",
      "Mapping 'idx2word' saved as './result/idx2word.json'.\n"
     ]
    }
   ],
   "source": [
    "# mapping of words to indices and vice versa\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted(vocab))}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "print(\"Building embedding matrix...\")\n",
    "\n",
    "glove_dict = load_glove_embeddings()\n",
    "\n",
    "for word in vocab:\n",
    "    idx = word2idx[word]\n",
    "    # if word is in glove vocab, use glove vector\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[idx] = glove_dict[word]\n",
    "    else:\n",
    "    # word is not in glove vocab, we remove it from our vocab\n",
    "        # embedding_matrix[idx] = np.zeros(EMBEDDING_DIM)\n",
    "        word2idx.pop(word)\n",
    "        idx2word.pop(idx)\n",
    "\n",
    "print(\"Embedding matrix built successfully.\")\n",
    "\n",
    "np.save(EMBEDDING_MATRIX_PATH, embedding_matrix)\n",
    "print(f\"Embedding matrix saved as '{EMBEDDING_MATRIX_PATH}'.\")\n",
    "\n",
    "with open(WORD2IDX_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'word2idx' saved as '{WORD2IDX_PATH}'.\")\n",
    "\n",
    "with open(IDX2WORD_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Mapping 'idx2word' saved as '{IDX2WORD_PATH}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding matrix:  (18030, 100)\n",
      "Data type of the elements:  float64\n",
      "First few rows of the embedding matrix: \n",
      "[ 0.38472     0.49351001  0.49096    -1.54340005 -0.33614001  0.62220001\n",
      "  0.32264999  0.075331    0.65591002 -0.23517001  1.21140003  0.06193\n",
      " -0.62004     0.31371     0.38947999 -0.24381    -0.065643    0.58797002\n",
      " -0.86382002  0.63165998  0.68362999  0.39647001 -0.62388003 -0.25094\n",
      "  0.92830998  1.51520002 -0.43917     0.22249     1.36950004 -0.53097999\n",
      "  0.39811     0.77113998  0.49043     0.58853     0.2376      0.31619999\n",
      " -0.011962   -0.047074    0.34584999 -1.29439998  0.18596999  0.27002001\n",
      " -0.70602    -0.20652001 -0.25194001 -0.48679999 -0.71538001 -0.23886999\n",
      " -0.041612   -0.55488002 -0.54225999  0.21235999  0.025341    0.96517003\n",
      " -0.88182998 -1.86810005  0.32657     1.16890001  1.17589998 -0.17393\n",
      " -0.3371      0.87535    -1.01139998 -0.61809999  1.00800002  0.31505999\n",
      "  0.24417     0.064393    0.33678001  0.33632001  0.45975     0.22813\n",
      " -0.37505001 -0.37507999  0.089301    0.53862     0.039714   -0.0036392\n",
      " -0.25023001 -0.18223999  0.42730999 -0.79118001 -0.29409    -0.40693\n",
      " -1.09080005 -0.16475999 -0.41457999 -0.67899001  0.28319001  0.30937001\n",
      "  0.49304    -0.067002    0.50221997  0.73958999 -0.47350001 -0.47341999\n",
      " -0.20242     0.026263    0.39052001  0.52217001]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Load the embedding matrix\n",
    "embedding_matrix = np.load('result/embedding_matrix.npy')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Shape of the embedding matrix: \", embedding_matrix.shape)\n",
    "print(\"Data type of the elements: \", embedding_matrix.dtype)\n",
    "\n",
    "# Display the first few entries\n",
    "print(\"First few rows of the embedding matrix: \")\n",
    "print(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Question 1. Word Embedding\n",
    "(a) What is the size of the vocabulary formed from your training data?   \n",
    "`18030`\n",
    "\n",
    "(b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?    \n",
    "`1867`\n",
    "   \n",
=======
    "# Question 1. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(a) What is the size of the vocabulary formed from your training data?   \n",
    "\n",
    "Answer:\n",
    "`16570`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?\n",
    "\n",
    "Answer:\n",
    "`638`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 5b329c5c9087bbe9498e090b92c5450e51a7a58e
    "(c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove).\n",
    "Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you\n",
    "think is the best strategy to mitigate such limitation? Implement your solution in your source\n",
    "code. Show the corresponding code snippet. \n",
    "\n",
    "Answer:\n",
    "\n",
    "(1) Using an <UNK> Token, with its Embeddings randomized. Map any OOV words to the <UNK> Token\n",
    "\n",
    "We explore the code snippet below\n",
    "\n",
    "```python\n",
    "for word in extended_vocab:\n",
    "    idx = word2idx[word]\n",
    "    # if word is in glove vocab, use glove vector\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[idx] = glove_dict[word]\n",
    "    else:\n",
    "        # use random vector for unknown words\n",
    "        if word == UNK_TOKEN:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "        else:\n",
    "            embedding_matrix[idx] = embedding_matrix[word2idx[UNK_TOKEN]]\n",
    "```\n",
    "\n",
    "This is a useful strategy as we can use the embeddings of the <UNK> token to\n",
    "represent any unknown words. Thus, now for any unknown words, we can use the\n",
    "<UNK> token to represent them and for the vocabulary words that are not in the\n",
    "pretrained embeddings, we can use the embeddings of the <UNK> token to represent\n",
    "it.\n",
    "\n",
    "(2) There are many kinds of static embeddings. An extension of word2vec, fasttext (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words—words that appear in a test corpus but were unseen in the training corpus.\n",
    "\n",
    "A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols < and > added to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
<<<<<<< Updated upstream
=======
   "source": [
    "## Part 2. Model Training & Evaluation - RNN   \n",
    "Now with the pretrained word embeddings acquired from Part 1 and the dataset acquired from\n",
    "Part 0, you need to train a deep learning model for sentiment classification using the training set,\n",
    "conforming to these requirements:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Use the pretrained word embeddings from Part 1 as inputs; do not update them during training\n",
    "(they are “frozen”).   \n",
    "\n",
    "• Design a simple recurrent neural network (RNN), taking the input word embeddings, and\n",
    "predicting a sentiment label for each sentence. To do that, you need to consider how to\n",
    "aggregate the word representations to represent a sentence.   \n",
    "\n",
    "• Use the validation set to gauge the performance of the model for each epoch during training.\n",
    "You are required to use accuracy as the performance metric during validation and evaluation. \n",
    "   \n",
    "• Use the mini-batch strategy during training. You may choose any preferred optimizer (e.g.,\n",
    "SGD, Adagrad, Adam, RMSprop). Be careful when you choose your initial learning rate and\n",
    "mini-batch size. (You should use the validation set to determine the optimal configuration.)\n",
    "Train the model until the accuracy score on the validation set is not increasing for a few\n",
    "epochs.\n",
    "   \n",
    "• Evaluate your trained model on the test dataset, observing the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences into proper input for RNN -> sequence modelling \n",
    "# decide on RNN model? -> Vanilla Rnn / LSTM \n",
    "# dataloaders \n",
    "# train + validate -> decide on best hyperparams: optimizer = nn.optimizer.Adams\n",
    "# test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: 1 is pos, 0 is neg, can just use Sigmoid \n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "# create train, val, test dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load(EMBEDDING_MATRIX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams \n",
    "optimizer = nn.optimizer.Adams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanillaRNN(torch.nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim):\n",
    "    super().__init__() \n",
    "    # self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "    self.embedding = nn.Embedding.from_pretrained(embeddings=torch.from_numpy(embedding_matrix), freeze=True)\n",
    "    self.rnn = nn.RNN(input_dim, embedding_dim, hidden_dim, num_layers, batch_first=True) \n",
    "    # NOTE: batch_first = True, means need to input with the size of batch as the first dim\n",
    "    # NOTE: x will take the shape of (batch size, seq size, input size.)\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "  def forward(self, x):\n",
    "    embedded = self.embedding(x)\n",
    "    output, _ = self.rnn(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use an LSTM \n",
    "class LSTM(nn.Module):\n",
    "  pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotten tomatoes dataset (train) \n",
    "# parameters --> word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model many (word seq) to one (sentiment)\n",
    "# vanilla RNN \n",
    "\n",
    "# LSTM (Gated RNN)\n",
    "\n",
    "# how to aggregate word representations to represent a sentence (concatenate / averaging)\n",
    "\n",
    "# sentiment classification => "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. RNN\n",
    "(a) Report the final configuration of your best model, namely the number of training epochs,\n",
    "learning rate, optimizer, batch size.   \n",
    "\n",
    "(b) Report the accuracy score on the test set, as well as the accuracy score on the validation\n",
    "set for each epoch during training.   \n",
    "\n",
    "(c) RNNs produce a hidden vector for each word, instead of the entire sentence. Which methods\n",
    "have you tried in deriving the final sentence representation to perform sentiment classification?\n",
    "Describe all the strategies you have implemented, together with their accuracy scores on the\n",
    "test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Enhancement\n",
    "The RNN model used in Part 2 is a basic model to perform the task of sentiment classification. In\n",
    "this section, you will design strategies to improve upon the previous model you have built. You are\n",
    "required to implement the following adjustments:\n",
    "\n",
    "1. Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "2. As discussed in Question 1(c), apply your solution in mitigating the influence of OOV words\n",
    "and train your model again.\n",
    "3. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a biLSTM model and a biGRU model, incorporating recurrent computations in both directions and\n",
    "stacking multiple layers if possible.\n",
    "4. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a Convolutional Neural Network (CNN) to produce sentence representations and perform sentiment\n",
    "classification.\n",
    "5. Further improve your model. You are free to use any strategy other than the above mentioned solutions. Changing hyper-parameters or stacking more layers is not counted towards\n",
    "a meaningful improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
