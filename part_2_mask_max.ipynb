{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# We omit warnings to keep the output clean\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "\n",
    "from common_utils import load_glove_embeddings, set_seed, EmbeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed \n",
    "set_seed()\n",
    "\n",
    "# initialize parameters\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = 100 # word embedding size \n",
    "HIDDEN_SIZE = 128 # just as a starter to see \n",
    "NUM_EPOCHS = 100 \n",
    "EMBEDDING_DIM=100\n",
    "GRADIENT_CLIP=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from huggingface first \n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "with open('result/word2idx.json', \"r\") as file:\n",
    "    word2idx = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of <PAD> is:  18030\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# initialize word embeddings\n",
    "word_embeddings = EmbeddingMatrix.load()\n",
    "word_embeddings.add_padding()\n",
    "\n",
    "print(\"The index of <PAD> is: \", word_embeddings.pad_idx)\n",
    "\n",
    "print(word_embeddings.to_tensor[word_embeddings.pad_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validate and test datasets and dataloaders\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, X, y, word_embeddings:EmbeddingMatrix =word_embeddings):\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.X = X # train_dataset['text']\n",
    "        self.y = y # train_dataset['label']\n",
    "        self.len = len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # tokenize the sentence\n",
    "        tokens = self.tokenize_sentence(self.X[index])\n",
    "        return tokens, self.y[index] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len \n",
    "\n",
    "    def tokenize_sentence(self, x): \n",
    "        '''\n",
    "    returns a list containing the embeddings of each token \n",
    "    '''\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        # word tokens to index, skip if token is not in the word embeddings\n",
    "        tokens = [self.word_embeddings.get_idx(token) for token in tokens if self.word_embeddings.get_idx(token) is not None]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def pad_collate(batch, pad_value):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    # convert xx to a tensor\n",
    "    xx = [torch.tensor(x, dtype=torch.int64) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
    "    return xx_pad, torch.tensor(yy, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ed = EmbeddingsDataset(\n",
    "    train_dataset[\"text\"], train_dataset[\"label\"]\n",
    ")\n",
    "validation_dataset_ed = EmbeddingsDataset(\n",
    "    validation_dataset[\"text\"], validation_dataset[\"label\"]\n",
    ")\n",
    "test_dataset_ed = EmbeddingsDataset(test_dataset[\"text\"], test_dataset[\"label\"])\n",
    "\n",
    "pad_value = word_embeddings.pad_idx\n",
    "# implement minibatch training\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset_ed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_collate(x, pad_value),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Embeddings \n",
    "class VanillaRNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,  embedding_matrix_torch:torch.tensor, padding_idx: int):\n",
    "        super(VanillaRNNWithEmbedding, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_torch, freeze=True, padding_idx=padding_idx)\n",
    "        self.num_layers = num_layers \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # this is the num rows of the input matrix \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x).float()\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, dtype=torch.float).to(x.device)\n",
    "        # Pass the embeddings through the RNN layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Max pooling\n",
    "        res, _ = torch.max(out, dim=1)\n",
    "        # Only take the last output for each sequence\n",
    "        #res = hidden[-1]\n",
    "        # Pass through the fully connected layer\n",
    "        res = self.fc(res)\n",
    "        # Apply sigmoid activation (for binary classification)\n",
    "        res = self.sigmoid(res)\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_bce(train_dataloader, model, loss_fn, optimizer):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "    size = len(train_dataloader.dataset)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch_no, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        if train_on_gpu:\n",
    "            X_batch = X_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        \n",
    "        pred = pred.squeeze(1)\n",
    "        pred_binary = (pred >= 0.5).long()\n",
    "        loss = loss_fn(pred, y_batch.float())\n",
    "        train_loss += loss.item() \n",
    "        train_correct += (pred_binary==y_batch.long()).sum().item() \n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # TODO add main branch\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= num_batches \n",
    "    train_correct /= size \n",
    "\n",
    "    return train_loss, train_correct \n",
    "   \n",
    "\n",
    "def test_loop_bce(validate_dataloader, model, loss_fn):\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    num_batches = len(validate_dataloader)\n",
    "    size = len(validate_dataloader.dataset)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in validate_dataloader:\n",
    "            if train_on_gpu:\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            pred = model(X_batch)\n",
    "            pred = pred.squeeze(1)\n",
    "            pred_binary = (pred >= 0.5).long()\n",
    "            test_loss += loss_fn(pred, y_batch.float()).item()\n",
    "            test_correct += (pred_binary == y_batch.long()).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    return test_loss, test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, criterion, training_dataloader, validation_dataloader, epoch = NUM_EPOCHS):\n",
    "  validation_acc = [] \n",
    "  train_acc = []\n",
    "  train_losses, validate_losses = [], []\n",
    "  for i in range(epoch):\n",
    "    train_loss, train_correct = train_loop_bce(training_dataloader, model, criterion, optim) \n",
    "    validate_loss, validate_correct = test_loop_bce(validation_dataloader, model, criterion)\n",
    "    validation_acc.append(validate_correct)\n",
    "    train_acc.append(train_correct)\n",
    "    train_losses.append(train_loss)\n",
    "    validate_losses.append(validate_loss)\n",
    "\n",
    "    print(f\"Epoch {i+1}, Train Loss: {train_loss:.4f}, Validate Loss: {validate_loss:.4f}\")\n",
    "    #if i%10 == 0:\n",
    "    print(f\"Epoch:{i+1} \\t Train Acc:{train_correct} \\t Validation Acc:{validate_correct}\")\n",
    "  return train_acc, validation_acc, train_losses, validate_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6718, Validate Loss: 0.6189\n",
      "Epoch:1 \t Train Acc:0.6039859320046893 \t Validation Acc:0.6660412757973734\n",
      "Epoch 2, Train Loss: 0.5866, Validate Loss: 0.5654\n",
      "Epoch:2 \t Train Acc:0.6936694021101993 \t Validation Acc:0.7101313320825516\n",
      "Epoch 3, Train Loss: 0.5514, Validate Loss: 0.5520\n",
      "Epoch:3 \t Train Acc:0.720398593200469 \t Validation Acc:0.7166979362101313\n",
      "Epoch 4, Train Loss: 0.5347, Validate Loss: 0.5341\n",
      "Epoch:4 \t Train Acc:0.7316529894490035 \t Validation Acc:0.7373358348968105\n",
      "Epoch 5, Train Loss: 0.5155, Validate Loss: 0.5705\n",
      "Epoch:5 \t Train Acc:0.7468933177022274 \t Validation Acc:0.700750469043152\n",
      "Epoch 6, Train Loss: 0.5029, Validate Loss: 0.5124\n",
      "Epoch:6 \t Train Acc:0.7533411488862837 \t Validation Acc:0.7626641651031895\n",
      "Epoch 7, Train Loss: 0.4928, Validate Loss: 0.5073\n",
      "Epoch:7 \t Train Acc:0.7610785463071512 \t Validation Acc:0.7560975609756098\n",
      "Epoch 8, Train Loss: 0.4819, Validate Loss: 0.5129\n",
      "Epoch:8 \t Train Acc:0.7685814771395076 \t Validation Acc:0.7485928705440901\n",
      "Epoch 9, Train Loss: 0.4735, Validate Loss: 0.5050\n",
      "Epoch:9 \t Train Acc:0.7756154747948417 \t Validation Acc:0.7692307692307693\n",
      "Epoch 10, Train Loss: 0.4655, Validate Loss: 0.5016\n",
      "Epoch:10 \t Train Acc:0.7801875732708089 \t Validation Acc:0.7607879924953096\n",
      "Epoch 11, Train Loss: 0.4593, Validate Loss: 0.5125\n",
      "Epoch:11 \t Train Acc:0.7821805392731536 \t Validation Acc:0.7457786116322702\n",
      "Epoch 12, Train Loss: 0.4574, Validate Loss: 0.4961\n",
      "Epoch:12 \t Train Acc:0.7865181711606096 \t Validation Acc:0.7636022514071295\n",
      "Epoch 13, Train Loss: 0.4494, Validate Loss: 0.5075\n",
      "Epoch:13 \t Train Acc:0.7903868698710433 \t Validation Acc:0.7570356472795498\n",
      "Epoch 14, Train Loss: 0.4423, Validate Loss: 0.4942\n",
      "Epoch:14 \t Train Acc:0.7917936694021102 \t Validation Acc:0.7692307692307693\n",
      "Epoch 15, Train Loss: 0.4365, Validate Loss: 0.4961\n",
      "Epoch:15 \t Train Acc:0.7960140679953107 \t Validation Acc:0.7607879924953096\n",
      "Epoch 16, Train Loss: 0.4352, Validate Loss: 0.5049\n",
      "Epoch:16 \t Train Acc:0.7970691676436108 \t Validation Acc:0.7654784240150094\n",
      "Epoch 17, Train Loss: 0.4266, Validate Loss: 0.5026\n",
      "Epoch:17 \t Train Acc:0.8018757327080891 \t Validation Acc:0.7654784240150094\n",
      "Epoch 18, Train Loss: 0.4204, Validate Loss: 0.4942\n",
      "Epoch:18 \t Train Acc:0.8084407971864009 \t Validation Acc:0.7532833020637899\n",
      "Epoch 19, Train Loss: 0.4154, Validate Loss: 0.4894\n",
      "Epoch:19 \t Train Acc:0.8109026963657678 \t Validation Acc:0.773921200750469\n",
      "Epoch 20, Train Loss: 0.4100, Validate Loss: 0.4900\n",
      "Epoch:20 \t Train Acc:0.8150058616647128 \t Validation Acc:0.7720450281425891\n",
      "Epoch 21, Train Loss: 0.4056, Validate Loss: 0.4927\n",
      "Epoch:21 \t Train Acc:0.8141852286049238 \t Validation Acc:0.7692307692307693\n",
      "Epoch 22, Train Loss: 0.3988, Validate Loss: 0.5214\n",
      "Epoch:22 \t Train Acc:0.817936694021102 \t Validation Acc:0.7523452157598499\n",
      "Epoch 23, Train Loss: 0.3934, Validate Loss: 0.5052\n",
      "Epoch:23 \t Train Acc:0.8215709261430246 \t Validation Acc:0.7589118198874296\n",
      "Epoch 24, Train Loss: 0.3941, Validate Loss: 0.5021\n",
      "Epoch:24 \t Train Acc:0.8242672919109026 \t Validation Acc:0.7673545966228893\n",
      "Epoch 25, Train Loss: 0.3834, Validate Loss: 0.5085\n",
      "Epoch:25 \t Train Acc:0.8296600234466589 \t Validation Acc:0.7607879924953096\n",
      "Epoch 26, Train Loss: 0.3790, Validate Loss: 0.4983\n",
      "Epoch:26 \t Train Acc:0.8318874560375147 \t Validation Acc:0.7664165103189493\n",
      "Epoch 27, Train Loss: 0.3729, Validate Loss: 0.5149\n",
      "Epoch:27 \t Train Acc:0.8327080890973036 \t Validation Acc:0.7598499061913696\n",
      "Epoch 28, Train Loss: 0.3691, Validate Loss: 0.5172\n",
      "Epoch:28 \t Train Acc:0.8344665885111372 \t Validation Acc:0.7551594746716698\n",
      "Epoch 29, Train Loss: 0.3611, Validate Loss: 0.5136\n",
      "Epoch:29 \t Train Acc:0.8417350527549824 \t Validation Acc:0.7448405253283302\n",
      "Epoch 30, Train Loss: 0.3554, Validate Loss: 0.5209\n",
      "Epoch:30 \t Train Acc:0.8424384525205159 \t Validation Acc:0.7476547842401501\n",
      "Epoch 31, Train Loss: 0.3511, Validate Loss: 0.5080\n",
      "Epoch:31 \t Train Acc:0.8475967174677609 \t Validation Acc:0.7392120075046904\n"
     ]
    }
   ],
   "source": [
    "vanilla_rnn = VanillaRNNWithEmbedding(input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=2, num_classes=1, embedding_matrix_torch=word_embeddings.to_tensor, padding_idx=word_embeddings.pad_idx)\n",
    "optim = torch.optim.Adam(vanilla_rnn.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_acc_vanilla_rnn, validation_acc_vanilla_rnn, train_loss_vanilla_rnn, validation_loss_vanilla_rnn = train(vanilla_rnn, optim, criterion, train_dataloader, validation_dataloader, epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
