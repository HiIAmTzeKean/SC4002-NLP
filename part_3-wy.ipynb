{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Enhancement\n",
    "The RNN model used in Part 2 is a basic model to perform the task of sentiment classification. In\n",
    "this section, you will design strategies to improve upon the previous model you have built. You are\n",
    "required to implement the following adjustments:\n",
    "\n",
    "1. Instead of keeping the word embeddings fixed, now update the word embeddings (the same\n",
    "way as model parameters) during the training process.\n",
    "2. As discussed in Question 1(c), apply your solution in mitigating the influence of OOV words\n",
    "and train your model again.\n",
    "3. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a biLSTM model and a biGRU model, incorporating recurrent computations in both directions and\n",
    "stacking multiple layers if possible.\n",
    "4. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a Convolutional Neural Network (CNN) to produce sentence representations and perform sentiment\n",
    "classification.\n",
    "5. Further improve your model. You are free to use any strategy other than the above mentioned solutions. Changing hyper-parameters or stacking more layers is not counted towards\n",
    "a meaningful improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from gensim.models import FastText\n",
    "from common_utils import EmbeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/word2idx.json', 'r', encoding='utf-8') as f:\n",
    "    word2idx = json.load(f)\n",
    "\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "embedding_matrix = np.load('result/embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100  \n",
    "VOCAB_SIZE = max(word2idx.values()) + 1\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FASTTEXT\n",
    "# embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# for word, idx in word2idx.items():\n",
    "#     if word in fasttext_model.wv:\n",
    "#         embedding_matrix[idx] = fasttext_model.wv[word]\n",
    "#     else:\n",
    "#         # Initialize missing embeddings with random values\n",
    "#         embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, num_filters, filter_sizes, dropout):\n",
    "        super(CNN, self).__init__()\n",
    "        # Create input layer, for the pretrained embedding matrix\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) \n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))  \n",
    "\n",
    "        # Enable back propagation -> Part 3 Task 1, allowing embeddings to be updated\n",
    "        self.embedding.weight.requires_grad = True \n",
    "\n",
    "        # To store a list of Convolutions Layers, for 2-gram, 3-gram, 4-gram, 5-gram, 6-gram models.\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs) for fs in filter_sizes # fs = N-gram for varying length\n",
    "        ])\n",
    "\n",
    "        # Randomly drops some elements in the input tensor as a form of Regulatization, to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # My attempt to incorporate Attention layer by applying more weights on importat feature? Not really helping actually\n",
    "        self.attention_layer = nn.Linear(len(filter_sizes) * num_filters, len(filter_sizes) * num_filters) \n",
    "        \n",
    "        # Output the final result => Classes, 2 for our binary classification case\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        x = self.embedding(x) \n",
    "\n",
    "        # Reshape to (batch_size, embedding_dim, sequence_length), as per Conv1d's expectation\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Applying ReLU activation function for each N-gram Conv1d layer\n",
    "        conv_results = [F.relu(conv(x)) for conv in self.convs]\n",
    "\n",
    "        # To perform max-over-time-pooling, taking the max value across entire sequence length for each filter, reducing the dimentionality of the output\n",
    "        # Maybe can consider avg_pool1d too\n",
    "        pool_results = [F.max_pool1d(conv_result, conv_result.shape[2]).squeeze(2) for conv_result in conv_results]\n",
    "        \n",
    "        # Concatenate the pooled features\n",
    "        x = torch.cat(pool_results, 1) \n",
    "\n",
    "        # Prevent Overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Attention is all you need? (Unfortunately not really in our case :')\n",
    "        attention_weights = torch.softmax(self.attention_layer(x), dim=1)\n",
    "        x = x * attention_weights\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "NUM_FILTERS = 100 \n",
    "FILTER_SIZES = [2, 3, 4, 5, 6] # Essentially treating single CNN 1 Dimensional as N-gram model (Use the filter size to determine computation with kernel)\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "model = CNN(VOCAB_SIZE, EMBEDDING_DIM, NUM_CLASSES, NUM_FILTERS, FILTER_SIZES, DROPOUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are to prepare dataset for mini-batch training\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "train_texts = [example['text'] for example in dataset['train']]\n",
    "train_labels = [example['label'] for example in dataset['train']]\n",
    "\n",
    "test_texts = [example['text'] for example in dataset['test']]\n",
    "test_labels = [example['label'] for example in dataset['test']]\n",
    "\n",
    "def tokenize(texts, word2idx, max_len=512):\n",
    "    tokenized = []\n",
    "    for text in texts:\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        token_ids = [word2idx.get(word, word2idx[UNK_TOKEN]) for word in tokens]\n",
    "        tokenized.append(torch.tensor(token_ids[:max_len]))  # Truncate to max_len, or should I remove it?\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = tokenize(train_texts, word2idx)\n",
    "test_tokenized = tokenize(test_texts, word2idx)\n",
    "\n",
    "train_tokenized = pad_sequence(train_tokenized, batch_first=True)\n",
    "test_tokenized = pad_sequence(test_tokenized, batch_first=True)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "train_data = data.TensorDataset(train_tokenized, train_labels)\n",
    "test_data = data.TensorDataset(test_tokenized, test_labels)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6789, Accuracy: 0.5489\n",
      "Epoch [2/5], Loss: 0.5729, Accuracy: 0.6937\n",
      "Epoch [3/5], Loss: 0.4539, Accuracy: 0.8023\n",
      "Epoch [4/5], Loss: 0.3447, Accuracy: 0.8696\n",
      "Epoch [5/5], Loss: 0.3255, Accuracy: 0.8698\n",
      "Test Accuracy: 0.7636\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) #Idea: Can use Adam / MiniAdam / SGD with Momentum\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct_preds = 0\n",
    "        total_samples = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        scheduler.step()\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_preds / total_samples\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = correct_preds / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, EPOCHS)\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result\\\\cnn_model_graph.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "import torch\n",
    "\n",
    "sample_input = torch.randint(0, VOCAB_SIZE, (1, 100)).to('cpu') \n",
    "output = model(sample_input)\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "dot.format = 'png'  \n",
    "dot.render('result/cnn_model_graph')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3. Enhancement\n",
    "(a) Report the accuracy score on the test set when the word embeddings are updated (Part 3.1).\n",
    "   \n",
    "(b) Report the accuracy score on the test set when applying your method to deal with OOV words\n",
    "in Part 3.2.\n",
    "   \n",
    "(c) Report the accuracy scores of biLSTM and biGRU on the test set (Part 3.3).\n",
    "   \n",
    "(d) Report the accuracy scores of CNN on the test set (Part 3.4).\n",
    "   \n",
    "(e) Describe your final improvement strategy in Part 3.5. Report the accuracy on the test set\n",
    "using your improved model.\n",
    "   \n",
    "(f) Compare the results across different solutions above and describe your observations with possible discussions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002-nlp-LiAJfnhK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
